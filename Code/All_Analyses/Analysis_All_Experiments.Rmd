---
title: "Spanumbra"
subtitle: "Full statistical analysis and plots"
author: "Mathilde Josserand [mathilde.josserand@gmail.fr], Elena Eccher []"
date: "`r date()`"
output:
  html_document:
    highlight: textmate
    theme: journal
    toc: yes
    toc_depth: 6
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '6'
  word_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,                   # default code chunk options
                      fig.width=11, fig.height=6, fig.align="center", comment=NA, # default figure dimensions
                      fig.path="./figures/",                                      # save images to ./figures/
                      dpi=70, dev="jpeg",                                         # please set dpi=300 and comment out dev="jpeg" for high resolution but very big images
                      cache=TRUE, autodep=TRUE);                                  # cache chunks

```


```{r , echo=F, message=F, warning=FALSE}

library(knitr) # for opts_knitr and having captions to figures
library(data.table) # read some dataframe
library(tidyr) # for data processing
library(dplyr) # for data processing
library(ggplot2) # for plotting
library(rstatix) # for basic statistical test
library(gridExtra) # a few others functions for plotting
library(heatmaply) # for correlation matric
library(caret) # for confusion matrix
library(nnet) # multinomial regression
library(lme4) # for glmer function
library(Hmisc) # for function cut2
library(tibble) # for data processing
library(car) # for vif function
library(FactoMineR) # for PCA
library(factoextra) # also for PCA

wid_freq = 10
hei_freq = 6

# function to count length of a vector without NA
length2 <- function (x, na.rm=FALSE) {
    if (na.rm) sum(!is.na(x))
    else       length(x)
}

# function to compute standard error
se <- function(x) sd(x, na.rm = TRUE)/sqrt(length2(x))

# to do summary of variable
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}




# make a plot of frequency (used for ordering cards exp)
frequency_plot <- function(input_dataframe, list_exp, variable, type_cards, titlegraph){
  
  if (type_cards != "Gaze"){
    # filter the dataframe 
    input_dataframe <- input_dataframe[input_dataframe$exp %in% list_exp,]
  
  }
  
  # take a dataset without the missing data
  input_dataframe[[variable]] <- as.character(input_dataframe[[variable]])
  input_dataframe_nodat <- input_dataframe[is.na(input_dataframe[[variable]])==FALSE & input_dataframe[[variable]]!= "" ,]
  
  # Create a table for the relative frequency
  list_modality <- as.character(unique(input_dataframe_nodat[[variable]]))
  df_rf <- data.frame(myvar= rep(list_modality, each=3), 
                      order= rep(c("L", "N", "R"), length(list_modality)), 
                      relat_freq=rep(0,length(list_modality)*3), 
                      N_part=rep(0,length(list_modality)*3))
  
  
  if (type_cards=="Gaze"){
      df_rf <- data.frame(myvar=rep(list_modality, each=4), 
                      automatic= rep(c("Left", "Right"), 2*length(list_modality)), 
                      condition = c(rep(c("Decrease"), length(list_modality)), rep(c("Increase"), length(list_modality)), rep(c("Decrease"), length(list_modality)), rep(c("Increase"), length(list_modality))),
                      relat_freq=rep(0,length(list_modality)*4),
                      N_part=rep(0,length(list_modality)*4))
  }

  for (row in c(1:nrow(df_rf))){
    # add the relative frequency of each condition
    if (type_cards!="" & type_cards!="Gaze"){
      df_rf[row,3] <- 
        round(nrow(input_dataframe_nodat[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                               input_dataframe_nodat[[type_cards]]==df_rf[row,2],]) /
              nrow(input_dataframe_nodat[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                               (input_dataframe_nodat[[type_cards]]=="N" | input_dataframe_nodat[[type_cards]]=="L" | input_dataframe_nodat[[type_cards]]=="R" ),])*100)
      
      # add the total number of subject for each condition
      df_rf[row,4] <- 
        length(unique(input_dataframe_nodat$participant_id[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                                                          is.na(input_dataframe_nodat[[type_cards]])==FALSE & 
                                                          input_dataframe_nodat[[type_cards]]==df_rf[row,2]]))
    } else if (type_cards == "") {
      df_rf[row,3] <- 
        round(nrow(input_dataframe_nodat[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                               input_dataframe_nodat$order==df_rf[row,2],]) /
              nrow(input_dataframe_nodat[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                               (input_dataframe_nodat$order=="N" | input_dataframe_nodat$order=="L" | input_dataframe_nodat$order=="R" ),])*100)
      
      # add the total number of subject for each condition
      df_rf[row,4] <- 
         length(unique(input_dataframe_nodat$participant_id[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                                                            is.na(input_dataframe_nodat$order)==FALSE &
                               input_dataframe_nodat$order==df_rf[row,2]]))
      
    } else if (type_cards == "Gaze"){
      df_rf[row,4] <- 
        round(nrow(input_dataframe_nodat[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                         input_dataframe_nodat$automatic==df_rf[row,2] &
                         input_dataframe_nodat$condition==df_rf[row,3],]) /
          nrow(input_dataframe_nodat[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                          input_dataframe_nodat$condition==df_rf[row,3],])*100)
      # add the total number of subject for each condition
      df_rf[row,5] <- 
        length(unique(input_dataframe_nodat$participant_id[input_dataframe_nodat[[variable]]==df_rf[row,1] &
                                                           is.na(input_dataframe_nodat[[variable]])==FALSE &
                               input_dataframe_nodat$automatic==df_rf[row,2] &
                               input_dataframe_nodat$condition==df_rf[row,3]]))

    }
            
  }
  
  # find the number of participant for the big condition
  df_rf %>%
    dplyr::group_by(myvar) %>%
    dplyr::summarize(N = sum(N_part)) %>%
    mutate(label = paste("N =", N)) -> dat_text
  
  if (type_cards == "Gaze"){
    df_rf %>%
      dplyr::group_by(myvar, condition) %>%
      dplyr::summarize(N = sum(N_part)) %>%
      mutate(label = paste("N =", N)) -> dat_text
  }
  
  # add exp component
  df_rf$exp <- list_exp
  
  # plot
  if (type_cards != "Gaze"){
    ggplot(data=df_rf, aes(x=order, y=relat_freq))+
      geom_histogram(stat="identity", position="dodge", size=1, aes(fill=order)) +
      theme_bw(base_size=15) +
      facet_grid(exp ~ myvar) +
      scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1"))+
      guides(fill=FALSE) +
      geom_text(aes(label=paste(relat_freq, "%")), vjust=-0.3, size=4.8)   +
      theme(plot.title = element_text(hjust = 0.5, size=15)) +
      labs(y="Relative frequency (in %)", x="",title=titlegraph) +
      ylim(0,100) +
      geom_text(data = dat_text, mapping = aes(x = 3, y = 56, label = label), hjust = -0.1, vjust = -1)
  } else {
    ggplot(data=df_rf, aes(x=automatic, y=relat_freq))+
      geom_histogram(stat="identity", position="dodge", size=1, aes(fill=automatic)) +
      theme_bw(base_size=15) +
      facet_grid(condition ~ myvar) +
      scale_fill_manual(values=c("dodgerblue",  "darkgoldenrod1"))+
      guides(fill=FALSE) +
      geom_text(aes(label=paste(relat_freq, "%")), vjust=-0.3, size=4.8)   +
      theme(plot.title = element_text(hjust = 0.5, size=15)) +
      labs(y="Relative frequency (in %)", x="",title=titlegraph) +
      ylim(0,100) +
      geom_text(data = dat_text, mapping = aes(x = 1, y = 70, label = label), hjust = -0.1, vjust = -1)
    
  }

}


### Plotting for VT, Squares, Snakes and Candies experiments
plot_bias <- function(input_dataframe, experiment, time_err_invefi, mean_or_median, variable, mytitle){
  
  ## UNFORTUNATELY I have to do it in base R :( Because with Dplyr I cannot transfer a column name in my function
  
  # compute the average mean for each condition, position and situation
  # all %>%
  #   filter(exp == "VT_2021" | exp == "VT_2022") %>%
  #   dplyr::group_by(condition, group, stimuli, exp, School) %>%
  #   dplyr::summarize(se = se(time),
  #                    time = mean(time)) -> rt_tab

  # compute the average mean for each condition, position and situation
  input_dataframe <- input_dataframe[(is.na(input_dataframe[[variable]])==FALSE),] 
  input_dataframe <- input_dataframe[input_dataframe$exp %in% experiment,]

  if (mean_or_median == "mean" & time_err_invefi == "time"){
     input_dataframe <- input_dataframe[input_dataframe$stimuli == "target" & input_dataframe$errors == 0,]
     input_dataframe_mean <- aggregate(list(time = input_dataframe$time), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = mean)
     input_dataframe_se <- aggregate(list(se = input_dataframe$time), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = se)
     input_dataframe_ag <- merge(input_dataframe_mean, input_dataframe_se, by=c("condition", "group", "exp", "variable"))

  } else if (mean_or_median == "median" & time_err_invefi == "time"){
     input_dataframe_mean <- aggregate(list(time = input_dataframe$time,
                                         se = input_dataframe$time), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = median)
     input_dataframe_se <- aggregate(list(se = input_dataframe$time), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = se)
     input_dataframe_ag <- merge(input_dataframe_mean, input_dataframe_se, by=c("condition", "group", "exp", "variable"))
   
  } else if (mean_or_median == "mean" & time_err_invefi == "errors"){
     input_dataframe_mean <- aggregate(list(error = input_dataframe$errors), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           stimuli = input_dataframe$stimuli,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = mean)
     input_dataframe_se <- aggregate(list(se = input_dataframe$errors), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = se)
     input_dataframe_ag <- merge(input_dataframe_mean, input_dataframe_se, by=c("condition", "group", "exp", "variable"))

  } else if (mean_or_median == "mean" & time_err_invefi == "invefi"){
     input_dataframe <- input_dataframe[input_dataframe$stimuli == "target" & input_dataframe$errors == 0,]
     input_dataframe_mean <- aggregate(list(mean_invefi = input_dataframe$invefi), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = mean)
     input_dataframe_se <- aggregate(list(se = input_dataframe$invefi), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = se)
     input_dataframe_ag <- merge(input_dataframe_mean, input_dataframe_se, by=c("condition", "group", "exp", "variable"))

  } else if (mean_or_median == "median" & time_err_invefi == "invefi"){
     input_dataframe <- input_dataframe[input_dataframe$stimuli == "target" & input_dataframe$errors == 0,]
     input_dataframe_mean <- aggregate(list(mean_invefi = input_dataframe$invefi), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = median)
      input_dataframe_se <- aggregate(list(se = input_dataframe$invefi), by=
                                      list(condition = input_dataframe$condition,
                                           group = input_dataframe$group,
                                           exp = input_dataframe$exp,
                                           variable = input_dataframe[[variable]]), FUN = se)
     input_dataframe_ag <- merge(input_dataframe_mean, input_dataframe_se, by=c("condition", "group", "exp", "variable"))

  }


  if (time_err_invefi == "time"){
    # plot table
    ggplot(input_dataframe_ag, aes(x=condition, y=time, colour=group, group=group)) + 
        geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
        geom_line(position=position_dodge(0.1)) +
        geom_point(position=position_dodge(0.1), size=3) +
      theme_bw(base_size=15) +
      facet_grid(variable ~ exp) +
      labs(x="", y="Mean time", title=mytitle)
  } else if ( time_err_invefi == "errors"){
    ggplot(input_dataframe_ag, aes(x=condition, y=error, colour=group, group=group)) + 
        geom_errorbar(aes(ymin=error-se, ymax=error+se), colour="black", width=.1, position=position_dodge(0.1)) +
        geom_line(position=position_dodge(0.1)) +
        geom_point(position=position_dodge(0.1), size=3) +
      theme_bw(base_size=15) +
      facet_grid(variable ~ exp) +
      labs(x="", y="Mean error", title=mytitle)
   } else if ( time_err_invefi == "invefi"){
    ggplot(input_dataframe_ag, aes(x=condition, y=mean_invefi, colour=group, group=group)) + 
        geom_errorbar(aes(ymin=mean_invefi-se, ymax=mean_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
        geom_line(position=position_dodge(0.1)) +
        geom_point(position=position_dodge(0.1), size=3) +
      theme_bw(base_size=15) +
      facet_grid(variable ~ exp) +
      labs(x="", y="Mean inverse efficiency", title=mytitle)
    
  }

}

# Figure and Table caption adapted from https://stackoverflow.com/questions/37116632/rmarkdown-html-number-figures: 
outputFormat = opts_knit$get("rmarkdown.pandoc.to"); # determine the output format of the document
if( is.null(outputFormat) ) outputFormat = ""; # probably not run within knittr
capTabNo = 1; capFigNo = 1; # figure and table caption numbering, for HTML do it manually
#Function to add the Table Number
capTab = function(x)
{
  if(outputFormat == 'html'){
    x = paste0("**Table ",capTabNo,".** ",x,"")
    capTabNo <<- capTabNo + 1
  }; x
}
#Function to add the Figure Number
capFig = function(x, show_R_version=TRUE, show_package_versions=NULL, is_map=FALSE)
{
  if(outputFormat == 'html')
  {
    x <- paste0("**Figure ",capFigNo,".** ",x,"");
    if( show_R_version || (!is.null(show_package_versions) && length(show_package_versions) > 0) )
    {
      x <- paste0(x, " Figure generated using ");
      if( show_R_version ) x <- paste0(x, stringr::str_replace(R.version.string, stringr::fixed("R "), "[`R`](https://www.r-project.org/) "));
      if( !is.null(show_package_versions) && length(show_package_versions) > 0 )
      {
        x <- paste0(x, ifelse( show_R_version, " and ", " "));
        x <- paste0(x, ifelse( length(show_package_versions) > 1, "packages ", "package "));
        x <- paste0(x, paste0(vapply(show_package_versions, function(x) paste0("`",x,"`", " (version ", packageVersion(x),")"), character(1)), collapse=", "), ".");
      }
      if( is_map ) x <- paste0(x, " Maps are using public domain data from the [Natural Earth project](https://www.naturalearthdata.com/) as provided by the `R` package `maps`.");
    }
    capFigNo <<- capFigNo + 1;
  }; 
  x;
}


```


# Introduction

< TBD: litterature >

This project aims to investigate further the relation between number and space. This RMarkdown file gives more information about the exact protocol and material used (see [Method]), as well as the descriptive and analytic statistics (see [Results]).

# Method

```{r , echo=F, message=F, warning=FALSE}
# read questionnaire
pathres = "C:/Users/Mathilde JOSSERAND/Desktop/Spanumbra/Results/"
```

We investigated different populations:
 - Himba people, all adults, during two different fieldwork (respectively in 2021 and 2022)
 - Italian (TBD)
 - Italian Adults (TBD)
 
## Himbas 2021

The experimental protocol consists in 6 distinct experiments. 

| Experiment number | Experiment name | Short description | Type of experiment |
|------------|-------------|-------------|-------------|
| 1 | Naming sets (from Pica et al, 2004) | counting stones in the participant's language(s) | Ecological (no technology involved) |
| 2 | Ordering cards (part 1) | ordering numerosities without specific instructions | Ecological (no technology involved) | 
| 3 | Gaze on numerosities (from Di-Giorgio et al, 2019) | passive presentation of changing numerosities, and gaze tracking | On the computer | 
| 4 | Reaction times | press on a button when presented with increasing or decreasing numerosities | On the computer  | 
| 5 | Basket experiment | hidden balls behind baskets: training with vertical sets, testing on horizontal sets | On the computer |
| 6 | Ordering cards (part 2) | ordering numerosities on a line (left to right or right to left) | Ecological (no technology involved) | 
| 7 | Number line experiment (from Dehaene et al, 2008) | placing numerosities on a line | On the computer | 

The experimenter (Mathilde Josserand) was present for each experiment with the help of a translator (Fanny Kavari), under the supervision of Serge Caparos. While the translator was autonomous for most experiments, the experimenter stayed behind in order to help with the program and check that everything was OK. Esther Boissin also provided help and support when needed in 2021.

### Demographic and data on numerical abilities

The demographical personal information was always collected before the data on numerical abilities. However, this part was done either before, either after the experiments on the mental number line. To account for potential bias of order effect, we recorded the order of passation of each experiment in this file. 

```{r order passation, echo=FALSE, message=FALSE, warning=FALSE}

library(ggplot2)

# read dataframe with experiment order
order_exp <-  read.table(paste(pathres, "Himbas_2021/order_exp.csv", sep=""), header=T, sep=";", quote='"', fill=TRUE) 
names(order_exp)[names(order_exp) == "ï..subject"] <- "participant_id"

# create a new column for plotting
order_exp <- data.frame(lapply(order_exp,as.numeric))
order_exp$order <- "nc"
order_exp$order[order_exp$manuela < order_exp$questionnaire] <- "questionnaire after"
order_exp$order[order_exp$manuela > order_exp$questionnaire] <- "questionnaire before"

# find the list of participants that had done Manuela and the questionnaire
newborn <-  read.table(paste(pathres, "Himbas_2021/GazeTracking/Summary/exp1.csv", sep=""), header=T, sep=",", quote='"', fill=TRUE) 
list_all_participants = unique(newborn$participant_id)
#list_all_participants = order_exp[!(is.na(order_exp$manuela)) & !(is.na(order_exp$questionnaire)),]$participant_id
my_exps <- order_exp[order_exp$subject %in% list_all_participants,]

# plot the data: questionnaire before or after?
ggplot(data=my_exps, aes(x=order, fill=order)) +
  geom_histogram(stat="count") +
  theme_bw(base_size=15) +
  guides(fill=FALSE) +
  labs(x="")


```



#### Collect personal information

Using an Eprime interface *and* a Python gui interface (depending on the participant), the experimenter collected some of the participant's personal information:

| Name variable | Values | Short description  |
|------------|-------------|-------------|
| *ethnicity*  | Himbas, Herero, Zemba | Most participants are Himbas. However, we also tested a few people from Herero and Zemba ethnicities living inside Himba village. These people were married with Himbas and they share the same language and the same way of life, but differs on some aspects of the culture (clothing, etc). They also come from the same genetic ancestry (bantoue). |
| *clothing_type* | Traditional, modern | The clothes the participant wear the day of the experiment | 
| *gender* | male, female |  | 
| *school* | yes, no | Whether the participant has ever attended school | 
| *grade* | from 0 to 6 | The highest grade completed by the participant | 
| *level_education* | none, incomplete_primary, complete_primary, incomplete_secondary, complete_seconday |  | 
| *type_school* | none, mobile, rural, town | The type of school the participant has been to | 
| *literacy* | none, little, good, verygood | Whether the participants knows how to read | 
| *literacy_group* | none, some | Binary classification of the previous variable | 
| *stress_life* | never, sometimes, often, always | How stressed the participant feels in his.her life | 
| *stress_now* | no, little, moderate, very | How stressed the participant feels now |
| *eye* | yes, no | Whether the participant has trouble to see. Due to the smoke present in the huts during the night, most participants suffer from conjunctivities. However, the rate of myopia is very low. |
| *lateralisation* | left-handed, right-handed |  | 
| *village_name* | Okatutura, Otjeme, Otjirumbu, Otuazuma | The experiments took place in different villages in the same geographical area |
| *age* | from 1 to 90  | The age may be approximate: whenever the participant did not know his.her real age, the translator estimated an approximate age | 
| *wealth* | from 0 to 350  | This number was calculated based on the number of goats and cows owned by the participant (1cow = 10pts; 1goat = 1pt as cows are way more valuable than goats | 
| *opuwo_year* | from 0 to 20  | How many times the participant has been to Opuwo (the main city nearby the village) this year. | 
| *opuwo_year_group* | from 0 to 5  | Classification of the previous variable inside 5 distinct categories based on the frequency with which the participant goes to the city (1=1 time, 2=2 times, 3=3 times, 4=4times, 5=5 times or more) | 
| *opuwo_life* | from 0 to 100  | How many times the participant has been to Opuwo in his.her life  | 
| *opuwo_reason* | free text  | The reason why the participant is going to the city  | 
| *nb_lang_spoken* | from 0 to 3  | How many languages the participant knows outside of Otjiherero (the mother tongue of Himbas)  |
| *english* | yes, no  | whether the participant knows English language  | 
| *telephone* | yes, no  | whether the participant possesses a phone number  | 
 
These data are combined with the anonymous ID number (chosen by the experimenter at the beginning of the experiments), and the date and time (automatically computed). All participants personal information is gathered in a csv file.

#### Investigating Himbas numerical abilities

We tested the numerical abilities of Himbas using a range of different experiments:

| Type of test | Subtype of test |  Calculation  | Explanation |  Data transformation
|------------|-------------|-------------|-------------|-------------|
| **Counting**  |  |  | the participant is asked to count; the translator note the highest correct value said by the participant (note that the participant is stopped above 40) |  A percentage was calculated, 40 corresponding to 100% |
| **Comparison**  |  |   (2 & 6) <br> (8 & 7) <br>	(16 & 11) <br>	(15 & 40) <br>	(60 & 50) <br>	(59 & 73) <br>	(80 & 9) <br>	(109 & 700) | the partipant is asked to say the biggest number in a pair of number | for each pair, we define the accuracy as being either a correct answer (acc = 1) or a wrong answer (acc = 0). Accuracy results across pairs are added and a global comparison score percentage is computed (8 --> 100%) | 
| **Calculation** | 2 additions (*abstract*) <br> 2 substractions (*abstract*) <br> 2 multiplications (*abstract*)  <br> 1 addition & substraction (*concrete*) <br> 1 division (*concrete*) | (4+3);	(15+12) <br>  (14-8);	(9-6) <br>	(3x2);	(2x6) <br> (7-(3+1)) <br> (12/2) | the participant is asked to do some calculations: either there is no context for the calculations (*abstract*), either they were included in a real-life problem (*concrete*) | for each calculation, we define the accuracy as a correct answer (acc = 1) or a wrong answer (acc = 0). Accuracy results for all calculations are added and a global calculation score percentage (8 --> 100%) |

We compute a **global numerical score** out of the 3 previous score: the *counting* score, the *comparison* score and the *calculation* score.


### Experiment 1: counting stones

#### Introduction

The participant is asked to count stones. He is first presented with 15 stones, then 8 stones. The experimenter records the strategy used to count the stones of the participant.

#### Material

For this experiment, the following material is used:

 - A small tissue bag with **15 stones**
 - **Program**: a Gui interface was created with Python to manually enter the data; the program asks the number said by the participant and the strategy used when presented with 15 and 8 stones
 
![*Illustration 1: stones.*](pictures/stones.jpg){width=30%}

#### Procedure

The experimenter places the stones on the table. The translator asks: *“how many stones are there?”* The experimenter uses the program to write the number said by the participant, and notes the counting strategy (count one by one, make groups...).


#### Data 

The Gui interface creates one .csv file per participant. These .csv file were later converted inside one single .csv file, with the following column:

 - *participant id* ;
 - *number of stones*;
 - *accuracy*: whether the participant used the right number to designate the number of stones;
 - *strategy*: strategy used by the participant, as written by the experimenter;
 - *strategy plot*: same as above, with data transformation for better visualisation



### Experiment 2: Ordering cards (part 1 part 2)

#### Introduction

This experiment aims at studying how participants order numerosities. It is split in three different parts:

 - First, we observe whether a bias to form a line exists. Thus, we will leave the participant **free to order the cards** (with **dots**) the way s.he wants, as soon as the cards lay on the table (*part 1*).
 - Second, we give the same instruction but with cards with **arabic numbers** (*part 2*);
 - Third, we force the participant to **order the cards** (with **dots**) **on a line**, and we observe the direction of the bias (left to right or right to left) (*part 3*).

The first two parts and the third part occur at different times of the experiment.

#### Material

For this experiment, the following material is used:

 - **10 round cards** of diameter 4 cm, with a white background and black dots. The location of the dots is scrambled and extracted from Dehaene et al (2008)’s paper. The cards are printed on thick paper (400mm) and plasticized.
 
  - **10 squared cards** of length 4 cm, with a white background and hand-written arabic numbers. The cards were fixed onto small stones, so that they do not fly away with the wind. 
 
 - **a phone** with a camera (for this experiment, a Xiaomi Redmi Note Pro 9 is used)
 
![*Illustration 2: Stimuli*](pictures/all_stimuli_ok.jpg)

 
#### Procedure

**Part 1**:

The cards are ordered **randomly in a pile** on the table. The experimenter asks the participant : *“please order the cards. You can place them the way you want, but they all have to lay on the table”* (these instructions are translated by the translator prior to the experiment). When the participant says that he is done, The experimenter places a little card indicating the participant number on the table and takes a picture of the configuration. She also manually inserts the data and her comments into the Python gui interface.

**Part 2**:

Then, we repeat exactly the same process with the cards with arabic numbers, except that the cards can not be ordered randomly in a pile. Instead, they are presented randomly laying on the table. Please note that the first 15 participants did not perform this task.


#### Data

The output is one .csv file per participant (containing the information for the dots free ordering cards, the arabic numbers ordering cards, and the linear dots ordering cards), which are combined into one big .csv file containing the following columns:

 - **participant id**
 - **type_experiment**: *dots_free* (part 1), *numbers_free* (part 2), *dots_line* (part 3)
 - **shape**: the shape of the cards for the free ordering experiments (part 1 and part 2)
 - **shape_plot**: same as above, but split into convenient categories for plotting
 - **order**: the numerosity order: *L* (for Left to right), *R* (for Right to left), *N* (for No order), *NC* (stimuli ordered but no order because of the shape form)
 - **order_all**: actual order for the cards placed by the participant (from 1 to 10)
 - **strategy**: notes taken by the experimenter
 - **strategy_plot**: same as below, but split into convenient categories for plotting
 - **order_placement**: how the participants placed the cards, independently of the numerosities on the cards
 - **orientation**: cards orientation: are the cards upside down?
 - **hand_used**: which hand was used to place the cards (*R*: right, *L*: left, *RL*: both hands)
 - **time**: only if the participant was particularly slow or fast to order the cards


### Experiment 3: Gaze Tracking

#### Introduction

This experiment is a replication of Di-Giorgio et al (2019). In this experiment, we aim to track the participant's gaze and see the side of the screen the participant prefers to look at when presented with big and small numerosities. The participant is passive and does not interact with the program.

#### Material

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python). The program simultaneously records a video of the participant's face and presents the stimuli. Thus, for each frame, it writes the stimulus location on the screen and the picture of the participant's eyes. 
 - **Gaze Tracking**: To know which side of the screen the participant has looked at, we use 3 different methods: 
     - A Python function, *cv2* (opencv) that automatically finds the x and y location of the eye center on the screen, as well as coordinates for the eyes.
     - A Deep Learning tracking method, *DeepLabCut* (originally used for tracking animals). First, the network is trained by manually labeling one the eye's pupil side on a few representative frames for each participant. Then, the program extracts the x and y location for the eye pupil side based on the trained network, which extract .
     - The relative position of the pupil inside the eye help helps determine the side of the screen the participant has looked at. Using machine learning techniques (Random Forest and SVM) and the data from the calibration, a model predicts which side of the screen the participant looks at based on the x and y locations of his/her eyes. 
     - To test the accuracy of these methods, the experimenters also manually tracks the side in *part 1* of this experiment (see below for more information). If the output from the 3 methods (cv2 and Deeplabcut) differ, the participant is excluded from the analysis. If the automatic methods are accurnte, the *part 2* of the experiment is not manually tracked.


#### Stimuli

We used exactly the same stimuli as Di-Giorgio et al (2019). The experiment is splitted into 2 parts, each of them preceded by a calibration phase:

 - **Part 1:** participants are habituated with two squares containing 12 dots each. Then, half of the participants are tested with two squares containing 4 and then 36, and half with 36 and 4. 
 - **Part 2:** participants are habituated with squares containing 4 or 36 dots, and then presented with a testing with 12 dots. The size of the perimeter is balanced between conditions.

##### Part 1

![*Illustration 3: Experiment 1. (a) Two squares containing 12 points are presented to the participant. (b) Two squares containing 4 points or 36 points will then be displayed (condition counterbalanced between participants). Our hypothesis maintains that the Himbas' gaze is preferably on the left square when presenting squares with 4 points, and preferentially on the right square when presenting with 36 points.*](pictures/exp1_num.jpg){width=50%}


##### Part 2

![*Illustration 4: Experiment 2. (a) Two squares containing 4 or 36 points are presented to the participant. (b) Next, two squares containing 12 points are presented, the total area of hich is counterbalanced with the previous presentation. Top: we hypothesize that the participant will preferentially look at the square on the right, because the numberity is greater than in (a). Bottom: the participant will preferentially look at the square on the left, because the numbers are lower than in (a).*](pictures/exp2_num.jpg){width=50%}


##### Dimensions

Size stimulus: 17 degrees

Distance between stimulus: 17 degrees

Duration flashing habituation stimuli: 0.3 seconds

Duration testing stimulus: 3 seconds

Duration white box after the testing stimulus: 1 second

Total duration of the experiment: **5 minutes** without the instructions and preparation


#### Procedure

The procedure is exactly the same for part 1 and 2.

**Calibration**. First, we need to calibrate the gaze tracking program to the participant. The instructions are: *“This experiment has 2 parts. First, you will see a white ball on the screen. It will move. You have to look at the ball. Please try not to move your head, but just your eyes”.* The program show two images of a ball moving.

**Testing**. *"Second, you will see two white boxes, and a cross in the middle. Look at the cross if there is one. When the cross disappears, you are free to look wherever you want on the screen."* The program goes on with 3 examples of testing pictures, so the participant understands well the instructions.

As the experiment can be a bit tiring for the eyes, the experimenter adds: *"the experiment last for 2 minutes. It is short, but if you feel that your eyes hurt, you are free to look elsewhere. If it is the case, please tell the experimenter".*

The experimenter plays the program for the first experiment. Part 1 and 2 are *not* consecutive, and the participant can take a break in between. 


#### Data

After the experiment, the program's output consists in 4 files for each participant:

 - 4 **.csv files** with the location of the stimulus for each frame (calibration_part1, testing_part1, calibration_part2, testing_part2)
 - 4 **videos** of the participant's face, containing same number of frames as there are rows in the .csv file (calibration_part1, testing_part1, calibration_part2, testing_part2)
 
After the videos and csv files are processed (see Procedure above), the output result is a csv file with the gaze direction on the screen (left, right, center, closed) for each participant. Then, these files are combined into one single file with the following columns name:

TO COMPLETE


### Experiment 4: Reaction times (VT, also called Newnewborn)

#### Introduction

This experiment measures reaction time and error rates. The participant is presented with increasing or decreasing numerosities appearing on the right or left side of the screen. We measure whether the participant is faster to detect decreasing numerosities on the left and increasing numerosities on the right. 

#### Material

For this experiment, the following material is used:

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python).
 - **Additional two-keys keyboard**: this keyboard is used by the participant to interact with the program.

##### Dimensions

Size stimulus: 17 degrees

Distance between stimulus: 17 degrees

##### Timing

Cross presentation: 1 sec
First numerosity (middle): 0.5 sec
In between numerosity timing: 0.2 sec
Second numerosity (side): 3 seconds if testing, 4 seconds if training

#### Procedure

The participant is first presented with a **training** phase. The training and the testing phase are exactly similar, except that the participant has more time to answer during the training.

Because many participants did not understand the experiment well, the experimenter and her supervisor  decided halfway of the experiment to add more additional time during the training phase. We believed it helped the participant as the mean rate of understand jumped from [TO COMPLETE] to [TO COMPLETE]. 

The numerosities presented are can be 4, 12 and 36 and are extracted from Di-Giorgio et al (2018) study (see Illustration 5 below):

![*Illustration 5: The stimuli used in Experiment 4.*](pictures/all_stim_reaction_time.jpg){width=70%}


The participants are splitted into 2 groups:

 - a group that has to press a button only when the numerosity **has decreased**. If the numerosity has increased, they do not press any button and wait for the second stimulus to disappear. 
 
 - a group that has to press a button only when the numerosity **has increased**. If the numerosity has decreased, they do not press any button and wait for the second stimulus to disappear. 
 
Both group has to use only one button using their most confortable hand. 

![*Illustration 5: (a) A cross is presented (1 second). (b) A numerosity (4, 12 or 36) is presented in the middle of the screen during 0.5 seconds to the participant, and is followed by a black screen (0.2 second). (c) Another numerosity is presented either on the left or the right side (counterbalanced). If the participant belongs to the "decreasing" group, s.he has to press only if the numerosity decreases, and vice versa. Otherwise, s.he just wait for the stimulus to disappear.*](pictures/reaction_time.jpg){width=50%}

Inside the **increasing condition**, the participant is presented with the following values:

 | Order | Type test | Amorce value | Testing value | Side  |
|------------|-------------|-------------|-------------| -------------|
 | 1|  training | 36 | 4 | left | 
 | 2|  training| 4 | 12 | left | 
 | 3|  training| 12 | 36 | right | 
 | 4|  training| 36 | 4 | right | 
 | 5|  training| 12 | 4 | left | 
 | 6|  training| 4 | 36 | left | 
 | 7|  training| 36 | 12 | right |
 | 8|  training| 4 | 12 | right |   
 | 9|  training| 12 | 36 | left | 
 | 10|  training| 12 | 4 | right |   
 | 11|  training| 4 | 36 | right |   
 | 12|  training| 36 | 12 | left | 
 | 13|  training| 4 | 12 | left | 
 | 14|  training| 36 | 12 | right |  
 | 15|  training| 4 | 36 | right | 
 | 16|  training| 36 | 4 | left | 
 | 17|  training| 12 | 4 | right | 
 | 18|  training| 4 | 36 | left |
 | 19 | testing | 12 | 36 | right | 
 | 20 | testing  | 12 | 4 | left | 
 | 21 | testing  | 4 | 36 | left | 
 | 22 | testing  | 36 | 4 | right | 
 | 23 | testing  | 12 | 36 | right | 
 | 24 | testing  | 4 | 36 | left | 
 | 25 | testing  | 12 | 4 | left | 
 | 26 | testing  | 12 | 36 | left | 
 | 27 | testing  | 4 | 36 | right | 
 | 28 | testing  | 36 | 4 | left | 
 | 29 | testing  | 4 | 12 | left | 
 | 30 | testing  | 4 | 36 | left | 
 | 31 | testing  | 36 | 4 | right | 
 | 32 | testing  | 4 | 12 | right | 
 | 33 | testing  | 12 | 36 | left | 
 | 34 | testing  | 12 | 4 | right | 
 | 35 | testing  | 4 | 36 | right | 
 | 36 | testing  | 4 | 12 | left | 
 | 37 | testing  | 12 | 36 | right | 
 | 38 | testing  | 12 | 36 | right | 
 | 39 | testing  | 4 | 36 | left | 
 | 40 | testing  | 36 | 4 | left | 
 | 41 | testing  | 4 | 12 | left | 
 | 42 | testing  | 12 | 36 | right | 
 | 43 | testing  | 4 | 12 | right | 
 | 44 | testing  | 12 | 36 | left | 
 | 45 | testing  | 12 | 4 | right | 
 | 46 | testing  | 4 | 36 | right | 
 | 47 | testing  | 4 | 12 | left | 
 | 48 | testing  | 36 | 12 | right | 
 | 49 | testing  | 4 | 12 | left | 
 | 50 | testing  | 4 | 12 | right | 
 | 51 | testing  | 36 | 12 | left | 
 | 52 | testing  | 12 | 36 | left | 
 | 53 | testing  | 4 | 36 | right | 
 | 54 | testing  | 36 | 12 | right | 
 | 55 | testing  | 36 | 12 | left | 
 | 56 | testing  | 36 | 4 | left | 
 | 57 | testing  | 12 | 4 | left | 
 | 58 | testing  | 4 | 36 | left | 
 | 59 | testing  | 36 | 4 | right | 
 | 60 | testing  | 4 | 12 | right | 
 | 61 | testing  | 12 | 36 | left | 
 | 62 | testing  | 12 | 4 | right | 
 | 63 | testing  | 4 | 36 | right | 
 | 64 | testing  | 36 | 12 | right | 
 | 65 | testing  | 36 | 12 | left | 
 | 66 | testing  | 4 | 12 | right | 
 
Inside the **decreasing condition**, the participant is presented with the following values:

 
 | Order | Type test | Amorce value | Testing value | Side  |
|------------|-------------|-------------|-------------| -------------|
    | 1 | training |  12 |4 | right |
    | 2 | training |  4 |12 | right |
    | 3 | training |  4 |36 | right |
    | 4 | training |  36 |4 | left |
    | 5 | training |  36 |12 | right |
    | 6 | training |  12 |4 | left |
    | 7 | training |  12 |36 | right |
    | 8 | training |  4 |36 | left |
    | 9 | training |  12 |36 | left |
    | 10 | training |  36 |4 | right |
    | 11 | training |  36 |12 | left |
    | 12 | training |  4 |12 | left |
    | 13 | training |  36 |12 | right |
    | 14 | training |  12 |4 | right |
    | 15 | training |  4 |36 | left |
    | 16 | training |  4 |36 | right |
    | 17 | training |  4 |12 | left |
    | 18 | training |  36 |4 | left |
    | 19 | testing |  12 |4 | right |
    | 20 | testing |  12 |36 | right |
    | 21 | testing |  4 |12 | left |
    | 22 | testing |  12 |4 | right |
    | 23 | testing |  4 |12 | right |
    | 24 | testing |  4 |36 | right |
    | 25 | testing |  36 |4 | left |
    | 26 | testing |  36 |4 | right |
    | 27 | testing |  36 |12 | left |
    | 28 | testing |  12 |4 | right |
    | 29 | testing |  36 |12 | right |
    | 30 | testing |  12 |4 | left |
    | 31 | testing |  12 |36 | right |
    | 32 | testing |  4 |36 | left |
    | 33 | testing |  12 |36 | left |
    | 34 | testing |  36 |4 | right |
    | 35 | testing |  4 |12 | right |
    | 36 | testing |  4 |36 | right |
    | 37 | testing |  36 |4 | left |
    | 38 | testing |  36 |12 | right |
    | 39 | testing |  12 |4 | left |
    | 40 | testing |  36 |12 | right |
    | 41 | testing |  36 |12 | left |
    | 42 | testing |  4 |36 | left |
    | 43 | testing |  12 |36 | left |
    | 44 | testing |  36 |4 | right |
    | 45 | testing |  36 |12 | left |
    | 46 | testing |  4 |36 | right |
    | 47 | testing |  36 |4 | left |
    | 48 | testing |  36 |12 | right |
    | 49 | testing |  12 |4 | left |
    | 50 | testing |  36 |4 | right |
    | 51 | testing |  12 |4 | left |
    | 52 | testing |  12 |36 | right |
    | 53 | testing |  4 |36 | left |
    | 54 | testing |  12 |36 | left |
    | 55 | testing |  36 |4 | right |
    | 56 | testing |  36 |12 | left |
    | 57 | testing |  4 |12 | left |
    | 58 | testing |  12 |4 | right |
    | 59 | testing |  36 |4 | left |
    | 60 | testing |  36 |4 | left |
    | 61 | testing |  12 |4 | right |
    | 62 | testing |  36 |12 | left |
    | 63 | testing |  36 |12 | right |
    | 64 | testing |  12 |4 | left |
    | 65 | testing |  4 |12 | left |
    | 66 | testing |  4 |12 | right | 



For the **increasing** condition, the participant sees a total of 66 presentations, including 18 for the training. Among the 48 left, the participant sees:

 - 15 increasing stimuli presented on the right;	
 - 15 increasing stimuli presented on the left;
 - 9 decreasing stimuli presented on the right;
 - 9 decreasing stimuli presented on the left
	
	
For the **decreasing** condition, the participant sees a total of 66 presentations, including 18 for the training. Among the 48 left, the participant sees:

 - 15 decreasing stimuli presented on the right;	
 - 15 decreasing stimuli presented on the left;
 - 9 increasing stimuli presented on the right;
 - 9 increasing stimuli presented on the left
 
Half of the participant had the **increasing** condition and the other half of the participants had the **decreasing** condition: it's a cross-subject experimental design. 


#### Data

Data for each participant is gathered into a single csv file, containing the following information: 
 
 - the **participant id**
 - the **amorce** numerosity (4, 12 or 36)
 - the **testing** numerosity (4, 12 or 36, always smaller or bigger than the amorce numerosity)
 - the **time** needed to press the button
 - the **response**: correct (1) or incorrect (0)
 - the **position**: left or right
 - the **condition**: decreasing or increasing
 - the **type** of test: training or testing
 - the **key pressed**: no key or the x key
 - the **situation**: whether the numerosity has increased (inc) or decreased (dec)



### Experiment 5: Basket

#### Introduction

This experiment is a replication NAME_STUDY (?). This is a computerized experiment aiming at understanding how participants map numerosities into space. 

#### Material

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python). This program is interactive but only the experimenter uses the mouse. The participant shows the answer with his/her finger on the screen.
 - the **experimenter sheet**, because the two conditions are counterbalanced between participants. 

#### Stimuli

The experiment breaks down into two stages: a *training* and a *testing* phase, which presentations successively alternate.

##### Training phase

In this phase, a **vertical** row of baskets is printed (see Figure below).


![*Illustration 6: Training phase. (a) Baskets, the interior of which is hidden. (b) The baskets are gradually uncovered, and a red ball is present in the 3rd (or 8th) basket. (c) The baskets are covered again, and the experimenter asks in which basket lies the red ball. The participant points to the basket, the experimenter clicks on this basket, and (d) the baskets are uncovered again with positive or negative feedback depending on the participant's response.*](pictures/basket_training.jpg){width=100%}

##### Testing phase

In this phase, a vertical row of baskets is printed (see Figure below), but after the baskets in the vertical row have uncovered, the participant interacts with the program on an **horizontal** row.

![*Illustration 7: Testing phase. (a) and (b): same steps as in the training. (c) the baskets are presented on a horizontal line. The participant points to where he thinks the red ball might lie, and the experimenter clicks on the basket he points to. No feedback is presented.*](pictures/basket_testing.jpg){width=80%}

##### Dimensions

The baskets uncover every 0.5 seconds. The total size of the line is 16 cm which corresponds to the height of the monitor screen (16.875 cm). (Note: the program automatically adapts the size of the line according to the size of the screen).

When the ball is at location 3 and 8: 

 - 3 training, followed by 2 testing
 - 1 training, followed by 2 testing
 - 1 training, followed by 2 testing, 
 - 1 training, followed by 2 testing.

There are 6 trainings and 8 testings per location (ball at location 3 or 8). So in total, there are **16 testing stimuli** during the whole experiment.

#### Procedure

At the beginning of the experiment, a window opens and asks for the participant’s name and date. It will write the output file using this information, with the same type of format as for the personal information.

Then, the experimenter gives very basic instructions: *“You see on the screen many baskets. One of these baskets contains a red ball. The baskets are now hidden, but they will open, and you will see in which basket the red ball is. During the experiment, we will ask you where you think the red ball is. Sometimes there will be a feedback, sometimes not.”*

Then, after each training and each testing, the experimenter asks: *“under which basket is the red ball? show us on the screen with your finger”*

The duration of this experiment is around **4:30 min** without the instructions and preparation.


#### Data analysis

The program produces a .csv file output which contains the following information: 

 - **basket number** designated by the participant in the training and test phase
 - **phase** (training or testing) 
 - the associated **response time**
 
### Experiment 6: Ordering cards (part 3)

#### Introduction

This is the third part of the experiment 2 explained above.

#### Material

- The same 10 round cards mentioned above
- A horizontal linear printed paper where participant have to place the cards (see Illustration below)

![*Illustration 8: Stimuli*](pictures/stimuli_to_print.jpg)

#### Procedure

The cards are ordered randomly in a pile on a table. The experimenter asks the participant : *“order the cards in a way that looks nice. This time, you have to place them on this horizontal line”*. When the participant is ready, the experimenter starts the clock. When the participant says that he is done, we record the time, and take a picture of the disposition of the cards on the table.


### Experiment 7: Number line (logarithmic or linear)

#### Introduction

This experiment is an exact replication from Dehaene et al (2008). However, we use only cards with **dots** using more trials.


#### Material

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python). This program is interactive but only the experimenter uses the mouse. The participant shows the answer with his/her finger on the screen. This program uses exactly the same testing stimuli as for the ordering cards experiment (experiment 2 and 6).
 
 
#### Stimuli

Two fixed stimuli are placed at the end of the white segments. These reference stimuli were matched on total intensity and occupied area, but varied in item size and inter-item (see Figure below). For more information about how the disposition of the dots are controlled, please refer to Dehaene et al (2008)'s paper.

The left and right reference stimuli as well as the line stay identical during the whole experiment. Only the target stimulus (that appears centered on the bottom of the line) changes. This target stimulus is extracted from the different sets of dots.

![*Illustration 9: Experimental scheme. This picture was extracted from Dehaene et al (2008) paper, as this experiment is an exact replication.*](pictures/dehaene.jpg)


##### Training phase

The experiment starts with 2 training presentations. In this training, sets of dots whose numerosities corresponded to the ends of the scale (**1 and 10**) are presented. The participant is told that these two stimuli belonged to their respective ends, but that other stimuli could be placed at any location. The experimenter shows the location with her fingers.

Because no participants understood the instructions this way, we decided to let the translator free to add more explaination if the participant was answering completely randomly.

##### Testing phase

Then, in the testing phase, the participant is presented with the sets of dots shown in the Figure above (**numerosities 1-10**). Each set of dots is presented three times, so in total, 30 sets are presented to the participant. It is higher than inside the experiment from Dehaene et al (2008), where sets of dots are presented only twice.

##### Dimensions

The dimensions of the experiment had to be adapted to the size of the screen. Thus, the size horizontal segment is 20 cm and the size of the stimuli are 4 cm.

#### Procedure

The first training trial appears (1 dot) and the participant is told: *"this stimulus corresponds to the left end of the scale"* (the experimenter shows with her finger on the screen). The second training trial appears (10 dots) and the participant is told: *"this stimulus corresponds to the right end of the scale."* 

Then, the experimenter tells the participant *“The two stimuli you have seen belong to their respective end. But other stimuli could be placed at any location on this line. For each new stimulus, please indicate with your finger where you think is the appropriate position on the line”.* (the experimenter moves her finger on the line).


#### Data analysis

The program opens a .csv file name with the participant number and the date, and contains the following information:

 - Numerosity of the testing set
 - location on the x axis designated by the participant 
 - Associated time
 
These csv file are gathered inside a big csv file.

## Himbas 2022


The experimental protocol consists in 6 distinct experiments. 

| Experiment number | Experiment name | Short description | Type of experiment | Counterbalancement |
|------------|-------------|-------------|-------------|-------------|
| 1 | Ordering cards (part 1 and part 1bis) | ordering numerosities (part 1) and colored cards (part 1 bis) without specific instructions | Ecological (no technology involved) | no counterbalancement |
| 2 | VT experiment (previously called *newnewborn* or *Reaction times*) | same experiment as with Himbas 2021 | On the computer  | *decreasing* or *increasing* group |
| 3 | Luminance experiment | new experiment, participant should press if a square is lighter or darker than the background | On the computer  | *lighter* or *darker* group |
| 4 | Squares experiment | new experiment, participant should press if the numerosity (composed of squares) is big or small | On the computer  | *small* or *big* group (also called decreasing for small or increasing for big) |
| 5 | Valence experiment (Snakes and candies) | new experiment, participant should press if the numerosity (composed of candies or snakes) is big or small | On the computer  | *small* or *big* group (also called decreasing for small or increasing for big) |
| 6 | GazeTracking (from Di-Giorgio et al, 2019) | passive presentation of changing numerosities, and gaze tracking | On the computer  | *small* or *big* group (also called decreasing for small or increasing for big) |
| 7 | GazeTracking (from Di-Giorgio et al, 2019) | same as before with other group | On the computer  | *small* or *big* group (also called decreasing for small or increasing for big) |
| 8 | VT experiment | same as before with other group  |  | *decreasing* or *increasing* group |
| 9 | Luminance experiment | same as before with other group | On the computer  | *lighter* or *darker* group |
| 10 | Squares experiment | same as before with other group | On the computer  | *small* or *big* group (also called decreasing for small or increasing for big) |
| 11 | Valence experiment (Snakes and candies) | same as before with other group | On the computer  | *small* or *big* group (also called decreasing for small or increasing for big) |
| 12 | Ordering cards (part 2) | ordering cards with digits without specific instructions | Ecological (no technology involved) | no counterbalancement |

There are two possibilities:
 
 - half of the participant starts with the *decreasing* condition for experiment 2, 3, 4, 5, 6, and then they have the exact same experiments but with the *increasing* condition in experiments 7, 8, 9, 10, 11
  - half of the participant starts with the *increasing* condition for experiment 2, 3, 4, 5, 6, and then they have the exact same experiments but with the *decreasing* condition in experiments 7, 8, 9, 10, 11
  
At the end, all experiments are within groups, only the order is counterbalanced.

This time, the experimenter (Mathilde Josserand) was *not* present all the time for each experiment, but was often checking the experiments. The experiments were translated and guided by the same translator as in 2021 (Fanny Kavari). Serge Caparos was also there to supervise and control the experimental settings. 

### Demographic data

The questions are sensibly the same as with Himbas 2021, but there are a few differences. For examples, more questions were asked regarding the exposure to the modern world (see [Knowledge and attractivity to modern life]). 

#### Questions

Using an Eprime interface , the experimenter collected some of the participant's personal information:

```{r , echo=FALSE, message=FALSE, warning=FALSE}

questionnaire <- read.csv("C:/Users/Mathilde JOSSERAND/Desktop/Spanumbra/Explain/Questions_Questionnaires.csv", header=TRUE, sep=";", stringsAsFactors = TRUE)

questionnaire$Responses <- paste(questionnaire$Responses.choice, questionnaire$X, questionnaire$X.1, questionnaire$X.2, questionnaire$X.3, questionnaire$X.4, questionnaire$X.5, questionnaire$X.6, questionnaire$X.7, questionnaire$X.8)

kable(questionnaire[,c(1,2, ncol(questionnaire))])
```

These data are combined with the anonymous ID number (chosen by the experimenter at the beginning of the experiments), and the date and time (automatically computed). All participants personal information is gathered in a csv file.

#### Numerical abilities

We tested the numerical abilities of Himbas using a range of different experiments:



```{r , echo=FALSE, message=FALSE, warning=FALSE}

numericalabilities <- read.csv("C:/Users/Mathilde JOSSERAND/Desktop/Spanumbra/Explain/Questions_NumericalAbilities.csv", header=TRUE, sep=";", stringsAsFactors = TRUE)

kable(numericalabilities)
```

A correct answer is coded as "1" while an uncorrect answer is coded as "0". The mean answer of all these questions is then calculated: we call this measure the **global numerical score**.

#### Knowledge and attractivity to modern life

##### UrbanIndex

To examine how attracted an individual is from the modern city, we use 3 indexes:

 - **UrbanIndex**: this index is computed using:
    - the *outfit* score: as a reminder, traditionnal is 1, modern is 0. We do the following computation: 3-OutfitScore * 3 to obtain the *NewOutfitScore*
    - the *CultureChangeWorry* question: please check the table above to know exactly what is this question and the different possible answer. We do the following computation: 3-CultureChangeWorryScore = *NewCultureChangeWorryScore*
    - the *LikeOpuwo* score: see above for more information on this variable. No additional computation were performed on this one, so *LikeOpuwoScore* = *NewLikeOpuwoScore*
    - the *Telephone* score. As a reminder, yes is 1, and no is 0. We do the following computation: *TelephoneScore* * 3 = *NewTelephoneScore*
    
Thus, **UrbanIndex** is the mean of *NewOutfitScore*, *NewCultureChangeWorryScore*, *NewLikeOpuwoScore*, *NewTelephoneScore*.

##### UrbanPref and UrbanKnow

However, we also added another experiment. In this experiment, we present two pictures to the participant. These pictures show two alternatives of a situation, either traditional, either modern. Then, the experimenter :

 - 1. (**UrbanKnow**) asks the participant to describe what is on the picture. If the participant knows what is on the picture, the experimenter types "1" (the participant has the knowledge of this specific modern thing). If the participant does *not* know what is on the picture, the experiment types 0. 
 
 - 2. (**UrbanPref**) asks the participant what situation he chooses (1 for traditional and 0 for modern). It is supposed to reflect the preference for modern versus traditionnal life. 
 
The following stimuli are presented here:

 - a cattle *versus* money
 - a donkey *versus* a car
 - Himba jewelry *versus* modern jewelry
 - a Himba woman *versus* a modern style woman
 - a Himba man washing himself in a river *versus* a man showering in a shower
 - Himba woman hairstyle *versus* other Afro hairstyle
 - Himba man hairstyle *versus* other Afro hairstyle
 - Goats *versus* money
 - traditional Himba village *versus* Opuwo
 
The pictures are presented side by side, their order as well as their position (left or right) is randomized.
Answers for all these questions, for both **UrbanKnow** and **UrbanPref**, are averaged for each participant. 

##### Conclusion on modern life indexes

Thus, we use 3 variables:

 - **UrbanIndex** is supposed to reflect how modern an individual is in his daily life;
 - **UrbanKnow** is supposed to reflect how much an individual knows about modern life
 - **UrbanPref** is supposed to reflect how much an individual prefers modern life


### Experiment 1: Ordering cards (part 1 only)

This experiment is the same as with Himbas 2021 (but we do not have part 3 in 2022 but we have another part, that we call **part 1 bis**). 

This experiment aims at studying how participants order numerosities. It is split in three different parts:

 - First, we observe whether a bias to form a line exists. Thus, we will leave the participant **free to order the cards** (with **dots**) the way s.he wants, as soon as the cards lay on the table (*part 1*).
 - Second, we observe whether there is a bias to form a line with coloured cards of different luminance (ranging from black to white) (*part 1 bis*).
 - Third, we give the same instruction but with cards with **arabic numbers** (*part 2*);

The **first** and **second** part occurs at different moments of the experiments.

#### Material

For this experiment, the following material is used:

 - **10 round cards** of diameter 3.2 cm, with a white background and black dots. The location of the dots is scrambled and extracted from Dehaene et al (2008)’s paper. The cards are printed on normal paper, then pasted on cardboard round pieces (bought online).
 
 - **10 round cards** of diameter 3.2 cm, with different colors. The colors were extracted using [this website](https://gka.github.io/palettes/#/6|s|000000,ffffff|ffffe0,ff005e,93003a|1|1), which allows to control for differences in luminance. The two input colors were black [000000] and white [ffffff], and we created six colors. The cards are printed on normal paper, then pasted on cardboard round pieces (bought online). Please note that the printer was *not* calibrated.

![*Illustration N: Finding different luminances*](pictures/ColorPalette.png)
![*Illustration 2: Colored Cards Stimuli*](pictures/ColorCards.jpg){width=30%}

 - **10 round cards** of diameter 3 cm. These are wood slices (bought online) on which I performed pyrography to print the arabic numbers.

![*Illustration 2: Digits cards*](pictures/DigitsOrdered.jpg){width=70%}

 - **a phone** with a camera (for this experiment, a Xiaomi Redmi Note Pro 9 is used)
 

#### Procedure

**Part 1**:

The cards are ordered **randomly in a pile** on the table. The experimenter asks the participant : *“please order the cards. You can place them the way you want, but they all have to lay on the table”* (these instructions are translated by the translator prior to the experiment). When the participant says that he is done, The experimenter places a little card indicating the participant number on the table and takes a picture of the configuration. She also manually inserts the data and her comments into the Python gui interface.

**Part 1 bis**:

Same procedure as for part 1.

**Part 2**:

Same procedure as for part 1.


#### Data

The output is one .csv file per participant (containing the information for the dots free ordering cards, the arabic numbers ordering cards), which are combined into one big .csv file containing the following columns:

 - **participant id**
 - **type_experiment**: *dots_free* (part 1), *col_free* (part 1 bis) *numbers_free* (part 2)
 - **shape**: the shape of the cards for the free ordering experiments (part 1 and part 2)
 - **order**: the numerosity order: *L* (for Left to right), *R* (for Right to left), *N* (for No order or impossible to say because of non-horizontal linear shape form); in the case of color, this is *B* (black to white), *W* (white to black), or *N* (no order)
 - **order_all**: actual order for the cards placed by the participant (from 1 to 10); in the case of colors, 1 is Black and 6 is White
 - **hand_used**: which hand was used to place the cards (*R*: right, *L*: left, *RL*: both hands)
 - **strategy**: this part is a bit different from Himbas 2021. It is mostly used for taking notes (if specific conditions) and not record the strategy (it has not effect previously) 

### Experiment 2: Newnewborn (also called VT)

This experiment is the same as the one used with Himbas in 2021. The only difference is that the training is slightly shorter, but there is a possibility to restart the training if the experimenter feels that the participant has not understood.

Also, another change is that the experiment is now **within-subject**: all participants have both conditions (increasing and decreasing)

Finally, we used another keyboard which has only one button (see picture below)

![*Illustration 2: Digits cards*](pictures/OneKey_Keyboard.jpg){width=20%}


#### Introduction

This experiment measures reaction time and error rates. The participant is presented with increasing or decreasing numerosities appearing on the right or left side of the screen. We measure whether the participant is faster to detect decreasing numerosities on the left and increasing numerosities on the right. 

#### Material

For this experiment, the following material is used:

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python).
 - **Additional one key keyboard**: this keyboard is used by the participant to interact with the program.

##### Dimensions

Size stimulus: 17 degrees

Distance between stimulus: 17 degrees


##### Timing

Cross presentation: 1 sec
First numerosity (middle): 0.5 sec
In between numerosity timing: 0.2 sec
Second numerosity (side): 3 seconds if testing, 4 seconds if training

#### Procedure

The participant is first presented with a **training** phase. The training and the testing phase are exactly similar, except that the participant has more time to answer during the training. Also, the training phase is started with a click of the mouse for every image for the first stimuli, so that the experiment have time to explain everything.

The numerosities presented are can be 4, 12 and 36 and are extracted from Di-Giorgio et al (2018) study (see Illustration 5 below):

![*Illustration N: The stimuli used in Experiment 4.*](pictures/all_stim_reaction_time.jpg){width=70%}


Each participant do the two conditions (which we will call **group** here, for more clarity given the terminology used for the experiments in 2021):

 - pressing a button only when the numerosity **has decreased**. If the numerosity has increased, no button should be pressed and the participant should wait for the second stimulus to disappear. 
 
  - pressing a button only when the numerosity **has increased**. If the numerosity has decreased, no button should be pressed and the participant should wait for the second stimulus to disappear. 


Participants could use their dominant hand (recorded in the questionnaire), always using their thumb.

![*Illustration 5: (a) A cross is presented (1 second). (b) A numerosity (4, 12 or 36) is presented in the middle of the screen during 0.5 seconds to the participant, and is followed by a black screen (0.2 second). (c) Another numerosity is presented either on the left or the right side (counterbalanced). If the participant belongs to the "decreasing" group, s.he has to press only if the numerosity decreases, and vice versa. Otherwise, s.he just wait for the stimulus to disappear.*](pictures/reaction_time.jpg){width=50%}

Inside the **increasing condition**, the participant is presented with the following values:

 | Order | Type test | Amorce value | Testing value | Side  |
|------------|-------------|-------------|-------------| -------------|
 | 1 |  training | 36 | 4 | left | 
 | 2 |  training | 4 | 12 | left | 
 | 3 |  training | 12 | 36 | right | 
 | 4 |  training | 36 | 4 | right | 
 | 5 |  training | 12 | 4 | left | 
 | 6 |  training | 4 | 36 | left | 
 | 7 |  training | 36 | 12 | right |
 | 8 |  training | 4 | 12 | right |   
 | 9 |  training | 12 | 36 | left | 
 | 10 |  training | 12 | 4 | right |   
 | 11 |  training | 4 | 36 | right |   
 | 12 |  training | 36 | 12 | left | 
 | 13 | testing | 12 | 36 | right | 
 | 14 | testing  | 12 | 4 | left | 
 | 15 | testing  | 4 | 36 | left | 
 | 16 | testing  | 36 | 4 | right | 
 | 17 | testing  | 12 | 36 | right | 
 | 18 | testing  | 4 | 36 | left | 
 | 19 | testing  | 12 | 4 | left | 
 | 20 | testing  | 12 | 36 | left | 
 | 21 | testing  | 4 | 36 | right | 
 | 22 | testing  | 36 | 4 | left | 
 | 23 | testing  | 4 | 12 | left | 
 | 24 | testing  | 4 | 36 | left | 
 | 25 | testing  | 36 | 4 | right | 
 | 26 | testing  | 4 | 12 | right | 
 | 27 | testing  | 12 | 36 | left | 
 | 28 | testing  | 12 | 4 | right | 
 | 29 | testing  | 4 | 36 | right | 
 | 30 | testing  | 4 | 12 | left | 
 | 31 | testing  | 12 | 36 | right | 
 | 32 | testing  | 12 | 36 | right | 
 | 33 | testing  | 4 | 36 | left | 
 | 34 | testing  | 36 | 4 | left | 
 | 35 | testing  | 4 | 12 | left | 
 | 36 | testing  | 12 | 36 | right | 
 | 37 | testing  | 4 | 12 | right | 
 | 38 | testing  | 12 | 36 | left | 
 | 39 | testing  | 12 | 4 | right | 
 | 40 | testing  | 4 | 36 | right | 
 | 41 | testing  | 4 | 12 | left | 
 | 42 | testing  | 36 | 12 | right | 
 | 43 | testing  | 4 | 12 | left | 
 | 44 | testing  | 4 | 12 | right | 
 | 45 | testing  | 36 | 12 | left | 
 | 46 | testing  | 12 | 36 | left | 
 | 47 | testing  | 4 | 36 | right | 
 | 48 | testing  | 36 | 12 | right | 
 | 49 | testing  | 36 | 12 | left | 
 | 50 | testing  | 36 | 4 | left | 
 | 51 | testing  | 12 | 4 | left | 
 | 52 | testing  | 4 | 36 | left | 
 | 53 | testing  | 36 | 4 | right | 
 | 54 | testing  | 4 | 12 | right | 
 | 55 | testing  | 12 | 36 | left | 
 | 56 | testing  | 12 | 4 | right | 
 | 57 | testing  | 4 | 36 | right | 
 | 58 | testing  | 36 | 12 | right | 
 | 59 | testing  | 36 | 12 | left | 
 | 60 | testing  | 4 | 12 | right | 
 
Inside the **decreasing condition**, the participant is presented with the following values:

 
 | Order | Type test | Amorce value | Testing value | Side  |
|------------|-------------|-------------|-------------| -------------|
    | 1 | training |  12 |4 | right |
    | 2 | training |  4 |12 | right |
    | 3 | training |  4 |36 | right |
    | 4 | training |  36 |4 | left |
    | 5 | training |  36 |12 | right |
    | 6 | training |  12 |4 | left |
    | 7 | training |  12 |36 | right |
    | 8 | training |  4 |36 | left |
    | 9 | training |  12 |36 | left |
    | 10 | training |  36 |4 | right |
    | 11 | training |  36 |12 | left |
    | 12 | training |  4 |12 | left |
    | 13 | testing |  12 |4 | right |
    | 14 | testing |  12 |36 | right |
    | 15 | testing |  4 |12 | left |
    | 16 | testing |  12 |4 | right |
    | 17 | testing |  4 |12 | right |
    | 18 | testing |  4 |36 | right |
    | 19 | testing |  36 |4 | left |
    | 20 | testing |  36 |4 | right |
    | 21 | testing |  36 |12 | left |
    | 22 | testing |  12 |4 | right |
    | 23 | testing |  36 |12 | right |
    | 24 | testing |  12 |4 | left |
    | 25 | testing |  12 |36 | right |
    | 26 | testing |  4 |36 | left |
    | 27 | testing |  12 |36 | left |
    | 28 | testing |  36 |4 | right |
    | 29 | testing |  4 |12 | right |
    | 30 | testing |  4 |36 | right |
    | 31 | testing |  36 |4 | left |
    | 32 | testing |  36 |12 | right |
    | 33 | testing |  12 |4 | left |
    | 34 | testing |  36 |12 | right |
    | 35 | testing |  36 |12 | left |
    | 36 | testing |  4 |36 | left |
    | 37 | testing |  12 |36 | left |
    | 38 | testing |  36 |4 | right |
    | 39 | testing |  36 |12 | left |
    | 40 | testing |  4 |36 | right |
    | 41 | testing |  36 |4 | left |
    | 42 | testing |  36 |12 | right |
    | 43 | testing |  12 |4 | left |
    | 44 | testing |  36 |4 | right |
    | 45 | testing |  12 |4 | left |
    | 46 | testing |  12 |36 | right |
    | 47 | testing |  4 |36 | left |
    | 48 | testing |  12 |36 | left |
    | 49 | testing |  36 |4 | right |
    | 50 | testing |  36 |12 | left |
    | 51 | testing |  4 |12 | left |
    | 52 | testing |  12 |4 | right |
    | 53 | testing |  36 |4 | left |
    | 54 | testing |  36 |4 | left |
    | 55 | testing |  12 |4 | right |
    | 56 | testing |  36 |12 | left |
    | 57 | testing |  36 |12 | right |
    | 58 | testing |  12 |4 | left |
    | 59 | testing |  4 |12 | left |
    | 60 | testing |  4 |12 | right | 


For the **increasing** condition, the participant sees a total of 60 presentations, including 12 for the training. Among the 48 left, the participant sees:

 - 15 increasing stimuli presented on the right;	
 - 15 increasing stimuli presented on the left;
 - 9 decreasing stimuli presented on the right;
 - 9 decreasing stimuli presented on the left
	
	
For the **decreasing** condition, the participant sees a total of 60 presentations, including 12 for the training. Among the 48 left, the participant sees:

 - 15 decreasing stimuli presented on the right;	
 - 15 decreasing stimuli presented on the left;
 - 9 increasing stimuli presented on the right;
 - 9 increasing stimuli presented on the left
 
Contrary to the experiment in 2021 (where half of the participant had the **increasing** condition and the other half of the participants had the **decreasing** condition in cross-subject experimental), in 2022, all participant have both the increasing **and** decreasing conditions: it is a **within-subject** design.


#### Data

Data for each participant is gathered into a single csv file, containing the following information: 
 
 - the **participant id**
 - the **amorce** numerosity (4, 12 or 36)
 - the **testing** numerosity (4, 12 or 36, always smaller or bigger than the amorce numerosity)
 - the **time** needed to press the button
 - the **response**: correct (1) or incorrect (0)
 - the **position**: left or right
 - the **condition**: decreasing or increasing
 - the **type** of test: training or testing
 - the **key pressed**: no key or the x key
 - the **situation**: whether the numerosity has increased (inc) or decreased (dec)


### Experiment 3: Squares

This experiment is very similar to the one before. There are three main differences:

 - now, the first numerosity that appears in the center is flashing (0.2 seconds) and always has 12 dots
 - then, the instructions are different: now, the two conditions (that we will call *groups* too, for more clarity with previous 2021 experiments) are "small" and "big". When the condition (*group*) is *big*, the participant should press if the second numerosity (that appears either on the right or on the left) is big (do nothing if small), and vice versa
 - finally, all stimuli are different: now a specific number of stimuli containing 4, 12 and 36 dots have been generated: see Illustration below (EXPLAIN METHOD HERE)


Tthe experiment is now **within-subject**: all participants have both conditions (*group*) (big and small)

Finally, we used another keyboard which has only one button (see picture below). 
Please note that because a 12 dots stimulus is flashing, we can consider that the *small* condition is equivalent to a *decreasing* condition, while the *big* condition can be equivalent to an *increasing* condition.

![*Illustration 2: Digits cards*](pictures/OneKey_Keyboard.jpg){width=20%}


#### Introduction

This experiment measures reaction time and error rates. The participant is presented with big or small numerosities appearing on the right or left side of the screen. We measure whether the participant is faster to detect small numerosities (*decreasing* numerosity) on the left and big numerosities (*increasing* numerosities) on the right. 

#### Material

For this experiment, the following material is used:

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python).
 - **Additional one key keyboard**: this keyboard is used by the participant to interact with the program.

##### Dimensions

Size stimulus: 17 degrees

Distance between stimulus: 17 degrees

##### Timing

Cross presentation: 0.5 sec
First flashing numerosity (12 dots, centered): 0.2 sec
In between numerosity: 0.2 sec
Second numerosity (4 or 36, side): 3 sec (for both testing and training)

#### Procedure

The participant is first presented with a **training** phase. The training and the testing phase are exactly similar, except that the participant has more time to answer during the training.

The numerosities presented are can be 4, 12 and 36 and are **different** from Di-Giorgio et al (2018) study (see Illustration 5 below):

![*Illustration N: The stimuli used in Experiment 4. a. Three examples of the "4" stimuli. b. Three examples of the "12"stimuli. c. Three examples of the "36" stimuli. Note that we generated enough stimuli for each presentation to be unique.* ](pictures/SquaresStim.jpg){width=70%}


Each participant do the two conditions (which we will call **group** here, for more clarity given the terminology used for the experiments in 2021):

 - pressing a button only when the numerosity is **small**. If the numerosity is big, no button should be pressed and the participant should wait for the second stimulus to disappear. 
 
  - pressing a button only when the numerosity is **big**. If the numerosity is small, no button should be pressed and the participant should wait for the second stimulus to disappear. 


Participants could use their dominant hand (recorded in the questionnaire), always using their thumb.

![*Illustration 5: (a) A cross is presented (0.5 second). (b) A 12 numerosity is flashed in the middle of the screen (0.2 seconds), and is followed by a black screen (0.2 second). (c) Another numerosity is presented either on the left or the right side (counterbalanced). If the participant belongs to the "big" group, s.he has to press only if the numerosity decreases, and vice versa. Otherwise, s.he just wait for the stimulus to disappear.*](pictures/Square_exp.jpg){width=90%}

Inside the **small condition**, the participant is presented with the following values:

| Order | Type test | position | Second numerosity printed |
|------------|-------------|-------------|-------------|
| 1 | training | right | 36 |
| 2 | training | right | 4 |
| 3 | training | left | 36 |
| 4 | training | right | 36 |
| 5 | training | right | 4 |
| 6 | training | left | 4 |
| 7 | training | left | 36 |
| 8 | testing | right | 4 |
| 9 | testing | left | 4 |
| 10 | testing | right | 36 |
| 11 | testing | right | 4 |
| 12 | testing | right | 4 |
| 13 | testing | left | 4 |
| 14 | testing | right | 4 |
| 15 | testing | right | 36 |
| 16 | testing | right | 36 |
| 17 | testing | left | 4 |
| 18 | testing | left | 36 |
| 19 | testing | left | 4 |
| 20 | testing | left | 36 |
| 21 | testing | left | 4 |
| 22 | testing | left | 4 |
| 23 | testing | right | 4 |
| 24 | testing | left | 36 |
| 25 | testing | right | 4 |
| 26 | testing | left | 4 |
| 27 | testing | right | 36 |
| 28 | testing | right | 4 |
| 29 | testing | left | 36 |
| 30 | testing | right | 4 |
| 31 | testing | left | 4 |
 
 
Inside the **big condition**, the participant is presented with the following values:

 
| Order | Type test | Side | Second numerosity printed |
|------------|-------------|-------------|-------------|
| 1 | training | right | 36 | 
| 2 | training | right | 4 |
| 3 | training | left | 36 |
| 4 | | training | right | 36 |
| 5 | training | right | 4 |
| 6 | training | left | 4 |
| 7 | training | left | 36 |
| 8 | testing | left | 36 |
| 9 | testing | right | 36 |
| 10 | testing | right | 4 |
| 11 | testing | right | 36 |
| 12 | testing | left | 36 |
| 13 | testing | left | 36 |
| 14 | testing | left | 36 |
| 15 | testing | left | 4 |
| 16 | testing | left | 4 |
| 17 | testing | right | 36 |
| 18 | testing | right | 36 |
| 19 | testing | left | 4 |
| 20 | testing | right | 4 |
| 21 | testing | left | 36 |
| 22 | testing | left | 36 |
| 23 | testing | right | 36 |
| 24 | testing | right | 4 |
| 25 | testing | left | 4 |
| 26 | testing | left | 36 |
| 27 | testing | right | 36 |
| 28 | testing | left | 36 |
| 29 | testing | right | 36 |
| 30 | testing | right | 36 |
| 31 | testing | right | 4 |

For the **small** condition, the participant sees a total of 31 presentations, including 7 for the training. Among the 24 left, the participant sees:

 - 8 small stimuli presented on the right;	
 - 8 small stimuli presented on the left;
 - 4 big stimuli presented on the right;
 - 4 big stimuli presented on the left
	
	
For the **big** condition, the participant sees a total of 60 presentations, including 7 for the training. Among the 24 left, the participant sees:

 - 8 big stimuli presented on the right;	
 - 8 big stimuli presented on the left;
 - 4 small stimuli presented on the right;
 - 4 small stimuli presented on the left
 
All participant have both the small **and** big conditions: it is a **within-subject** design.


#### Data

Data for each participant is gathered into a single csv file, containing the following information: 
 
 - the **participant id**
 - the **testing** numerosity (4 or 36, always smaller or bigger than the amorce numerosity)
 - the **time** needed to press the button
 - the **response**: correct (1) or incorrect (0)
 - the **position**: left or right
 - the **group**: small or big
 - the **type** of test: training or testing
 - the **key pressed**: no key or the x key



### Experiment 4: Luminance

This experiment is very similar to the experiment 3, except that here, the participant is presented with grey squares on a grey background and they should press not for "big" or "small", but for "darker" or "lighter" (color of the square compared to the color of the background).


#### Introduction

This experiment measures reaction time and error rates. The participant is presented with dark or light colors appearing on the right or left side of the screen. We measure whether the participant is faster to detect dark numerosities (*decreasing* numerosity) on the left and big numerosities (*increasing* numerosities) on the right. 
This experiment is inspired by the study "Space-luminance crossmodal correspondences in domestic chicks" (Loconsole et al., 2021).

#### Material

For this experiment, the following material is used:

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python).
 - **Additional one-key keyboard**: this keyboard is used by the participant to interact with the program.

##### Dimensions

Size stimulus: 6 degrees

Distance between stimulus: 17 degrees

##### Timing

Cross presentation: 0.5 sec

First coloured square (grey1, grey2, grey4, or grey5, side): 3 sec (for both testing and training)

#### Procedure

The participant is first presented with a **training** phase. The training and the testing phase are exactly similar, except that the participant has more time to answer during the training. 

The background colour is *grey3*. 

The coloured squares presented on the sides can be of colour *grey1* or *grey2* (**darker**) or *grey4* or *grey5* (**lighter**). The coloured were obtained using the same website as for the ordering cards: see  [here](https://gka.github.io/palettes/#/7|s|000000,ffffff|ffffe0,ff005e,93003a|1|1), which allows to control for differences in luminance. The two input colors were black [000000] and white [ffffff], and we created seven colors (see Illustration below).

![*Illustration N: The colours used for the luminance experiment. The first colour, black, is not used in this experiment. The two next colours (grey1 are grey2) are two colours used for the coloured square (darker colours). The fourth colour (grey3) is the colour of the background. The fifth and sixth colour (grey4 and grey5) are also used for the coloured squares (lighter). Finally, the white colour is not used.* ](pictures/ColorPalette_LuminanceExp.png){width=70%}

Please note that the number of stimuli of *grey1*, *grey2*, *grey4*, and *grey5* are equal.


Each participant do the two conditions (which we will call **group** here, for more clarity given the terminology used for the experiments in 2021):

 - pressing a button only when the coloured square is **lighter**. If the coloured square is darker, no button should be pressed and the participant should wait for the second stimulus to disappear. 
 
  - pressing a button only when the coloured square is **darker**. If the coloured square is lighter, no button should be pressed and the participant should wait for the second stimulus to disappear. 


Participants could use their dominant hand (recorded in the questionnaire), always using their thumb.

![*Illustration 5: (a) A cross is presented (0.5 second). (b) A coloured square is presented either on the left or the right side (counterbalanced). If the participant belongs to the "darker" group, s.he has to press only if the coloured sqaure is darker than the background, and vice versa. Otherwise, s.he just wait for the stimulus to disappear. The coloured square can be either darker (grey1 or grey2) or lighter (grey4 or grey5); in this picture, the color of the coloured square is grey2. The coloured of the background is always grey3.*](pictures/LuminanceExp.jpg){width=50%}

In the code, for more simplicity, we call the colors with the following name;

 - **b1** ("black1") refers to *grey2*, which refer to the color #292929
 - **b2** ("black2") refers to *grey3*, which refer to the color #4e4e4e
 - **w2** ("white2") refers to *grey4*, which refer to the color #a2a2a2
 - **w1** ("white1") refers to *grey5*, which refer to the color #d0d0d0


Inside the **lighter condition**, the participant is presented with the following values:

| Order | Type test | Side | Type Coloured Squares |
| ------- | ------ | ------ | --------------- |
| 1 | training | left | b2 |
| 2 | training | left | w1 |
| 3 | training | right | w1 |
| 4 | training | right | w2 |
| 5 | training | left | b1 |
| 6 | training | right | b1 |
| 7 | training | right | b2 |
| 8 | testing | left | w2 |
| 9 | testing | right | w2 |
| 10 | testing | left | w1 |
| 11 | testing | right | w1 |
| 12 | testing | left | w2 |
| 13 | testing | left | w2 |
| 14 | testing | right | b2 |
| 15 | testing | left | w1 |
| 16 | testing | right | w1 |
| 17 | testing | right | w1 |
| 18 | testing | left | w2 |
| 19 | testing | left | b1 |
| 20 | testing | left | b2 |
| 21 | testing | left | b2 |
| 22 | testing | right | b1 |
| 23 | testing | left | w1 |
| 24 | testing | right | b2 |
| 25 | testing | left | w2 |
| 26 | testing | right | b1 |
| 27 | testing | left | b2 |
| 28 | testing | right | w2 |
| 29 | testing | left | w2 |
| 30 | testing | right | w2 |
| 31 | testing | right | w1 |
| 32 | testing | left | b1 |
| 33 | testing | right | w2 |
| 34 | testing | right | b1 |
| 35 | testing | right | w2 |
| 36 | testing | left | w1 |
| 37 | testing | left | w1 |
| 38 | testing | right | b1 |
| 39 | testing | left | w1 |
| 40 | testing | right | w1 |
| 41 | testing | right | w2 |
| 42 | testing | right | w2 |
| 43 | testing | left | b1 |
| 44 | testing | right | b2 |
| 45 | testing | left | w1 |
| 46 | testing | right | w1 |
| 47 | testing | left | b2 |
| 48 | testing | right | b2 |
| 49 | testing | right | w1 |
| 50 | testing | right | w1 |
| 51 | testing | left | w1 |
| 52 | testing | right | w2 |
| 53 | testing | left | w2 |
| 54 | testing | left | w2 |
| 55 | testing | left | b1 |
 
 
Inside the **darker condition**, the participant is presented with the following values:

 
| Order | Type test | Side | Type Coloured Squares |
| ------- | ------ | ------ | --------------- |
| 1 | training | left | b2 |
| 2 | training | left | w1 |
| 3 | training | right | w1 |
| 4 | training | right | w2 |
| 5 | training | left | b1 |
| 6 | training | right | b1 |
| 7 | training | right | b2 |
| 8 | testing | right | b2 |
| 9 | testing | right | b1 |
| 10 | testing | right | w1 |
| 11 | testing | right | b1 |
| 12 | testing | right | b2 |
| 13 | testing | left | w1 |
| 14 | testing | left | b2 |
| 15 | testing | left | b2 |
| 16 | testing | left | w1 |
| 17 | testing | left | b1 |
| 18 | testing | right | w1 |
| 19 | testing | right | b1 |
| 20 | testing | left | w2 |
| 21 | testing | right | w2 |
| 22 | testing | right | b2 |
| 23 | testing | right | w2 |
| 24 | testing | right | b1 |
| 25 | testing | right | b1 |
| 26 | testing | right | b2 |
| 27 | testing | right | b1 |
| 28 | testing | left | b1 |
| 29 | testing | left | w1 |
| 30 | testing | left | b2 |
| 31 | testing | right | b2 |
| 32 | testing | left | w1 |
| 33 | testing | left | b2 |
| 34 | testing | right | b1 |
| 35 | testing | left | b1 |
| 36 | testing | left | b2 |
| 37 | testing | left | b1 |
| 38 | testing | left | b1 |
| 39 | testing | left | w2 |
| 40 | testing | left | b2 |
| 41 | testing | left | w2 |
| 42 | testing | right | w2 |
| 43 | testing | left | b2 |
| 44 | testing | left | b1 |
| 45 | testing | right | w1 |
| 46 | testing | right | b1 |
| 47 | testing | right | w2 |
| 48 | testing | right | b2 |
| 49 | testing | left | w2 |
| 50 | testing | right | b2 |
| 51 | testing | left | b2 |
| 52 | testing | left | b1 |
| 53 | testing | right | w1 |
| 54 | testing | right | b2 |
| 55 | testing | left | b1 |

For the **lighter** condition, the participant sees a total of 55 presentations, including 7 for the training. Among the 48 left, the participant sees:

 - 8 "w1" stimuli presented on the right;	
 - 8 "w1" stimuli presented on the left;
 - 8 "w2" stimuli presented on the right;	
 - 8 "w2" stimuli presented on the left;

 - 4 "b1" stimuli presented on the right;
 - 4 "b1" stimuli presented on the left;
 - 4 "b2" stimuli presented on the right;
 - 4 "b2" stimuli presented on the left;
	
	
For the **big** condition, the participant sees a total of 55 presentations, including 7 for the training. Among the 48 left, the participant sees:

 - 8 "b1" stimuli presented on the right;	
 - 8 "b1" stimuli presented on the left;
 - 8 "b2" stimuli presented on the right;	
 - 8 "b2" stimuli presented on the left;

 - 4 "w1" stimuli presented on the right;
 - 4 "w1" stimuli presented on the left;
 - 4 "w2" stimuli presented on the right;
 - 4 "w2" stimuli presented on the left;
 
All participant have both the darker **and** lighter conditions: it is a **within-subject** design.


#### Data

Data for each participant is gathered into a single csv file, containing the following information: 
 
 - the **participant id**
 - the **test** numerosity (w1, w2, b1, b2)
 - the **time** needed to press the button
 - the **response**: correct (1) or incorrect (0)
 - the **position**: left or right
 - the **group**: small or big
 - the **type** of test: training or testing
 - the **key pressed**: no key or the x key


### Experiment 5: Valence

This experiment is very similar to the experiment 3. There are two main differences:

 - first, squares are replaced either by *snakes* (negative valence) either by *candies* (positive valence)
 - second, instead of having 4 (small), **12** (flashing), and **36** (big), we have now 4 (small), **8** (flashing), and **16** (big). This change was performed due to the difficulty to see the snake or candy elements in a 36 elements image.

#### Introduction

This experiment measures reaction time and error rates. The participant is presented with big or small numerosities appearing on the right or left side of the screen. We measure whether the participant is faster to detect small numerosities (*decreasing* numerosity) on the left and big numerosities (*increasing* numerosities) on the right. 

#### Material

For this experiment, the following material is used:

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python).
 - **Additional one-key keyboard**: this keyboard is used by the participant to interact with the program.

##### Dimensions

Size stimulus: 17 degrees

Distance between stimulus: 17 degrees

##### Timing

Cross presentation: 0.5 sec

First flashing numerosity (12 dots, centered): 0.2 sec

In between numerosity: 0.2 sec

Second numerosity (4 or 36, side): 3 sec (for both testing and training)

#### Procedure

The participant is first presented with a **training** phase. The training and the testing phase are exactly similar, except that the participant has more time to answer during the training. 

The numerosities presented are can be 4, 8 and 16 and are extracted from Di-Giorgio et al (2018) study (see Illustration 5 below):

![*Illustration N: The snakes stimuli used in Experiment 5. a. Three examples of the "4" stimuli. b. Three examples of the "12" stimuli. c. Three examples of the "36" stimuli.* ](pictures/SnakesStim.jpg){width=70%}

![*Illustration N: The candies stimuli used in Experiment 5. a. Three examples of the "4" stimuli. b. Three examples of the "12" stimuli. c. Three examples of the "36" stimuli.* ](pictures/CandiesStim.jpg){width=70%}

Each participant do the two conditions (which we will call **group** here, for more clarity given the terminology used for the experiments in 2021):

 - pressing a button only when the numerosity is **small**. If the numerosity is big, no button should be pressed and the participant should wait for the second stimulus to disappear. 
 
  - pressing a button only when the numerosity is **big**. If the numerosity is small, no button should be pressed and the participant should wait for the second stimulus to disappear. 


Participants could use their dominant hand (recorded in the questionnaire), always using their thumb.

![*Illustration 5: (a) A cross is presented (0.5 second). (b) A 12 numerosity is flashed in the middle of the screen (0.2 seconds), and is followed by a black screen (0.2 second). (c) Another numerosity is presented either on the left or the right side (counterbalanced). If the participant belongs to the "big" group, s.he has to press only if the numerosity decreases, and vice versa. Otherwise, s.he just wait for the stimulus to disappear.*](pictures/Snakes_exp.jpg){width=90%}

![*Illustration 5: (a) A cross is presented (0.5 second). (b) A 12 numerosity is flashed in the middle of the screen (0.2 seconds), and is followed by a black screen (0.2 second). (c) Another numerosity is presented either on the left or the right side (counterbalanced). If the participant belongs to the "big" group, s.he has to press only if the numerosity decreases, and vice versa. Otherwise, s.he just wait for the stimulus to disappear.*](pictures/Candies_exp.jpg){width=90%}

Inside the **small condition**, the participant is presented with the following values:

| Order | Type test | Position | Second numerosity printed | Valence |
| ------- | ------ | ------ | --------------- |--------------- |
| 1 | training | right | 4 | candies |
| 2 | training | right | 4 |  snakes |
| 3 | training | left | 4 |  snakes |
| 4 | training | right | 4 |  candies |
| 5 | training | right | 36 |  candies |
| 6 | training | left | 36 |  snakes |
| 7 | training | left | 4 |  candies |
| 8 | training | left | 36 |  candies |
| 9 | training | left | 4 |  snakes |
| 10 | training | left | 4 |  candies |
| 11 | training | left | 36 |  snakes |
| 12 | training | right | 36 |  candies |
| 13 | training | right | 36 |  snakes |
| 14 | training | right | 36 |  snakes |
| 15 | training | left | 36 |  candies |
| 16 | training | right | 4 |  snakes |
| 17 | testing | left | 4 | candies |
| 18 | testing | right | 36 | candies |
| 19 | testing | left | 36 | snakes |
| 20 | testing | left | 4 | snakes |
| 21 | testing | right | 36 | snakes |
| 22 | testing | left | 4 | snakes |
| 23 | testing | left | 4 | candies |
| 24 | testing | right | 4 | candies |
| 25 | testing | left | 4 | candies |
| 26 | testing | left | 4 | candies |
| 27 | testing | left | 4 | candies |
| 28 | testing | right | 4 | snakes |
| 29 | testing | left | 4 | snakes |
| 30 | testing | left | 4 | snakes |
| 31 | testing | right | 36 | candies |
| 32 | testing | left | 4 | candies |
| 33 | testing | left | 36 | candies |
| 34 | testing | right | 4 | snakes |
| 35 | testing | right | 4 | snakes |
| 36 | testing | right | 4 | snakes |
| 37 | testing | right | 4 | candies |
| 38 | testing | right | 4 | candies |
| 39 | testing | left | 36 | snakes |
| 40 | testing | left | 4 | snakes |
| 41 | testing | left | 4 | snakes |
| 42 | testing | left | 36 | candies |
| 43 | testing | right | 36 | snakes |
| 44 | testing | right | 4 | candies |
| 45 | testing | left | 4 | candies |
| 46 | testing | right | 4 | snakes |
| 47 | testing | right | 4 | candies |
| 48 | testing | right | 4 | candies |
| 49 | testing | left | 36 | candies |
| 50 | testing | right | 4 | snakes |
| 51 | testing | left | 36 | snakes |
| 52 | testing | right | 36 | snakes |
| 53 | testing | left | 36 | snakes |
| 54 | testing | right | 36 | candies |
| 55 | testing | right | 4 | candies |
| 56 | testing | right | 4 | snakes |
| 57 | testing | left | 4 | candies |
| 58 | testing | right | 4 | snakes |
| 59 | testing | right | 36 | candies |
| 60 | testing | left | 4 | snakes |
| 61 | testing | left | 36 | candies |
| 62 | testing | right | 4 | candies |
| 63 | testing | left | 4 | snakes |
| 64 | testing | right | 36 | snakes |

 
 
Inside the **big condition**, the participant is presented with the following values:

 
| Order | Type test | Position | Second numerosity printed | Valence |
| ------- | ------ | ------ | --------------- |--------------- |
| 1 | training | right | 4 | candies | 
| 2 | training | right | 4 | snakes | 
| 3 | training | left | 4 | snakes | 
| 4 | training | right | 4 | candies | 
| 5 | training | right | 36 | candies | 
| 6 | training | left | 36 | snakes | 
| 7 | training | left | 4 | candies | 
| 8 | training | left | 36 | candies | 
| 9 | training | left | 4 | snakes | 
| 10 | training | left | 4 | candies | 
| 11 | training | left | 36 | snakes | 
| 12 | training | right | 36 | candies | 
| 13 | training | right | 36 | snakes | 
| 14 | training | right | 36 | snakes | 
| 15 | training | left | 36 | candies | 
| 16 | training | right | 4 | snakes | 
| 17 | testing | right | 36 | candies | 
| 18 | testing | left | 36 | snakes | 
| 19 | testing | left | 4 | snakes | 
| 20 | testing | left | 4 | candies | 
| 21 | testing | right | 36 | snakes | 
| 22 | testing | right | 36 | snakes | 
| 23 | testing | left | 36 | candies | 
| 24 | testing | right | 4 | candies | 
| 25 | testing | right | 4 | snakes | 
| 26 | testing | left | 4 | candies | 
| 27 | testing | right | 36 | candies | 
| 28 | testing | left | 36 | candies | 
| 29 | testing | left | 36 | snakes | 
| 30 | testing | right | 36 | snakes | 
| 31 | testing | left | 36 | candies | 
| 32 | testing | right | 36 | snakes | 
| 33 | testing | left | 36 | snakes | 
| 34 | testing | right | 36 | candies | 
| 35 | testing | right | 36 | snakes | 
| 36 | testing | right | 36 | candies | 
| 37 | testing | left | 36 | candies | 
| 38 | testing | left | 4 | candies | 
| 39 | testing | right | 36 | snakes | 
| 40 | testing | left | 36 | snakes | 
| 41 | testing | right | 4 | candies | 
| 42 | testing | right | 4 | snakes | 
| 43 | testing | left | 4 | snakes | 
| 44 | testing | left | 36 | snakes | 
| 45 | testing | right | 36 | snakes | 
| 46 | testing | right | 4 | snakes | 
| 47 | testing | right | 36 | candies | 
| 48 | testing | left | 4 | snakes | 
| 49 | testing | left | 36 | candies | 
| 50 | testing | right | 4 | snakes | 
| 51 | testing | left | 36 | candies | 
| 52 | testing | right | 36 | candies | 
| 53 | testing | right | 36 | snakes | 
| 54 | testing | left | 36 | snakes | 
| 55 | testing | left | 36 | candies | 
| 56 | testing | right | 4 | candies | 
| 57 | testing | left | 4 | snakes | 
| 58 | testing | right | 36 | candies | 
| 59 | testing | left | 4 | candies | 
| 60 | testing | right | 4 | candies | 
| 61 | testing | left | 36 | snakes | 
| 62 | testing | right | 36 | candies | 
| 63 | testing | left | 36 | snakes | 
| 64 | testing | left | 36 | candies | 


For the **small** condition, the participant sees a total of 64 presentations, including 16 for the training. Among the **48 left**, the participant sees:

 - 8 presentations of "4 candies" presented on the right;	
 - 8 presentations of "4 candies" presented on the left;
 - 8 presentations of "4 snakes" presented on the right;	
 - 8 presentations of "4 snakes" presented on the left;

 - 4 presentations of "16 candies" stimuli presented on the right;
 - 4 presentations of "16 candies" stimuli presented on the left;
 - 4 presentations of "16 snakes" stimuli presented on the right;
 - 4 presentations of "16 snakes" stimuli presented on the left;
	
	
For the **big** condition, the participant sees a total of 64 presentations, including 16 for the training. Among the **48 left**, the participant sees:

 - 8 presentations of "16 candies" presented on the right;	
 - 8 presentations of "16 candies" presented on the left;
 - 8 presentations of "16 snakes" presented on the right;	
 - 8 presentations of "16 snakes" presented on the left;

 - 4 presentations of "4 candies" stimuli presented on the right;
 - 4 presentations of "4 candies" stimuli presented on the left;
 - 4 presentations of "4 snakes" stimuli presented on the right;
 - 4 presentations of "4 snakes" stimuli presented on the left;
 
All participant have both the small **and** big conditions: it is a **within-subject** design.

### Experiment 6: Gaze Tracking

This experiment is almost the same as with Himbas 2021.
There are three main differences:

 - first, there is no cross during the habituation phase (the participant is free to look at the squares flashing)
 - we did not perform part 2 here;
 - the participant should point with the finger at the ball moving during the calibration phase
 
#### Introduction

This experiment is a replication of Di-Giorgio et al (2019). In this experiment, we aim to track the participant's gaze and see the side of the screen the participant prefers to look at when presented with big and small numerosities. The participant is passive and does not interact with the program.

#### Material

 - **A computer**: here, we used a DELL Latitude 7400, with a Intel® Core™ i5-8365U CPU @ 1.60GHz × 8.
 - **Program**: this experiment is coded in Psychopy (Python). The program simultaneously records a video of the participant's face and presents the stimuli. Thus, for each frame, it writes the stimulus location on the screen and the picture of the participant's eyes. 
 - **Gaze Tracking**: To know which side of the screen the participant has looked at, we use 3 different methods: 
     - A Python function, *cv2* (opencv) that automatically finds the x and y location of the eye center on the screen, as well as coordinates for the eyes.
     - A Deep Learning tracking method, *DeepLabCut* (originally used for tracking animals). First, the network is trained by manually labeling one the eye's pupil side on a few representative frames for each participant. Then, the program extracts the x and y location for the eye pupil side based on the trained network, which extract .
     - The relative position of the pupil inside the eye help helps determine the side of the screen the participant has looked at. COMPLETE THIS PART 


#### Stimuli

We used exactly the same stimuli as Di-Giorgio et al (2019). The experiment is splitted into 2 parts, each of them preceded by a calibration phase. Here, we only performed part 1:

 - **Part 1:** participants are habituated with two squares containing 12 dots each. Then, half of the participants are tested with two squares containing 4 and then 36, and half with 36 and 4. 


##### Part 1

![*Illustration 3: Experiment 1. (a) Two squares containing 12 points are presented to the participant. (b) Two squares containing 4 points or 36 points will then be displayed (condition counterbalanced between participants). Our hypothesis maintains that the Himbas' gaze is preferably on the left square when presenting squares with 4 points, and preferentially on the right square when presenting with 36 points.*](pictures/exp1_num.jpg){width=50%}



##### Dimensions

Size stimulus: 17 degrees

Distance between stimulus: 17 degrees

Duration flashing habituation stimuli: 0.3 seconds

Duration testing stimulus: 3 seconds

Duration white box after the testing stimulus: 1 second

Total duration of the experiment: **5 minutes** without the instructions and preparation


#### Procedure


**Calibration**. First, we need to calibrate the gaze tracking program to the participant. The instructions are: *“This experiment has 2 parts. First, you will see a white ball on the screen. It will move. You have to point at the ball with your finger. Please try not to move your head, but just your eyes”.* The program show images of a ball moving.

**Testing**. *"Now, you have to look at the screen."* The program goes on with 3 examples of testing pictures, so the participant understands well the instructions.

The experiment is counterbalanced: when participants start with the *decreasing* group, they first look at "4" dots and then "36" dots. When participants start with the *increasing* group, they first look at "36" dots and then "4" dots. 

More concretely, for a participant starting with *increasing* group, the experiment looks like this:

 - calibration phase: ball moving
 - 2 squares of 12 dots on each side flashing 15 times
 - 2 squares of **36** dots on each side remaining a few seconds
 - 2 squares of 12 dots on each side flashing 15 times
 - 2 squares of **36** dots on each side remaining a few seconds
 - 2 squares of 12 dots on each side flashing 15 times
 - 2 squares of **36** dots on each side remaining a few seconds
 - 2 squares of 12 dots on each side flashing 15 times
 - 2 squares of **4** dots on each side remaining a few seconds
 - 2 squares of 12 dots on each side flashing 15 times
 - 2 squares of **4** dots on each side remaining a few seconds
 - 2 squares of 12 dots on each side flashing 15 times
 - 2 squares of **4** dots on each side remaining a few seconds

#### Data

After the experiment, the program's output consists in 4 files for each participant:

 - 4 **.csv files** with the location of the stimulus for each frame (calibration_part1, testing_part1, calibration_part2, testing_part2)
 - 4 **videos** of the participant's face, containing same number of frames as there are rows in the .csv file (calibration_part1, testing_part1, calibration_part2, testing_part2)
 
After the videos and csv files are processed (see Procedure above), the output result is a csv file with the gaze direction on the screen (left, right, center, closed) for each participant. Then, these files are combined into one single file with the following columns name:

TO COMPLETE, also looking at the eye-tracking file

## Summary all experiments and method

This table shows the which experiment has been with which population. 

Please note that the version of the Gaze Tracking experiment is slightly different between the children/Himbas2021 population on one side and the Himbas2022 population on the other side.

```{r , echo=F, message=F, warning=FALSE}


d_exp <- data.frame(OrderCards=c("v", "v", "v", "v"),
                    CountStones = c("v", "", "", ""),
                    GazeTrack = c("v", "v", "v", ""),
                    VT_OldExp = c("v", "v", "v", "v"),
                    Squares = c("", "v", "", ""),
                    Valence = c("", "v", "v", "v"),
                    Luminance = c("", "v", "", "v"),
                    Baskets = c("v", "", "v", "v"),
                    NumberLine = c("v", "", "", ""))

rownames(d_exp) <- c("Himbas2021", "Himbas2022", "ItalChildren", "ItalAdults")

kable(d_exp)


```

# Results

## Read Data

### Himbas 2021

#### Questionnaire

This part reads the file with the results from the **questionnaire** (both demographic and numerical abilities).

```{r , echo=F, message=F, warning=FALSE}
# read questionnaire
quest2021 <-  read.table(paste(pathres, "Himbas_2021/Questionnaire/quest_complete.csv", sep=""), header=T, sep=",", quote='"', fill=TRUE, stringsAsFactors = TRUE) 

# change a few name to fit with quest2022
quest2021 %>%
  rename(Gender=gender,
         Village=village_name,
         School=school,
         Grade=grade,
         Ethnicity=ethnicity,
         Outfit=clothing_type,
         Age = age) -> quest2021


```

#### Cards

This part reads the file with the results of the **ordering cards** experiment.
Individual files were preliminary gathered in a single csv file with all information.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

## better to use this aggregated cards file, because I recall that I have added some missing information directly inside it

# read table
cards2021 <-  read.table(paste(pathres, "Himbas_2021/Cards/cards_all.csv", sep=""), header=T, sep=";", quote='"', fill=TRUE) 

# change name inside column for better plotting
cards2021 %>%
    mutate(type = case_when(type == "num_free" ~ "Numbers (free)",
                               type == "dots_free" ~ "Dots (free)",
                               type == "dots_line" ~ "Dots (linear)",
                               type ==  "dots_line_new" ~ "Dots (linear) for N"),
           type = factor(type, levels=c("Dots (free)", "Numbers (free)", "Dots (linear)", "Dots (linear) for N" ))) -> cards2021

# add an index to know if the cards are ordered or not
cards2021$ordered <- "yes"
cards2021$ordered[cards2021$order=="N"] <- "no"

# merge two categories NC and N together
cards2021$orderNC <- cards2021$order
cards2021$order[cards2021$orderNC=="NC"] <- "N"

```

#### Baskets

This part reads the file with the results of the **basket** experiment.

Please note that we removed a few participants. Indeed, during this experiment, the experimenter was always behind the translator. For 6 participants (at the end), the experimenter has left the translator to perform this task by himself. When she came back, she realized that the translator was correcting the participants so that they count from the left. Thus, she decided to remove these 6 participants (that may have been all biased, maybe not) from the results. She checked for all the other participants. 

```{r read table exp 5, echo=FALSE, message=FALSE, warning=FALSE}

files <- list.files(path = paste(pathres, "Himbas_2021/Baskets/", sep=""),pattern = ".csv", full.names = TRUE)
temp <- lapply(files, fread, sep=",")

# merge them inside a single file
baskets <- rbindlist(temp)

list_all_bask = c()
for (el in unique(baskets$participant)){
  subdf =  baskets[baskets$participant==el,]
  startbask = subdf$basket[1]
  list_all_bask = c(list_all_bask, startbask)
}

# select only the testing session
basket_testing = baskets[baskets$session=="testing",]
names(basket_testing)[names(basket_testing) == "participant"] <- "participant_id"

# add a new variable for the bias + change column names
basket_testing %>%
  mutate(bias = ifelse(((basket == 3 & position_dot==3) | (basket == 8 & position_dot==8)), "L", ifelse(((basket == 8 & position_dot==3) | (basket == 3 & position_dot==8)), "R", "N")),
    position_dot = case_when(position_dot == "3" ~ "Ball on 3rd pos",
                               position_dot == "8" ~ "Ball on 8th pos"),
    basket = factor(basket)) -> basket_testing


### Also compute a new df for the first frame
baskets$index_bypart <- 0
for (part in unique(baskets$participant)){
  baskets$index_bypart[baskets$participant==part] <- c(1:nrow(baskets[baskets$participant==part,]))}

# Add the experiment value
baskets$exp <- "Himbas2021"

#
baskets %>%
  filter(session == "testing" & (index_bypart == 4 | index_bypart == 19)) %>%
  mutate(order_par = ifelse(((position_dot == 3 & index_bypart ==4) | (position_dot == 8 & index_bypart ==19)), "Order 3 then 8", "Order: 8 then 3"),
         bias = ifelse(((basket == 3 & position_dot==3) | (basket == 8 & position_dot==8)), "L", ifelse(((basket == 8 & position_dot==3) | (basket == 3 & position_dot==8)), "R", "N")),
         position_dot = case_when(position_dot == 3 ~ "Ball on 3rd pos",
                               position_dot == 8 ~ "Ball on 8th pos"),
         basket = factor(basket)) %>%
  rename(participant_id = participant)-> basket_first_test_himb

# add the order value in big dataframe too
baskets %>%
  filter(session == "testing" & (index_bypart == 4)) %>%
  mutate(order_par = ifelse((position_dot == 3 & index_bypart ==4), "Order: 3 then 8", "Order: 8 then 3")) %>%
  rename(participant_id = participant) %>%
  select(participant_id, order_par)-> basket_per_part_himb

basket_testing_himb <- merge(basket_testing, basket_per_part_himb, by="participant_id", all.x=TRUE)

# merge with questionnaire
baskets_quest_himb <- merge(basket_testing_himb, quest2021, by="participant_id")
baskets_quest_himb %>%
  mutate(bias_cards_free = case_when(bias_cards_free=="L" ~ "Cards: L",
                               bias_cards_free=="R" ~ "Cards: R",
                               bias_cards_free=="N" ~ "Cards: N")) -> baskets_quest_himb
baskets_quest_himb$exp <- "Himbas2021"


```

#### GazeTracking

This part reads the file with the results of the **gaze tracking** experiment. Please note that the files used here were preliminary processed using another Rmarkdown file (eye_tracking.RmD). In this other file, we gathered the information coming from the manual and automatic analysis. We also made the outlier removal based on correlations rate. For more information, please refer to this other file.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

gt2021_part1 <-  read.table(paste(pathres, "Himbas_2021/GazeTracking/Summary/part1.csv", sep=""), header=T, sep=",", quote='"', stringsAsFactors = T, fill=TRUE)

gt2021_part2 <-  read.table(paste(pathres, "Himbas_2021/GazeTracking/Summary/part2.csv", sep=""), header=T, sep=",", quote='"', stringsAsFactors = T, fill=TRUE) 

exp1_small <- c(1007, 1021, 1023, 1025, 1027, 1046, 1061, 1142, 1148)
exp2_small <- c(1007, 1008, 1046, 1125, 1130)


exp1_big <- c(1007, 1021, 1022, 1023, 1024, 1025, 1027, 1028, 1046, 1061, 1062, 1064, 1085, 1110, 1142, 1148, 1163, 1168)
exp2_big <- c(1004, 1006, 1007, 1008, 1024, 1028, 1029, 1044, 1046, 1061, 1065, 1069, 1087, 1090, 1110, 1124, 1126, 1130, 1141, 1142, 1145, 1146, 1148, 1168, 1181)

part1_small <- gt2021_part1[gt2021_part1$participant_id %in% exp1_small,]
part1_big <- gt2021_part1[gt2021_part1$participant_id %in% exp1_big,]

part2_small <- gt2021_part2[gt2021_part2$participant_id %in% exp2_small,]
part2_big <- gt2021_part2[gt2021_part2$participant_id %in% exp2_big,]


```

We kept two different dataset:

 - participants who have high correlations between manual and automatic tracking for *part 1*: in total, this is `r length(exp1_big)`
 - participants who have high correlations between manual and automatic tracking for *part 2*: in total, this is `r length(exp2_big)`
 - participants who have **very** high correlations between manual and automatic tracking (more strict criteria of seleciton) in *part 1*: in total, this is `r length(exp1_small)`
  - participants who have **very** high correlations between manual and automatic tracking (more strict criteria of seleciton) in *part 2*: in total, this is `r length(exp2_small)`


#### NumberLine

This part reads the file with the results of the **NumberLine** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE}

files <- list.files(path = paste(pathres, "Himbas_2021/NumberLine/results/", sep=""), pattern = ".csv", full.names = TRUE)
temp <- lapply(files, fread, sep=",")

list_all_nbline = c()
for (el in 1:length(files)){
  my_el = files[el]
  part_id =  as.numeric(substr(my_el, 97, 100))
  temp[[el]]$participant_id = part_id
}
# merge them inside a single file
numberline <- rbindlist(temp)

numberline[,4] <- NULL
colnames(numberline) <- c("stim_number", "time", "x_location", "participant_id")

# remove training item
numberline2 <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(numberline2) <- colnames(numberline)
for (part in unique(numberline$participant_id)){
  particip <- numberline[numberline$participant_id==part,][-c(1,2),]
  numberline2 <- rbind(numberline2, particip)
}

nbline_aggr <- aggregate(numberline2$x_location, list(stim_number = numberline2$stim_number, participant_id = numberline2$participant_id), FUN=mean)

# change the minimum and maximum
nbline_aggr$x[nbline_aggr$x < -640] <- -640
nbline_aggr$x[nbline_aggr$x > 640] <- 640


```

We remove here participants that did not order the items from 1 to 10 (basically, who randomly positionned the cursor).

```{r , echo=FALSE, message=FALSE, warning=FALSE}

list_part_remove <- c()
for (part in unique(nbline_aggr$participant_id)){
  items = as.numeric(nbline_aggr$x[nbline_aggr$participant_id==part])
  item_ordered = sort(items, decreasing=FALSE)
  if (all(items==item_ordered)==FALSE){
    list_part_remove <- c(list_part_remove, part)
  }
}

nbline_noout <- nbline_aggr[!(nbline_aggr$participant_id %in% list_part_remove),]

# merge with questionnaire
nbline_quest <- merge(nbline_noout, quest2021, all.x=TRUE, all.y=FALSE, by="participant_id")
```

Using this method, we remove `r length(list_part_remove)`. We will work with a file excluding these participants in the future section (see [NumberLine (logarithmic or linear)] part). Please note that we will also explore other outlier removal methods in the next sections.

#### VT experiment (newnewborn)

This part reads the file with the results of the **VT** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Himbas_2021/Newnewborn/results/", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathres, "Himbas_2021/Newnewborn/results/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 15)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# correct an error in terms of terminology
testing$group[testing$group=="decrease"] <- "decreasing"

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# create situation column (whether stimuli increased or decreased)
testing$situation <- "dec"
testing$situation[testing$amorce < testing$test] <- "inc"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "decreasing" & testing$situation == "dec") | (testing$group == "increasing" & testing$situation == "inc")] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$situation == "dec" & testing$position == -15) | (testing$situation == "inc" & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 66) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_VT_2021 <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]



```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_VT_2021)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in%list_part_remove_VT_2021),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_chosen <- testing_noout1[!(testing_noout1$participant_id %in% 1058),] # see below for the reason why we exclude this one too
# -> we filter outlier later, when gathering all files

# add index
testing$index <- c(1:nrow(testing))
nextindex <- nrow(testing) + 1

# FILTER FOR TIME: select only good answers and target stimuli for time
VT_time_2021 = testing[testing$resp==1 & testing$stimuli=="target",]

VT_time_2021 = VT_time_2021[!(VT_time_2021$participant_id %in% list_part_remove_VT_2021),]

# FILTER FOR ERRORS: select all for errors
VT_error_2021 = testing
VT_error_2021 = VT_error_2021[!(VT_error_2021$participant_id %in% list_part_remove_VT_2021),]


```

Above is how the new distribution looks like. In total, we keep `r length(unique(VT_time_2021$participant_id))` participants.

### Himbas 2022


```{r , echo=FALSE}

# read questionnaire of Himba 2022
quest2022 <- read.csv(paste(pathres, "Himbas_2022/Questionnaire/quest.csv", sep=""), header=TRUE, sep=";", stringsAsFactors = TRUE)


```


#### Questionnaire

This part reads the file with the results from the **questionnaire** (both demographic and numerical abilities).

```{r , echo=FALSE}


# print summary
summary(quest2022) 

# change format of literacy score
quest2022$LiteracyScore <- as.numeric(quest2022$LiteracyScore)

# change columns name to have same values with quest2021
quest2022 %>%
  rename(participant_id = Subject,
         ethnicity = Culture,
         telephone = Phone) -> quest2022

## convert to factor
#quest2022$OpuwoThisYear <- as.factor(quest2022$OpuwoThisYear)
quest2022$UrbanIndex <- as.numeric(quest2022$UrbanIndex)
quest2022$UrbanKnow <- as.numeric(quest2022$UrbanKnow)
quest2022$UrbanPref <- as.numeric(quest2022$UrbanPref)

# create a category like for Himba 2021
quest2022$literacy_group <- "some"
quest2022$literacy_group[quest2022$LiteracyScore== 1 | quest2022$LiteracyScore== 2] <- "none"
quest2022$literacy_group <- as.factor(quest2022$literacy_group)

# create a categorical category for age, matrice, numerical ability, and mct
quest2022$age_cut <- cut(quest2022$Age, breaks=c(0,25,35,45,100), labels = c("15-25", "25-35", "35-45", "45+"))
quest2022$mat <- as.numeric(cut_number(quest2022$ScoreMatrices,2))
quest2022$mat[quest2022$mat==2] <- "high"
quest2022$mat[quest2022$mat==1] <- "low"
quest2022$numab <- as.numeric(cut_number(quest2022$NumericalAbility,2))
quest2022$numab[quest2022$numab==2] <- "high"
quest2022$numab[quest2022$numab==1] <- "low"
quest2022$mct <- as.numeric(cut_number(quest2022$ScoreMCT,2))
quest2022$mct[quest2022$mct==2] <- "high"
quest2022$mct[quest2022$mct==1] <- "low"
quest2022$uknow <- as.numeric(cut_number(quest2022$UrbanKnow,2))
quest2022$uknow[quest2022$uknow==2] <- "high"
quest2022$uknow[quest2022$uknow==1] <- "low"
quest2022$upref <- as.numeric(cut_number(quest2022$UrbanPref,2))
quest2022$upref[quest2022$upref==2] <- "high"
quest2022$upref[quest2022$upref==1] <- "low"
quest2022$uind <- as.numeric(cut_number(quest2022$UrbanIndex,2))
quest2022$uind[quest2022$uind==2] <- "high"
quest2022$uind[quest2022$uind==1] <- "low"

# change format so that it is different from 2021
quest2022$participant_id <- as.numeric(quest2022$participant_id)
quest2022$participant_id <- quest2022$participant_id + 1000
quest2022$participant_id <- as.character(quest2022$participant_id) 


```

#### VT experiment (Newnewborn) 

This part reads the file with the results of the **VT** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Himbas_2022/Newnewborn", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 15)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# change format so that it is different from 2021
testing$participant_id <- as.numeric(testing$participant_id)
testing$participant_id <- testing$participant_id + 1000
testing$participant_id <- as.character(testing$participant_id) 

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# create situation column (whether stimuli increased or decreased)
testing$situation <- "dec"
testing$situation[testing$amorce < testing$test] <- "inc"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "decreasing" & testing$situation == "dec") | (testing$group == "increasing" & testing$situation == "inc")] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$situation == "dec" & testing$position == -15) | (testing$situation == "inc" & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

testing$key_pressed <- as.numeric(testing$key_pressed)
testing$key_pressed[testing$key_pressed==2] <- 1

aggreg_correct = aggregate(testing$key_pressed, by=list(group = testing$group, participant_id=testing$participant_id), FUN=sum, na.rm=TRUE)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 66) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_VT_2022 <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]
list_part_remove_VT_2022 = c(list_part_remove_VT_2022, 2058)


```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_VT_2022)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in%list_part_remove_VT_2022),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_chosen <- testing_noout1[!(testing_noout1$participant_id %in% 1058),] # see below for the reason why we exclude this one too
# -> we filter outlier later, when gathering all files

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# FILTER FOR TIME: select only good answers and target stimuli for time
VT_time_2022 = testing[testing$resp==1 & testing$stimuli=="target",]
VT_time_2022 = VT_time_2022[!(VT_time_2022$participant_id %in% list_part_remove_VT_2022),]

# FILTER FOR ERRORS: select all for errors
VT_error_2022 = testing
VT_error_2022 = VT_error_2022[!(VT_error_2022$participant_id %in% list_part_remove_VT_2022),]

```

Above is how the new distribution looks like. In total, we keep `r length(unique(VT_time_2022$participant_id))` participants.

#### Square experiment

This part reads the file with the results of the **square** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Himbas_2022/Squares", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 15)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# change format so that it is different from 2021
testing$participant_id <- as.numeric(testing$participant_id)
testing$participant_id <- testing$participant_id + 1000
testing$participant_id <- as.character(testing$participant_id) 

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# correct an error in terms of terminology
testing$group[testing$group=="decrease"] <- "decreasing"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "small" & testing$test == 4) | (testing$group == "big" & testing$test == 36)] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$test == 4 & testing$position == -15) | (testing$test == 36 & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_neut <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_neut)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in% list_part_remove_neut),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)


# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_neut),]
#testing_noout2 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_noout3 <- testing[!(testing$participant_id %in% list_part_remove_VT | testing$participant_id %in% list_part_remove_neut),]
# -> we filter outlier later, when gathering all files

# choose the dataframe you want to work with!
#testing_chosen <- testing_noout3

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# FILTER FOR TIME
NEUT_time = testing[testing$resp==1 & testing$stimuli=="target" ,]

NEUT_time = NEUT_time[!(NEUT_time$participant_id %in% list_part_remove_neut),]

# FILTER FOR ERRORS: select all for errors
NEUT_error = testing
NEUT_error = NEUT_error[!(NEUT_error$participant_id %in% list_part_remove_neut),]



```

Above is how the new distribution looks like. In total, we keep `r length(unique(NEUT_time$participant_id))` participants.

#### Valence experiment

This part reads the file with the results of the **valence** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal for the candies experiment.')}

# path to folder
pathf = (paste(pathres,"Himbas_2022/Valence", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 15)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# change format so that it is different from 2021
testing$participant_id <- as.numeric(testing$participant_id)
testing$participant_id <- testing$participant_id + 1000
testing$participant_id <- as.character(testing$participant_id) 

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# correct an error in terms of terminology
testing$group[testing$group=="decrease"] <- "decreasing"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "small" & testing$test == 4) | (testing$group == "big" & testing$test == 36)] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$test==4 & testing$position == -15) | (testing$test == 36 & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

test_candies <- testing[testing$element == "c",]
test_snakes <- testing[testing$element == "s",]

##############
# CANDIES
##############

testing <- test_candies

aggreg_correct = aggregate(test_candies$resp, by=list(group = test_candies$group, participant_id=test_candies$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_cand <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

CAND_time = testing[testing$resp==1 & testing$stimuli=="target" ,]
CAND_time = CAND_time[!(CAND_time$participant_id %in% list_part_remove_cand),]

# FILTER FOR ERRORS: select all for errors
CAND_error = testing
CAND_error = CAND_error[!(CAND_error$participant_id %in% list_part_remove_cand),]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_cand)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal for the snakes experiment.')}

##############
# SNAKES
##############

testing <- test_snakes

aggreg_correct = aggregate(test_snakes$resp, by=list(group = test_snakes$group, participant_id=test_snakes$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_snak <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1


SNAK_time = testing[testing$resp==1 & testing$stimuli=="target" ,]

# select only good answers
SNAK_time = SNAK_time[!(SNAK_time$participant_id %in% list_part_remove_snak),]

# FILTER FOR ERRORS: select all for errors
SNAK_error = testing
SNAK_error = SNAK_error[!(SNAK_error$participant_id %in% list_part_remove_snak),]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_snak)` participants.

In total, we keep `r length(unique(SNAK_time$participant_id))` for the snakes experiment and `r length(unique(CAND_time$participant_id))` for the candies experiment.

#### Luminance experiment

This part reads the file with the results of the **luminance** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Himbas_2022/Luminance", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 15)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# change format so that it is different from 2021
testing$participant_id <- as.numeric(testing$participant_id)
testing$participant_id <- testing$participant_id + 1000
testing$participant_id <- as.character(testing$participant_id) 

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

testing$luminance <- "dark"
testing$luminance[startsWith(testing$test, "w")] <- "light"

testing$strength <- "strong"
testing$strength[endsWith(testing$test, "1")] <- "weak"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "darker" & testing$luminance == "dark" )] <- "target"
testing$stimuli[(testing$group == "lighter" & testing$luminance == "light" )] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$luminance == "dark"  & testing$position == -15)] <- "congruent"
testing$condition[(testing$luminance == "light" & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_col <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_col)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in% list_part_remove_col),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)


# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_col),]
#testing_noout2 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_noout2 <- testing[!(testing$participant_id %in% list_part_rem_exp4_2),]
# choose the dataframe you want to work with!
#testing_chosen <- testing_noout1

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# FILTER FOR TIME
COL_time = testing[testing$resp==1 & testing$stimuli=="target" ,]

COL_time = COL_time[!(COL_time$participant_id %in% list_part_remove_col),]

# FILTER FOR ERRORS: select all for errors
COL_error = testing
COL_error = COL_error[!(COL_error$participant_id %in% list_part_remove_col),]

```

Above is how the new distribution looks like. In total, we keep `r length(unique(COL_time$participant_id))` participants.

#### GazeTracking

This part reads the file with the results of the **Gaze Tracking** experiment. 
Please note that the files used here were preliminary processed using another Rmarkdown file (eye_tracking.RmD). In this other file, we gathered the information coming from the manual and automatic analysis. Contrary to Himbas 2021, We did *not* make the outlier removal. However, this file is already aggregated by participant. For more information, please refer to this other file.


```{r read table exp 6, echo=FALSE, message=FALSE, warning=FALSE}

# read the three files:
gt2022_all <- read.csv(paste(pathres, "Himbas_2022/GazeTracking/Summary/ALL_Himbas2022.csv", sep=""), header=TRUE)


# add another column telling the number of habituation and testing phase
gt2022_all %>%
  mutate(type_exp_precise =         ifelse( (type_experiment == "habituation" | type_experiment == "flash_after_habituation") & (myindex > 90 & myindex < 271), "habituation_1", 
                                    ifelse( (type_experiment == "testing" ) & (myindex > 297 & myindex < 388), "testing_1", 
                                    ifelse( (type_experiment == "habituation" | type_experiment == "flash_after_habituation") & (myindex > 432 & myindex < 613), "habituation_2", 
                                    ifelse( (type_experiment == "testing") & (myindex > 639 & myindex < 730), "testing_2", 
                                    ifelse( (type_experiment == "habituation" | type_experiment == "flash_after_habituation") & (myindex > 774 & myindex < 955), "habituation_3", 
                                    ifelse( (type_experiment == "testing") & (myindex > 981 & myindex < 1072), "testing_3", "Other"))))))) -> gt2022_all

# aggregate by participant_id and step: do for manual
gt2022_all %>% group_by(participant_id,type_exp_precise, dots, position) %>% 
  summarise(sum_manual=n(),.groups = 'drop') %>% 
  as.data.frame() %>%
  filter(type_exp_precise != "Other" & is.na(position)==FALSE) %>%
  spread(position, sum_manual) %>%
  mutate(Center = coalesce(Center, 0),
         Left = coalesce(Left, 0),
         Right = coalesce(Right, 0),
         `Closed (or unsure)` = coalesce(`Closed (or unsure)`, 0)) %>%
  gather(position, sum_manual, Center:Right) %>%
  separate(type_exp_precise, c('type_exp', 'nb_exp'))-> gt2022_man



# aggregagate by participant_id and step: do for automatic
gt2022_all %>% group_by(participant_id,type_exp_precise, dots, auto_m2) %>% 
  summarise(sum_auto=n(),.groups = 'drop') %>% 
  as.data.frame() %>%
  filter(type_exp_precise != "Other" & is.na(auto_m2)==FALSE) %>%
  spread(auto_m2, sum_auto) %>%
  mutate(Center = coalesce(Center, 0),
         Left = coalesce(Left, 0),
         Right = coalesce(Right, 0),
         `Closed (or unsure)` = coalesce(`Closed (or unsure)`, 0)) %>%
  gather(auto_m2, sum_auto, Center:Right) %>%
  rename(position=auto_m2) %>%
  separate(type_exp_precise, c('type_exp', 'nb_exp'))-> gt2022_auto

# merge manual and automatic
gt_2022 <- merge(gt2022_man, gt2022_auto, by=c("participant_id", "type_exp", "nb_exp", "dots", "position"))

gt_2022 %>%
  mutate(sum_all = ifelse(type_exp == "habituation", 180, 90)) %>%
  mutate(ratio_manual = sum_manual / sum_all,
         ratio_auto = sum_auto / sum_all) %>%
  select(-c(sum_manual, sum_auto, sum_all)) -> gt_2022_per

# FINAL TABLE
gt_2022 <- merge(gt_2022, gt_2022_per, by=c("participant_id", "type_exp", "nb_exp", "dots", "position"))
gt_2022 %>%
  rename(condition = dots) %>%
  mutate(condition = case_when(condition == 4 ~ "Decrease",
                               condition == 36 ~ "Increase")) -> gt_2022 
```



#### Cards

This part reads the file with the results of the **ordering cards** experiment.
Here, individual files were *not* preliminary gathered in a single csv, contrary to Himbas 2021. Instead, we process them below.

```{r , echo=F, message=F, warning=FALSE}

# path to card files
pathf = (paste(pathres,"Himbas_2022/Cards/CardsInfo/file_per_participant/", sep=""))

# list all files in directory
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read all files and merge them into a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T, sep=";")
  df <- rbind(df, d)
}

# change format so that it is different from 2021
df$ID <- as.numeric(df$ID)
df$ID <- df$ID + 1000
df$ID <- as.character(df$ID) 

# keep this in a variable to use later
cards2022 <- df

# create subfiles according to card type

# cards with dots
df_dots <- df[df$type=="dots_free",c("ID", "order")]
colnames(df_dots) <- c("participant_id", "dots")

# cards with colors
df_col <- df[df$type=="col_free",c("ID", "order")]
colnames(df_col) <- c("participant_id", "colours")

# cards with digits
df_num <- df[df$type=="num_free",c("ID", "order")]
colnames(df_num) <- c("participant_id", "arabnum")

# merge these together
df_cards2022_all <- merge(df_dots, df_num, by="participant_id")
df_cards2022_all <- merge(df_cards2022_all, df_col, by="participant_id")

# write this file
# write.csv2(df_cards_all, paste(path, "cards_all_final.csv"), row.names=FALSE)

# add information to quest2022
id_knownum <- df_num$participant_id[df_num$arabnum=="L" | df_num$arabnum=="R"]
quest2022$knownum <- "no"
quest2022$knownum[quest2022$participant_id %in% id_knownum] <- "yes"

```


```{r , echo=FALSE, message=FALSE, warning=FALSE}

# we use here the file "df_cards_all" that we previously read.

# change name inside column for better plotting
cards2022 %>%
    mutate(type = case_when(type == "dots_free" ~ "Dots",
                               type == "col_free" ~ "Colors",
                               type == "num_free" ~ "Digits"),
           type = factor(type, levels=c("Dots", "Colors", "Digits"))) -> cards2022

# correct a few typos
cards2022$shape[cards2022$shape=="L"] <- "linear"
cards2022$shape[cards2022$shape=="liinear"] <- "linear"
cards2022$shape[cards2022$shape=="V-shape-like"] <- "V_shape"
cards2022$shape[cards2022$shape=="Circle"] <- "circle"
cards2022$shape[cards2022$shape=="linear"] <- "linear_horizontal"
cards2022$shape[cards2022$shape=="linearV"] <- "linear_vertical"
cards2022$shape[cards2022$shape=="weird"] <- "no_shape"
cards2022$shape[cards2022$shape=="two blocks"] <- "2_groups"
cards2022$shape[cards2022$shape=="two groups"] <- "2_groups"
cards2022$shape[cards2022$shape=="blocks of two"] <- "groups_of_2"
cards2022$shape[cards2022$shape=="groups of 2"] <- "groups_of_2"
cards2022$order[cards2022$order=="N "] <- "N"

# change a detail: when someone has done a shape which is not linear horizontal and I wrote "N", it actually means that it was impossible to say that whether it was L or R, not that the participant has not ordered
cards2022$order[cards2022$shape!="linear_horizontal" & cards2022$order == "N"] <- "NC"

# add an index to know if the cards are ordered or not
cards2022$ordered <- "yes"
cards2022$ordered[cards2022$order=="N"] <- "no"

# merge two categories NC and N together
cards2022$orderNC <- cards2022$order
cards2022$order[cards2022$orderNC=="NC"] <- "N"

# rename ID columns
cards2022 %>%
  rename(participant_id = ID) -> cards2022

```


### Adults italians

#### VT experiment (newnewborn)

This part reads the file with the results of the **VT** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Adults/Newnewborn/", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathres, "Adults/Newnewborn/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 13)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# correct an error in terms of terminology
testing$group[testing$group=="decrease"] <- "decreasing"

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# create situation column (whether stimuli increased or decreased)
testing$situation <- "dec"
testing$situation[testing$amorce < testing$test] <- "inc"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "decreasing" & testing$situation == "dec") | (testing$group == "increasing" & testing$situation == "inc")] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$situation == "dec" & testing$position == -15) | (testing$situation == "inc" & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 66) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_VT_adults <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_VT_adults)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in%list_part_remove_VT_adults),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_chosen <- testing_noout1[!(testing_noout1$participant_id %in% 1058),] # see below for the reason why we exclude this one too
# -> we filter outlier later, when gathering all files

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# FILTER FOR TIME: select only good answers and target stimuli for time
VT_time_adul = testing[testing$resp==1 & testing$stimuli=="target",]

VT_time_adul = VT_time_adul[!(VT_time_adul$participant_id %in% list_part_remove_VT_adults),]

# FILTER FOR ERRORS: select all for errors
VT_error_adul = testing
VT_error_adul = VT_error_adul[!(VT_error_adul$participant_id %in% list_part_remove_VT_adults),]

```

Above is how the new distribution looks like. In total, we keep `r length(unique(VT_time_adul$participant_id))` participants.


#### Baskets

This part reads the file with the results of the **basket** experiment.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

files <- list.files(path = paste(pathres, "Adults/Baskets/", sep=""),pattern = ".csv", full.names = TRUE)
temp <- lapply(files, fread, sep=",")

# merge them inside a single file
baskets <- rbindlist(temp)

list_all_bask = c()
for (el in unique(baskets$participant)){
  subdf =  baskets[baskets$participant==el,]
  startbask = subdf$basket[1]
  list_all_bask = c(list_all_bask, startbask)
}

# select only the testing session
basket_testing = baskets[baskets$session=="testing",]
names(basket_testing)[names(basket_testing) == "participant"] <- "participant_id"

# add a new variable for the bias + change column names
basket_testing %>%
  mutate(bias = ifelse(((basket == 3 & position_dot==3) | (basket == 8 & position_dot==8)), "L", ifelse(((basket == 8 & position_dot==3) | (basket == 3 & position_dot==8)), "R", "N")),
    position_dot = case_when(position_dot == "3" ~ "Ball on 3rd pos",
                               position_dot == "8" ~ "Ball on 8th pos"),
    basket = factor(basket)) -> basket_testing


### Also compute a new df for the first frame
baskets$index_bypart <- 0
for (part in unique(baskets$participant)){
  baskets$index_bypart[baskets$participant==part] <- c(1:nrow(baskets[baskets$participant==part,]))}

# Add the experiment value
baskets$exp <- "Adults"

#
baskets %>%
  filter(session == "testing" & (index_bypart == 4 | index_bypart == 19)) %>%
  mutate(order_par = ifelse(((position_dot == 3 & index_bypart ==4) | (position_dot == 8 & index_bypart ==19)), "Order 3 then 8", "Order: 8 then 3"),
         bias = ifelse(((basket == 3 & position_dot==3) | (basket == 8 & position_dot==8)), "L", ifelse(((basket == 8 & position_dot==3) | (basket == 3 & position_dot==8)), "R", "N")),
         position_dot = case_when(position_dot == 3 ~ "Ball on 3rd pos",
                               position_dot == 8 ~ "Ball on 8th pos"),
         basket = factor(basket)) %>%
  rename(participant_id = participant)-> basket_first_test_adul

# add the order value in big dataframe too
baskets %>%
  filter(session == "testing" & (index_bypart == 4)) %>%
  mutate(order_par = ifelse((position_dot == 3 & index_bypart ==4), "Order: 3 then 8", "Order: 8 then 3")) %>%
  rename(participant_id = participant) %>%
  select(participant_id, order_par)-> basket_per_part_adul

basket_testing_adul <- merge(basket_testing, basket_per_part_adul, by="participant_id", all.x=TRUE)

# merge with questionnaire
baskets_quest_adul <- merge(basket_testing_adul, quest2021, by="participant_id", all.x=TRUE)
# baskets_quest %>%
#   mutate(bias_cards_free = case_when(bias_cards_free=="L" ~ "Cards: L",
#                                bias_cards_free=="R" ~ "Cards: R",
#                                bias_cards_free=="N" ~ "Cards: N")) -> baskets_quest
baskets_quest_adul$exp <- "Adults"


```


#### Valence experiment

This part reads the file with the results of the **valence** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal for the candies experiment.')}

# path to folder
pathf = (paste(pathres,"Adults/Valence", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 13)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# correct an error in terms of terminology
testing$group[testing$group=="decrease"] <- "decreasing"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "small" & testing$test == 4) | (testing$group == "big" & testing$test == 36)] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$test==4 & testing$position == -15) | (testing$test == 36 & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

test_candies <- testing[testing$element == "c",]
test_snakes <- testing[testing$element == "s",]

##############
# CANDIES
##############

testing <- test_candies

aggreg_correct = aggregate(test_candies$resp, by=list(group = test_candies$group, participant_id=test_candies$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_adul_cand <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# FILTER FOR TIME: select only good answers
CAND_adul_time = testing[testing$resp==1 & testing$stimuli=="target" ,]

# FILTER FOR ERRORS
CAND_adul_error = testing

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_adul_cand)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal for the snakes experiment.')}

##############
# SNAKES
##############

testing <- test_snakes

aggreg_correct = aggregate(test_snakes$resp, by=list(group = test_snakes$group, participant_id=test_snakes$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_adul_snak <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# select only good answers
SNAK_adul_time = testing[testing$resp==1 & testing$stimuli=="target" ,]

# FILTER FOR ERRORS
SNAK_adul_error = testing

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_adul_snak)` participants.

In total, we keep `r length(unique(SNAK_adul_time$participant_id))` for the snakes experiment and `r length(unique(CAND_adul_time$participant_id))` for the candies experiment.

#### Luminance experiment

This part reads the file with the results of the **luminance** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Adults/Luminance", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathf, "/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 12, 13)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# remove training trials
testing = df[df$type_test=="testing",]

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

testing$luminance <- "dark"
testing$luminance[startsWith(testing$test, "w")] <- "light"

testing$strength <- "strong"
testing$strength[endsWith(testing$test, "1")] <- "weak"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "darker" & testing$luminance == "dark" )] <- "target"
testing$stimuli[(testing$group == "lighter" & testing$luminance == "light" )] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$luminance == "dark"  & testing$position == -15)] <- "congruent"
testing$condition[(testing$luminance == "light" & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_adul_col <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_adul_col)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in% list_part_remove_adul_col),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)


# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_col),]
#testing_noout2 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_noout2 <- testing[!(testing$participant_id %in% list_part_rem_exp4_2),]
# choose the dataframe you want to work with!
#testing_chosen <- testing_noout1

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1

# FILTER FOR TIME
COL_adul_time = testing[testing$resp==1 & testing$stimuli=="target" ,]

# FILTER FOR ERROR
COL_adul_error = testing


```

Above is how the new distribution looks like. In total, we keep `r length(unique(COL_adul_time))`

### Preschooler italians

#### VT experiment (newnewborn)

This part reads the file with the results of the **VT** experiment. 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy BEFORE outlier removal.')}

# path to folder
pathf = (paste(pathres,"Preschooler/Newnewborn/", sep=""))

# list all files in the folder
lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE)

# read these files, add the participant_id, and merge in a single file
df = data.frame(matrix(ncol=6, nrow=0))
for (fil in lfiles){
  d <- read.csv(paste(pathres, "Preschooler/Newnewborn/", fil, sep=""), header=T)
  part_id = substr( as.character(fil), 18, 19)
  d$participant_id = part_id
  df <- rbind(df, d)
}

# change participant id so that it's not like adults
df$participant_id <- as.numeric(df$participant_id)
df$participant_id <- df$participant_id + 100
df$participant_id <- as.character(df$participant_id)

# remove training trials
testing = df[df$type_test=="testing",]

# rename a column name that can be confusing
names(testing)[names(testing) == 'condition'] <- 'group'

# correct an error in terms of terminology
testing$group[testing$group=="decrease"] <- "decreasing"

# create an error column
testing$errors <- 0
testing$errors[testing$resp==0] <- 1

# create situation column (whether stimuli increased or decreased)
testing$situation <- "dec"
testing$situation[testing$amorce < testing$test] <- "inc"

# create stimuli column
testing$stimuli <- "non-target"
testing$stimuli[(testing$group == "decreasing" & testing$situation == "dec") | (testing$group == "increasing" & testing$situation == "inc")] <- "target"

# create condition column
testing$condition <- "incongruent"
testing$condition[(testing$situation == "dec" & testing$position == -15) | (testing$situation == "inc" & testing$position == 15)] <- "congruent"

# rename side column
testing %>%
  mutate(position = case_when(position == -15 ~ "Left", 
                              position == 15 ~ "Right")) -> testing

# Remove outlier
aggreg_correct = aggregate(testing$resp, by=list(group = testing$group, participant_id=testing$participant_id), FUN=mean)

ggplot(aggreg_correct, aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 66) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

list_part_remove_VT_presch <- aggreg_correct$participant_id[aggreg_correct$x <= 0.66]

```

This plot shows the histogram of accuracy of the participants. Here, we decide to remove participants who have an accuracy lower than 66% in at least one group (decreasing or increasing).
Doing so, we remove `r length(list_part_remove_VT_presch)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of accuracy AFTER outlier removal.')}

# After removing the participants, the distribution looks like this:
ggplot(aggreg_correct[!(aggreg_correct$participant_id %in%list_part_remove_VT_presch),], aes(x=x*100, fill=group)) +
  geom_histogram() +
  facet_grid(group ~ .) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = 62.5) +
  xlim(0,100) +
  labs(x="Accuracy (%)") +
  guides(fill=FALSE)

# remove participant according to the outlier methods
#testing_noout1 <- testing[!(testing$participant_id %in% list_part_remove_VT),]
#testing_chosen <- testing_noout1[!(testing_noout1$participant_id %in% 1058),] # see below for the reason why we exclude this one too
# -> we filter outlier later, when gathering all files

# add index
testing$index <- c(nextindex:(nextindex + nrow(testing) - 1))
nextindex <- nextindex + nrow(testing) + 1


# FILTER FOR TIME: select only good answers and target stimuli for time
VT_time_pres = testing[testing$resp==1 & testing$stimuli=="target",]

VT_time_pres = VT_time_pres[!(VT_time_pres$participant_id %in% list_part_remove_VT_presch),]

# FILTER FOR ERRORS: select all for errors
VT_error_pres = testing
VT_error_pres = VT_error_pres[!(VT_error_pres$participant_id %in% list_part_remove_VT_presch),]

```

Above is how the new distribution looks like. In total, we keep `r length(unique(VT_time_pres$participant_id))` participants.

#### Baskets

This part reads the file with the results of the **basket** experiment.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

files <- list.files(path = paste(pathres, "Preschooler/Baskets/", sep=""),pattern = ".csv", full.names = TRUE)
temp <- lapply(files, fread, sep=",")

# merge them inside a single file
baskets <- rbindlist(temp)

list_all_bask = c()
for (el in unique(baskets$participant)){
  subdf =  baskets[baskets$participant==el,]
  startbask = subdf$basket[1]
  list_all_bask = c(list_all_bask, startbask)
}

# select only the testing session
basket_testing = baskets[baskets$session=="testing",]
names(basket_testing)[names(basket_testing) == "participant"] <- "participant_id"

# add a new variable for the bias + change column names
basket_testing %>%
  mutate(bias = ifelse(((basket == 3 & position_dot==3) | (basket == 8 & position_dot==8)), "L", ifelse(((basket == 8 & position_dot==3) | (basket == 3 & position_dot==8)), "R", "N")),
    position_dot = case_when(position_dot == "3" ~ "Ball on 3rd pos",
                               position_dot == "8" ~ "Ball on 8th pos"),
    basket = factor(basket)) -> basket_testing


### Also compute a new df for the first frame
baskets$index_bypart <- 0
for (part in unique(baskets$participant)){
  baskets$index_bypart[baskets$participant==part] <- c(1:nrow(baskets[baskets$participant==part,]))}

# Add the experiment value
baskets$exp <- "Preschooler"

#
baskets %>%
  filter(session == "testing" & (index_bypart == 4 | index_bypart == 19)) %>%
  mutate(order_par = ifelse(((position_dot == 3 & index_bypart ==4) | (position_dot == 8 & index_bypart ==19)), "Order 3 then 8", "Order: 8 then 3"),
         bias = ifelse(((basket == 3 & position_dot==3) | (basket == 8 & position_dot==8)), "L", ifelse(((basket == 8 & position_dot==3) | (basket == 3 & position_dot==8)), "R", "N")),
         position_dot = case_when(position_dot == 3 ~ "Ball on 3rd pos",
                               position_dot == 8 ~ "Ball on 8th pos"),
         basket = factor(basket)) %>%
  rename(participant_id = participant)-> basket_first_test_pres

# add the order value in big dataframe too
baskets %>%
  filter(session == "testing" & (index_bypart == 4)) %>%
  mutate(order_par = ifelse((position_dot == 3 & index_bypart ==4), "Order: 3 then 8", "Order: 8 then 3")) %>%
  rename(participant_id = participant) %>%
  select(participant_id, order_par)-> basket_per_part_pres

basket_testing_pres <- merge(basket_testing, basket_per_part_pres, by="participant_id", all.x=TRUE)

# merge with questionnaire
baskets_quest_pres <- merge(basket_testing_pres, quest2021, by="participant_id", all.x=TRUE)
# baskets_quest %>%
#   mutate(bias_cards_free = case_when(bias_cards_free=="L" ~ "Cards: L",
#                                bias_cards_free=="R" ~ "Cards: R",
#                                bias_cards_free=="N" ~ "Cards: N")) -> baskets_quest
baskets_quest_pres$exp <- "Adults"


```

### Merge files together

#### Cards

Here, we merge the information coming from the cards file from **Himbas 2021** and **Himbas 2022**.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# rename and select some columns in cards 2021
cards2021 %>%
  select(-c(shape, strategy, orientation, time, X)) %>%
  rename(participant_id = ID,
         shape = shape_plot, 
         strategy = strategy_plot) -> cards2021_ok

# rename and select some columns in cards 2022
cards2022_ok <- cards2022
cards2022_ok$order_placement <- NA

# add year and type
cards2021_ok$exp <- "Himbas2021"
cards2022_ok$exp <- "Himbas2022"

# order by column name
cards2022_ok <- cards2022_ok[ , order(names(cards2022_ok))]
cards2021_ok <- cards2021_ok[ , order(names(cards2021_ok))]

# merge two files
cards_all <- rbind(cards2021_ok, cards2022_ok)

# adapt so that all names are similar
cards_all %>%
  mutate(type= case_when(type=="Dots (free)" ~ "Dots",
                         type=="Dots" ~ "Dots",
                         type=="Digits" ~ "Digits",
                         type=="Numbers (free)" ~ "Digits", 
                         type=="Dots (linear)" ~ "DotsLine",
                         type=="Colors" ~ "Colors")) -> cards_all

```

#### Questionnaire

Here, we merge the information coming from the questionnaire from **Himbas 2021** and **Himbas 2022**. Please note that this involves deleting a few columns that were specific to one experiment and not the other, or creating empty NA columns.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# change Questionnaire 2022
quest2022_ok <- quest2022
quest2022_ok %>%
  mutate(LeftHanded = case_when(LeftHanded=="no" ~ "right",
                                LeftHanded=="yes" ~ "left")) %>%
  rename(lateralisation = LeftHanded, 
         Ethnicity=ethnicity) %>%
  mutate(ScoreMatrices_percent = (ScoreMatrices*100)/8) %>%
  select(-c(Stress, CowsGoats, Village_specific, EyeMedicine, EyeVision, OpuwoReason, Languages, LanguagesN, NumChildrenSchool, NumSchoolAgeChildren, OpuwoLife, SchoolNumYrs, ScoreMatrices)) -> quest2022_ok
#quest2022_ok <- merge(quest2022_ok, df_cards_all, all.x=TRUE, all.y=FALSE, by="participant_id")

# change Questionnaire 2021
quest2021_ok <- quest2021
quest2021_ok %>%
  add_column(UrbanKnow = NA,
             UrbanPref= NA,
             UrbanIndex= NA,
             CultureChangeWorry= NA,
             LikeOpuwo= NA,
             uknow= NA,
             upref= NA, 
             uind= NA,
             ScoreMCT= NA,
             mct= NA) %>%
  mutate(Gender = case_when(Gender=="male"~"Male",
                            Gender=="female"~"Female")) %>%
  mutate(score_all_num = (as.numeric(calcul_score_percent) + as.numeric(comp_score_percent))/2) %>%
  mutate(numab = as.numeric(cut_number(score_all_num,2))) %>%
  mutate(numab = ifelse(numab == 1, "low", "high")) %>%
  rename(NumericalAbility=num_score,
         ScoreMatrices_percent=mat_score_percent,
         TotalLanguagesSpoken = nb_lang_spoken, 
         English=english, 
         dots = bias_cards_free,
         OpuwoThisYear = opuwo_year,
         LiteracyScore=literacy) %>%
  select(-c(score_all_num, count3when8, count8when8, count3when3, count8when3, percent_L_bias, percent_R_bias, software, interviewer, wealth, wealth_group, type_school, stress_life, stress_now, day, opuwo_reason, opuwo_life, calcul_score_percent, comp_score_percent, counting_score, eye_problem, eye_surgery,  knownum_left, numcal, numcomp, numcount, level_education, mat_score, opuwo_year_group, dots)) -> quest2021_ok

# Merge both questionnaire
quest2022_ok <- quest2022_ok[ , order(names(quest2022_ok))]
quest2022_ok$exp="Himbas2022"
quest2021_ok <- quest2021_ok[ , order(names(quest2021_ok))]
quest2021_ok$exp="Himbas2021"
quest_all <- rbind(quest2021_ok, quest2022_ok)

# Merge questionnaire with data from cards
cards_spread <- spread(cards_all[, c("participant_id", "type", "order", "exp"),], type, order)
cards_spread$`<NA>` <- NULL
quest_all <- merge(quest_all, cards_spread, all.x=TRUE, all.y=TRUE, by=c("participant_id", "exp"))
quest_all_gat <- gather(quest_all, type, order, Digits:DotsLine)

# add column knownum for quest2022
quest_all$knownum[quest_all$exp=="Himbas2022"] <- "no"
quest_all$knownum[quest_all$exp=="Himbas2022" & (quest_all$Digits=="R" | quest_all$Digits=="L")] <- "yes"

```

#### Baskets

We merge **Himbas 2021**, **Preschooler**, **Adults** files together.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# remove a column that prevents the files to be binded together
basket_testing_pres <- basket_testing_pres[,-c("Part_age")]
baskets_quest_pres <- baskets_quest_pres[,-c("Part_age")]
basket_per_part_pres <- basket_per_part_pres[,-c("Part_age")]
basket_first_test_pres <- basket_first_test_pres[,-c("Part_age")]

# add exp to testing files
basket_testing_adul$exp <- "Adults"
basket_testing_himb$exp <- "Himbas2021"
basket_testing_pres$exp <- "Preschooler"
baskets_quest_adul$exp <- "Adults"
baskets_quest_himb$exp <- "Himbas2021"
baskets_quest_pres$exp <- "Preschooler"
basket_per_part_adul$exp <- "Adults"
basket_per_part_himb$exp <- "Himbas2021"
basket_per_part_pres$exp <- "Preschooler"
basket_first_test_adul$exp <- "Adults"
basket_first_test_himb$exp <- "Himbas2021"
basket_first_test_pres$exp <- "Preschooler"

# merge all files together
baskets_quest <- rbind(baskets_quest_adul, baskets_quest_himb, baskets_quest_pres)
basket_testing <- rbind(basket_testing_adul, basket_testing_himb, basket_testing_pres)
basket_first_test <- rbind(basket_first_test_adul, basket_first_test_himb, basket_first_test_pres)
basket_per_part <- rbind(basket_per_part_adul, basket_per_part_himb, basket_per_part_pres)

```

#### GazeTracking

We did not merge the files of Gaze tracking together because the experiment were slightly different: in Himbas 2022, there were no cross in the middle during the flashing 12 dots phase.
Also, these files were not pre-processed in the same way, which makes it more difficult to merge them.


#### VT, Square, Candies, Snakes, Luminance

Here, we merge VT experiment files from **Adults**, **Preschooler**, **Himbas 2021** and **Himbas 2022**, as well as Squares, Candies, and Snakes experiment from **Himbas 2022** all together in a same file.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

####################################################

#### TIME ####

####################################################

# First, in order to be able to compare all experiment on the same level, we change a little bit the terminology for the VT (newnewborn experiment)
NEUT_time$group[NEUT_time$group=="small"] <- "decreasing"
NEUT_time$group[NEUT_time$group=="big"] <- "increasing"
CAND_time$group[CAND_time$group=="small"] <- "decreasing"
CAND_time$group[CAND_time$group=="big"] <- "increasing"
SNAK_time$group[SNAK_time$group=="small"] <- "decreasing"
SNAK_time$group[SNAK_time$group=="big"] <- "increasing"

CAND_adul_time$group[CAND_adul_time$group=="small"] <- "decreasing"
CAND_adul_time$group[CAND_adul_time$group=="big"] <- "increasing"
SNAK_adul_time$group[SNAK_adul_time$group=="small"] <- "decreasing"
SNAK_adul_time$group[SNAK_adul_time$group=="big"] <- "increasing"

# Write exp name
VT_time_2021$exp <- "VT_2021"
VT_time_2022$exp <- "VT_2022"
VT_time_adul$exp <- "VT_adul"
VT_time_pres$exp <- "VT_pres"
NEUT_time$exp <- "Squares_2022"
CAND_time$exp <- "Candies_2022"
SNAK_time$exp <- "Snakes_2022"
CAND_adul_time$exp <- "Candies_adul"
SNAK_adul_time$exp <- "Snakes_adul"
COL_time$exp <- "Luminance_2022"
COL_adul_time$exp <- "Luminance_adul"

# add amorce column in order to merge with VT
NEUT_time$amorce <- 12
CAND_time$amorce <- 12
SNAK_time$amorce <- 12
CAND_adul_time$amorce <- 12
SNAK_adul_time$amorce <- 12
COL_time$amorce <- NA
COL_adul_time$amorce <- NA

# merge the files with time for all experiment
time <- rbind(VT_time_2021[,c("index", "amorce", "participant_id", "group", "condition", "stimuli", "time",  "exp")],
              VT_time_2022[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time",  "exp")],
              VT_time_adul[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time",  "exp")],
              VT_time_pres[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time",  "exp")],
              NEUT_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")], 
              CAND_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")], 
              SNAK_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")],
              CAND_adul_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")],
              SNAK_adul_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")],
              COL_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")],
              COL_adul_time[,c("index","amorce","participant_id", "group", "condition", "stimuli", "time", "exp")])


# remove outliers of all experiments
paste("The number of participant before removing outliers is: ", as.character(length(unique(time$participant_id))))


#time <- time[!(time$participant_id %in% c(list_part_remove_VT_2021,list_part_remove_VT_2022, list_part_remove_neut, list_part_remove_cand, list_part_remove_snak, list_part_remove_col, list_part_remove_VT_adults, list_part_remove_VT_presch, list_part_remove_adul_cand, list_part_remove_adul_snak)),]


paste("The number of participant after removing outliers is: ", as.character(length(unique(time$participant_id))))

# aggregate by participant_id: compute mean and median time
time %>%
  dplyr::group_by(participant_id, group, condition, stimuli, exp) %>%
  dplyr::summarize(mean_time = mean(time),
                   median_time = median(time)) -> time_ag


####################################################

#### ERRORS ####

####################################################

# Same steps as before
NEUT_error$group[NEUT_error$group=="small"] <- "decreasing"
NEUT_error$group[NEUT_error$group=="big"] <- "increasing"
CAND_error$group[CAND_error$group=="small"] <- "decreasing"
CAND_error$group[CAND_error$group=="big"] <- "increasing"
SNAK_error$group[SNAK_error$group=="small"] <- "decreasing"
SNAK_error$group[SNAK_error$group=="big"] <- "increasing"

CAND_adul_error$group[CAND_adul_error$group=="small"] <- "decreasing"
CAND_adul_error$group[CAND_adul_error$group=="big"] <- "increasing"
SNAK_adul_error$group[SNAK_adul_error$group=="small"] <- "decreasing"
SNAK_adul_error$group[SNAK_adul_error$group=="big"] <- "increasing"

# add experiment name
VT_error_2021$exp <- "VT_2021"
VT_error_2022$exp <- "VT_2022"
VT_error_adul$exp <- "VT_adul"
VT_error_pres$exp <- "VT_pres"
NEUT_error$exp <- "Squares_2022"
CAND_error$exp <- "Candies_2022"
SNAK_error$exp <- "Snakes_2022"
SNAK_adul_error$exp <- "Snakes_adul"
CAND_adul_error$exp <- "Candies_adul"
COL_error$exp <- "Luminance_2022"
COL_adul_error$exp <- "Luminance_adul"

# add amorce column in order to merge with VT
NEUT_error$amorce <- 12
CAND_error$amorce <- 12
SNAK_error$amorce <- 12
CAND_adul_error$amorce <- 12
SNAK_adul_error$amorce <- 12
COL_error$amorce <- NA
COL_adul_error$amorce <- NA

# merge all errors file
error <- rbind(VT_error_2021[,c("index", "amorce", "participant_id", "group", "condition", "stimuli", "errors", "exp")], 
               VT_error_2022[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")],
               VT_error_adul[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")],
               VT_error_pres[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")],
              NEUT_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")], 
              CAND_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")], 
              SNAK_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")],
              CAND_adul_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")], 
              SNAK_adul_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")],
              COL_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")],
              COL_adul_error[,c("index","amorce","participant_id", "group", "condition", "stimuli", "errors", "exp")])


# remove outliers
#error <- error[!(error$participant_id %in% c(list_part_remove_VT_2021, list_part_remove_VT_2022, list_part_remove_neut, list_part_remove_cand, list_part_remove_snak, list_part_remove_col, list_part_remove_VT_adults, list_part_remove_VT_presch, list_part_remove_adul_cand, list_part_remove_adul_snak)),]

# aggregate by participant_id: mean error
error %>%
  dplyr::group_by(participant_id, group, condition, stimuli, exp) %>%
  dplyr::summarize(mean_error = mean(errors)) -> error_ag

####################################################

#### MERGING TIME AND ERRORS ####

####################################################

# merge the time and errors file (big file)
all <- merge(time, error, by=c("index", "amorce", "participant_id", "group", "stimuli", "condition", "exp"), all.x=TRUE, all.y=TRUE)

# merge the time and errors file (aggregated file)
all_ag <- merge(time_ag, error_ag, by=c("participant_id", "group", "stimuli", "condition", "exp"), all.x=TRUE, all.y=TRUE)

# CREATE INVEFI COLUMNS
all_ag$correct <- 1 - (all_ag$mean_error)
all_ag$invefi <- all_ag$mean_time / all_ag$correct
all_ag$invefi_med <- all_ag$median / all_ag$correct

# remove useless "correct" columns
all_ag <- all_ag[ , -which(names(all_ag) %in% c("correct"))]

####################################################

#### ADDING CARDS AND QUESTIONNAIRE INFO ####

####################################################


# Merge data with questionnaire 
all <- merge(all, quest_all, all.x=TRUE, all.y=FALSE, by="participant_id")
all_ag <- merge(all_ag, quest_all, all.x=TRUE, all.y=FALSE, by="participant_id")


# correct a few typos
all$dots[all$Dots=="N "] <- "N"
all_ag$dots[all_ag$Dots=="N "] <- "N"

# correct a problem when merging
all %>%
  rename(type_exp = exp.y,
         exp = exp.x) -> all
all_ag %>%
  rename(type_exp = exp.y,
         exp = exp.x) -> all_ag
# Write the final table
# write.csv(all_ag, paste(path, "all.csv"), row.names=FALSE)

```

By removing all participant that have failed at least one experiment, we end up with `r `length(unique(all_ag$participant_id))` participants.



<!-- ### Create file for Serge's program -->

<!-- This section aims to create a clean file usable by the software of Serge. It does not change anything to the content, it only format the data in a special way.  -->

<!-- #### Time -->


<!-- ```{r , echo=F, message=F, warning=FALSE} -->

<!-- # Select only important columns -->
<!-- final_sub <- testing[,c("participant_id", "group", "stimuli", "condition", "time", "errors")] -->

<!-- final_sub <- final_sub[final_sub$errors == 0 ,] -->

<!-- # Aggregate for time median -->
<!-- final_subb1 <- aggregate(list(med_time = final_sub$time), by=list( -->
<!--   participant_id = final_sub$participant_id, -->
<!--   group = final_sub$group, -->
<!--   stimuli = final_sub$stimuli, -->
<!--   condition = final_sub$condition), FUN=median) -->

<!-- # Spread (mulitple times..) -->

<!-- ### Spread for group (inc/dec) -->
<!-- final_subb11 <- spread(final_subb1, group, med_time) -->

<!-- ### Spread for stimuli -->
<!-- dec <- spread(final_subb11[, -which(names(final_subb11) %in% c("increasing"))], stimuli, decreasing) -->
<!-- inc <- spread(final_subb11[, -which(names(final_subb11) %in% c("decreasing"))], stimuli, increasing) -->

<!-- ### Spread for condition (cong/inc) -->
<!-- dec_t <- spread(dec[, -which(names(dec) %in% c("non-target"))], condition, `target`) -->
<!-- dec_nt <- spread(dec[, -which(names(dec) %in% c("target"))], condition, `non-target`) -->
<!-- inc_t <- spread(inc[, -which(names(inc) %in% c("non-target"))], condition, `target`) -->
<!-- inc_nt <- spread(inc[, -which(names(inc) %in% c("target"))], condition, `non-target`) -->

<!-- ### Change names -->
<!-- colnames(dec_t) <- c("participant_id", "VT_small_TAR_cong", "VT_small_TAR_inc") -->
<!-- colnames(dec_nt) <- c("participant_id", "VT_small_NONTAR_cong", "VT_small_NONTAR_inc") -->
<!-- colnames(inc_t) <- c("participant_id", "VT_big_TAR_cong", "VT_big_TAR_inc") -->
<!-- colnames(inc_nt) <- c("participant_id", "VT_big_NONTAR_cong", "VT_big_NONTAR_inc") -->

<!-- ### Merge all -->
<!-- all <- merge(dec_t, dec_nt, by="participant_id") -->
<!-- all <- merge(all, inc_t, by="participant_id") -->
<!-- all <- merge(all, inc_nt, by="participant_id") -->

<!-- # add column for participants to remove -->
<!-- all$VT_remove <- 0 -->
<!-- all$VT_remove[all$participant_id %in% list_part_remove_VT] <- 1 -->

<!-- ``` -->

<!-- #### Errors -->

<!-- ```{r , echo=F, message=F, warning=FALSE} -->

<!-- # Aggreagte by mean errors -->
<!-- final_subb2 <- aggregate(list(mean_error = final_sub$errors), by=list( -->
<!--   participant_id = final_sub$participant_id, -->
<!--   group = final_sub$group, -->
<!--   stimuli = final_sub$stimuli, -->
<!--   condition = final_sub$condition), FUN=mean) -->

<!-- # Spread (same as below) -->
<!-- final_subb22 <- spread(final_subb2, group, mean_error) -->

<!-- dec <- spread(final_subb22[, -which(names(final_subb22) %in% c("increasing"))], stimuli, decreasing) -->
<!-- inc <- spread(final_subb22[, -which(names(final_subb22) %in% c("decreasing"))], stimuli, increasing) -->
<!-- dec_t <- spread(dec[, -which(names(dec) %in% c("non-target"))], condition, `target`) -->
<!-- dec_nt <- spread(dec[, -which(names(dec) %in% c("target"))], condition, `non-target`) -->
<!-- inc_t <- spread(inc[, -which(names(inc) %in% c("non-target"))], condition, `target`) -->
<!-- inc_nt <- spread(inc[, -which(names(inc) %in% c("target"))], condition, `non-target`) -->

<!-- colnames(dec_t) <- c("participant_id", "VT_small_TAR_cong", "VT_small_TAR_inc") -->
<!-- colnames(dec_nt) <- c("participant_id", "VT_small_NONTAR_cong", "VT_small_NONTAR_inc") -->
<!-- colnames(inc_t) <- c("participant_id", "VT_big_TAR_cong", "VT_big_TAR_inc") -->
<!-- colnames(inc_nt) <- c("participant_id", "VT_big_NONTAR_cong", "VT_big_NONTAR_inc") -->

<!-- all_err <- merge(dec_t, dec_nt, by="participant_id") -->
<!-- all_err <- merge(all_err, inc_t, by="participant_id") -->
<!-- all_err <- merge(all_err, inc_nt, by="participant_id") -->
<!-- all_err$VT_remove <- 0 -->
<!-- all_err$VT_remove[all$participant_id %in% list_part_remove_VT] <- 1 -->

<!-- ``` -->

<!-- Using this file, we can have a closer look at the errors the participants have made.  -->

<!-- ```{r , echo=F, message=F, warning=FALSE} -->

<!-- for (part_id in unique(all_err$participant_id[all_err$VT_remove == 0])){ -->
<!--   subdf <- all_err[all_err$participant_id == part_id,] -->
<!--   for (num_col in 2:(ncol(all_err)-1)){ -->
<!--     if (as.numeric(subdf[num_col]) > 0.75){ -->
<!--       print(part_id) -->
<!--     } -->
<!--   } -->
<!-- } -->

<!-- testing_chosen <- testing_chosen[!(testing_chosen$participant_id %in% 1058),] -->

<!-- ``` -->

<!-- We observe that only one participant (except the one already removed) really failed one condition. It means that when looking at the average values, he was above 66% of good answer, but we realize looking at the data that he really failed (around 75% of the time) the *non-target*-*decreasing* condition.  -->

<!-- So now we will work with `length(unique(testing_chosen$participant_id))` participants. -->


## Demographic data

### Himbas 2021

```{r , echo=F, message=F, warning=FALSE}

# print summary
summary(quest2021)

```


#### Ethnicity

There are `r nrow(quest2021)` participants, including `r nrow(quest2021[quest2021$Ethnicity=="himba",])` Himbas, `r nrow(quest2021[quest2021$Ethnicity=="herero",])` Hereros, and `r nrow(quest2021[quest2021$Ethnicity=="zemba",])` Zemba. Please refer to [Collect personal information] for more information.

#### Sex

In total, there are `r nrow(quest2021[quest2021$Gender=="female",])` female participants and `r nrow(quest2021[quest2021$Gender=="male",])` male participants (male ratio: `r round((nrow(quest2021[quest2021$Gender=="male",])/(nrow(quest2021[quest2021$Gender=="female",]) + nrow(quest2021[quest2021$Gender=="male",])))*100)`%)

#### Age

The mean age is `r round(mean(quest2021$Age, na.rm=TRUE),2)` (SD=`r round(sd(quest2021$Age, na.rm=TRUE),2)`).

```{r plot questionnaire  age sex, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of male and female participants with age.')}

ggplot(quest2021, aes(x=Age))+
  geom_histogram(alpha=0.5) +
  labs(fill = "Sex", x="Age") +
  theme_bw(base_size=15)

ggplot(quest2021, aes(x=Age, fill=Gender))+
  geom_density(alpha=0.5) +
  labs(fill = "Sex", x="Age") +
  theme_bw(base_size=15)

```

#### Schooling

Some Himbas have been to school; it can either a *mobile* school, a *rural* school or a school in *town.* More information about the type of school and the highest grade they completed is available in the database (for pratical reasons, we do not plot it here). In total, in our database, `r nrow(quest2021[quest2021$School=="yes",])` participants out of `r nrow(quest2021)` have attended school (`r round(nrow(quest2021[quest2021$School=="yes",])/nrow(quest2021)*100)` %). More specifically, `r round(nrow(quest2021[quest2021$Gender=="female" & quest2021$School=="no",])/(nrow(quest2021[quest2021$Gender=="female",]))*100)`% of the female participants never attended School, while only `r round(nrow(quest2021[quest2021$Gender=="male" & quest2021$School=="no",])/(nrow(quest2021[quest2021$Gender=="male",]))*100)`% of the male participants never attended school. 

```{r plot questionnaire age school, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of participants that have been to school across age.')}

ggplot(quest2021, aes(x=Age, fill=School))+
  geom_density(alpha=0.5) +
  labs(fill = "Went to school?", x="Age") +
  theme_bw(base_size=15)

```

#### Literacy

Some Himbas have been to school; it can either a *mobile* school, a *rural* school or a school in *town.* More information about the type of school and the highest grade they completed is available in the database (for pratical reasons, we do not plot it here). In total, in our database, `r nrow(quest2021[quest2021$school=="yes",])` participants out of `r nrow(quest2021)` have attended school (`r round(nrow(quest2021[quest2021$School=="yes",])/nrow(quest2021)*100)` %). More specifically, `r round(nrow(quest2021[quest2021$Gender=="female" & quest2021$School=="no",])/(nrow(quest2021[quest2021$Gender=="female",]))*100)`% of the female participants never attended school, while only `r round(nrow(quest2021[quest2021$Gender=="male" & quest2021$School=="no",])/(nrow(quest2021[quest2021$Gender=="male",]))*100)`% of the male participants never attended school. 

```{r plot questionnaire age school 1, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of participants that have been to school across age.')}

ggplot(quest2021, aes(x=School, fill=literacy_group))+
  geom_histogram(stat="count", position = "dodge", alpha=0.8) +
  labs(fill = "Literacy", x="Went to school?") +
  theme_bw(base_size=15)

```
  

#### Modernity

 - **Clothing**. In total, `r nrow(quest2021[quest2021$Outfit=="traditionnal",])` of the participants were dressed in a traditional way: `r round(nrow(quest2021[quest2021$Outfit=="traditionnal" & quest2021$Gender=="female",])/nrow(quest2021[quest2021$Gender=="female",])*100)`% of the female participants and `r round(nrow(quest2021[quest2021$Outfit=="traditionnal" & quest2021$Gender=="male",])/nrow(quest2021[quest2021$Gender=="male",])*100)`% of the male participants.

 - **Telephone**. `r round(nrow(quest2021[quest2021$telephone=="yes",])/nrow(quest2021)*100)` % of the participants have a telephone, with no gender or age difference between the two groups.

 - **City**. Himba people mostly go to the city to buy some products or go to the hospital (see database for more specific information). The figure below shows the distribution of male and female participants going to the city 0 time (0), 1 time (1), 2 times (2), 3 times (3), 4 times (4), 5 times or more (5).
  
```{r plot questionnaire opuwo, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Exposure to the city for each gender. 0 means that the participant has not been not the city this year, 1 means only s.he has been 1 time to the city, 2 means 2 times, 3 means 3 times, 4 means 4 tours, and 5 means 5 times or more.')}


quest2021$opuwo_year_group <- as.factor(quest2021$opuwo_year_group)

ggplot(quest2021, aes(x=opuwo_year_group, fill=Gender))+
  geom_histogram(stat="count", position="dodge", binwidth = 0.3) +
  labs(fill = "Sex", x="number of times to the city (this year)") +
  #facet_grid(. ~ village_name) +
  theme_bw(base_size=15) 
```


#### Numerical data


##### Overall numerical score

The numerical scores is the average of the counting score, the comparison score and the computation score (see [Investigating Himbas numerical abilities] for more information). 

```{r numerical num score, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of numerosity scores for each gender. The higher the numerosity score, the better the performance during the different tasks.')}

quest2021$num_score <- as.numeric(quest2021$num_score)
ggplot(quest2021, aes(x=num_score, fill=Gender))+
  geom_density(alpha=0.5) +
  theme_bw(base_size=15) +
  labs(x="Numerosity score")

```


```{r numerical num score 22, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of the different numerosity scores (for calculation, comparaison, counting, a mix of comparaison+calculation, and all score together) for each gender. The higher the numerosity score, the better the performance during the different tasks.'), fig.width=13, fig.height=6}

quest2021_2 <- quest2021
quest2021_2$CalComp <- (quest2021_2$comp_score_percent + quest2021_2$calcul_score_percent) / 2

quest2021_2 <- gather(quest2021_2, condition, measurement, c(num_score:comp_score_percent, CalComp))

quest2021_2 %>%
  mutate(condition = case_when(condition == "calcul_score_percent" ~ "Calcul score (%)",
                               condition == "comp_score_percent" ~ "Comparaison score (%)",
                               condition == "counting_score" ~ "Counting score (%)",
                               condition =="CalComp" ~ "Calcul + comp scores (%)",
                               condition =="num_score" ~ "Overall score (%)"),
         school = case_when(School == "no" ~ "School: no",
                            School == "yes" ~ "School: yes"),
         condition = factor(condition, levels=c("Calcul score (%)", "Comparaison score (%)",  "Counting score (%)", "Calcul + comp scores (%)", "Overall score (%)"))) -> quest2021_2

         
ggplot(quest2021_2, aes(x=measurement, fill=Gender))+
  geom_density(alpha=0.5) +
  facet_grid(. ~ condition) +
  theme_bw(base_size=15) +
  labs(x="Numerosity score")


```


```{r numerical num score age, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Different numerosity scores for each gender and age.'), fig.width=13, fig.height=6}

#quest2021_2$measurement <- as.numeric(quest2021$measurement)
#quest2021_2$age <- as.numeric(quest2021$age)

ggplot(quest2021_2, aes(x=Age, y=measurement, color=Gender, fill=Gender))+
  geom_point(alpha=0.5) +
  facet_grid(. ~ condition) +
  geom_smooth(method="lm", alpha=0.3) +
  theme_bw(base_size=15) +
  labs(y="Numerosity score")

```


```{r numerical num score agee, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('DIfferent numerosity scores for each gender and age.'), fig.width=12, fig.height=9}

ggplot(quest2021_2[is.na(quest2021_2$School)==FALSE,], aes(x=Age, y=measurement, color=Gender, fill=Gender))+
  geom_point(alpha=0.5) +
  facet_grid(school ~ condition) +
  geom_smooth(method="lm", alpha=0.3) +
  theme_bw(base_size=15) +
  labs(y="Numerosity score") +
  ylim(0, 105)

```



##### WAIS matrice score

The WAIS matrice score is the score obtained to the WAIS matrice experiment converted to a percentage (see [Investigating Himbas numerical abilities]).

```{r numerical mat score, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of WAIS matrice score by gender.')}

quest2021$mat_score_percent <- as.numeric(quest2021$mat_score_percent)
ggplot(quest2021, aes(x=mat_score_percent, fill=Gender))+
  geom_density(alpha=0.5) +
  theme_bw(base_size=15) +
  labs(x="Raven matrice score (in %)")

```

Here too, the matrices are negatively correlated with age.

```{r numerical mat score age, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('WAIS matrice score by age and sex.')}
quest2021$num_score <- as.numeric(quest2021$mat_score_percent)
ggplot(quest2021, aes(x=Age, y=mat_score_percent, color=Gender, fill=Gender))+
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", alpha=0.3) +
  theme_bw(base_size=15) +
  labs(y="Raven matrice score (%)")

```

#### Correlation all variables

To do the correlation heatmap, we converted the continuous variables into **categorical** variables and we compute the **crammer's V index**. Sometimes a classification based on the median was used (scores with two categories: high and low), sometimes we chose to categorize into fixed categories (age with 4 different age range: 15-25, 25-35, 35-45, 45+).

```{r plot correlation, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Heatmap of correlation between the variables (all converted to categorical variables).'), fig.height=10, fig.width=12}

# Check correlation using another method
# library(psych)
# ICC(data.frame(wealth = as.numeric(as.factor(quest2021$wealth)), opuwo = as.numeric(as.factor(quest2021$mat))))


# add additionnal column to transform continuous variables into categorical variables

# correlation with school
listcol <- c(3, 8:10, 15, 16, 21, 24, 26, 30, 31, 39, 41:44)

crammerv <- matrix(0, nrow=length(listcol), ncol=length(listcol))
colnames(crammerv) <- colnames(quest2021[,listcol])
rownames(crammerv) <- colnames(quest2021[,listcol])


for (my_var1 in listcol){
  for (my_var2 in listcol){
    name_variable1 = colnames(quest2021)[my_var1]
    name_variable2 = colnames(quest2021)[my_var2]

    sub <- data.frame(participant_id = quest2021$participant_id, varname1 = as.factor(quest2021[,my_var1]), varname2 = as.factor(quest2021[,my_var2]))
    sub <- na.omit(sub)
    
    sub %>%
      mutate(varname2 = factor(varname2),
             varname1 = factor(varname1)) %>%
      dplyr::group_by(varname1, varname2) %>% 
      dplyr::count() %>%
      tidyr::spread(varname2, n) %>%
      as.data.frame() %>% 
      `rownames<-`(.[,1]) %>% 
      select(-varname1)-> subdf
    
    subdf[is.na(subdf)] <- 0
    #chi2 = chisq.test(subdf, correct=F)
    # print(name_variable)
    

    cram1 <- cramer_v(subdf)
    #cram <- sqrt(chi2$statistic / sum(tbl))
    crammerv[name_variable1, name_variable2] <- cram1
    
    # print(paste("Chi2 p-value:", as.character(c(chi2$p.value)))) # reject independency if lower than 0.5
    # print(paste("Chi2 statistic:", as.character(c(chi2$statistic))))
    # print(paste("Crammer's V:", as.character())) # the smaller, the lower the correlation
  }
}

colnames(crammerv) <- c("Know numbers", "Gender", "Clothes", "School", "Literacy", "Stress in life", "Village", "Wealth" , "Opuwo", "English", "Telephone", "Age", "Comp score", "Counting score", "Calcul score" , "Matrice")
rownames(crammerv) <- c("Know numbers", "Gender", "Clothes", "School", "Literacy", "Stress in life", "Village", "Wealth" , "Opuwo", "English", "Telephone", "Age", "Comp score", "Counting score", "Calcul score" , "Matrice")

heatmaply_cor(x = crammerv,
              xlab = "",
              ylab = "",
              k_col = 2,
              k_row = 2)

# Investigation more precisely the direction of the correlation
# quest2021 %>%
#   group_by(gender, literacy_group) %>%
#   count() -> quest20212
# 
# kable(quest20212)

```

#### Regression analysis: summary

What variables reflects the exposure to the convention? We have performed different types of regression analysis (linear, generalized linear, with random effects, ..) using the following covariates:

 - calcul score
 - comp score
 - counting score
 - matrice score
 - school
 - telephone
 - lateralisation
 - age
 - know numbers
 - literacy

Please refer to the method for more information on these variables.

##### Ordering cards experiment

The variables that best predict whether the participants will order the cards from left to right or right to left are the following:

 - know numbers
 - matrice score
 - age
 - school

However, the variables that most predict whether the participant will order the cards (N) or not (L or R) are the following :  

 - literacy
 - know numbers
 - counting score
 - age
 
##### Gaze cards experiment

The variables placed as covariates in the model do not explain the variance observed in the data.

 
##### Reaction time experiment

The variables placed as covariates in the model do not explain the variance observed in the data.

##### Basket and ball experiment

 - age
 - calculation score
 - matrice score
 - know numbers
 - school
 - literacy
 

##### Conclusion

The variables that explain the most variation are:

 - knowing numbers
 - matrice score
 - age
 - school
 
And, to a smaller extent:
 - literacy
 - calculation score
 

### Himbas 2022

#### Ethnicity

```{r , echo=FALSE}

quest2022 <- quest2022[quest2022$participant_id %in% unique(cards2022$participant_id),]

summary(quest2022$ethnicity)
```

In total there are `r length(unique(quest2022$participant_id))`` participants who completed all tasks.

There are `r nrow(quest2022)` participants, including `r nrow(quest2022[quest2022$ethnicity=="Himba",])` Himbas, `r nrow(quest2022[quest2022$ethnicity=="Hakaona",])` Hereros, and `r nrow(quest2022[quest2022$ethnicity=="Zemba",])` Zemba. Please refer to [Collect personal information] for more information.

Hakaona language is a Bantu Language, sometimes considered a dialect of Herero. 

#### Sex

In total, there are `r nrow(quest2022[quest2022$Gender=="Female",])` female participants and `r nrow(quest2022[quest2022$Gender=="Male",])` male participants (male ratio: `r round((nrow(quest2022[quest2022$Gender=="Male",])/(nrow(quest2022[quest2022$Gender=="Female",]) + nrow(quest2022[quest2022$Gender=="Male",])))*100)`%)

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of gender.'), fig.width=5, fig.height = 5}

ggplot(quest2022, aes(x=Gender)) +
  geom_histogram(stat = "count") +
  theme_bw(base_size=15)

```

#### Age

The mean age is `r round(mean(quest2022$Age, na.rm=TRUE),2)` (SD=`r round(sd(quest2022$Age, na.rm=TRUE),2)`).

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of Age'), fig.width=7, fig.height = 5}

ggplot(quest2022, aes(x=Age)) +
  geom_histogram() +
  theme_bw(base_size=15)
```

```{r , echo=FALSE,message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of Age, filled by Gender'), fig.width=7, fig.height = 5}

ggplot(quest2022, aes(x=Age, fill=Gender)) +
  geom_density(alpha=0.3) +
  theme_bw(base_size=15)

```


#### Schooling 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Histogram of school'), fig.width=5, fig.height = 5}

ggplot(quest2022, aes(x=School)) +
  geom_histogram(stat = "count") +
  theme_bw(base_size=15)

```


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Number for participant present in each grade'), fig.width=7, fig.height = 5}

ggplot(quest2022, aes(x=Grade, fill=School)) +
  geom_histogram(alpha=0.5) +
  theme_bw(base_size=15)
  

```


##### School & Age 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Density of plots showing  the proportion of participant attending school for different ages.'), fig.width=8, fig.height = 5 }

ggplot(quest2022, aes(x=Age, fill=School)) +
  geom_density(alpha=0.5)+
  theme_bw(base_size=15) 
```


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average age (red dots) for participants having attended school (in blue) or not (in red). Each point is a participant.'), fig.width=8, fig.height = 5 }

ggplot(quest2022, aes(x=School, y=Age, fill=School)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15)

```


##### School & Gender 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Number of participants attending school, split by Gender.'), fig.width=7, fig.height = 5 }

ggplot(quest2022, aes(x=Gender, fill=School)) +
  geom_histogram(stat="count", alpha=0.6)+
  theme_bw(base_size=15)

```


#### Literacy

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Number of participant for each literacy score.'), fig.width=9, fig.height = 6}

ggplot(quest2022, aes(x=LiteracyScore)) +
  geom_histogram(stat="count") +
  theme_bw(base_size=15) 
```

##### Literacy & School

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Literacy score of participant attending school (red) and those who never did (blue)'), fig.width=9, fig.height = 6}

ggplot(quest2022, aes(x=LiteracyScore, fill=School)) +
  geom_histogram(stat = "count", alpha=0.6) +
  theme_bw(base_size=15)

ggplot(quest2022, aes(x=School, y=LiteracyScore, fill=School)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15)

```


##### Literacy & Age 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Mean age (red dot) of participant not able to read (red) and those able to read to a certain extent (blue). Each point is a participant.'), fig.width=9, fig.height = 6}

# ggplot(quest2022, aes(x=Age, y=LiteracyScore)) +
#   geom_point()+
#   geom_smooth(method="lm")+
#   theme_bw(base_size=15)


ggplot(quest2022, aes(x=literacy_group, y=Age, fill=literacy_group)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  guides(fill=FALSE)


```


##### Literacy & Gender 

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Mean age (red dot) of female participant (red) and male participants (blue). Each point is a participant.'), fig.width=9, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=LiteracyScore, fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15)
```

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Number of participant per gender, filled by Literacy group.'), fig.width=9, fig.height = 6}

ggplot(quest2022, aes(x=Gender, fill=as.factor(literacy_group))) +
  geom_histogram(stat="count", alpha=0.6)+
  theme_bw(base_size=15)


```

#### Modernity

##### Outfit

 - **Clothing**. In total, `r nrow(quest2022[quest2022$Outfit=="traditionnal",])` of the participants were dressed in a traditional way: `r round(nrow(quest2022[quest2022$Outfit=="traditionnal" & quest2022$Gender=="Female",])/nrow(quest2022[quest2022$Gender=="Female",])*100)`% of the female participants and `r round(nrow(quest2022[quest2022$Outfit=="traditionnal" & quest2022$Gender=="Male",])/nrow(quest2022[quest2022$Gender=="Male",])*100)`% of the male participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Number of participants who have a traditionnal versus modern outfil, colored by gender.'), fig.width=9, fig.height = 6}

ggplot(quest2022[is.na(quest2022$Outfit)==FALSE & quest2022$Outfit != "",], aes(x=Outfit, fill=Gender)) +
  geom_histogram(stat="count", alpha=0.6) +
  theme_bw(base_size=15)

```

**Telephone**:

 - **Telephone**. `r round(nrow(quest2022[quest2022$telephone=="yes",])/nrow(quest2022)*100)` % of the participants have a telephone, with no gender or age difference between the two groups.

```{r , echo=FALSE, fig.cap=capFig('Number of participants who possess a telephone, colored by gender.'), fig.width=9, fig.height = 6}

ggplot(quest2022[is.na(quest2022$telephone)==FALSE & quest2022$telephone != "",], aes(x=telephone, fill=Gender)) +
  geom_histogram(stat="count", alpha=0.6) +
  theme_bw(base_size=15)

```

##### Going to the city

 - **City**. Himba people mostly go to the city to buy some products or go to the hospital (see database for more specific information). The figure below shows the distribution of male and female participants going to the city 0 time (0), 1 time (1), 2 times (2), 3 times (3), 4 times (4), 5 times or more (5).

```{r , echo=FALSE, fig.cap=capFig('Number of participants who have been to the city N number of times this year, colored by gender.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=as.factor(OpuwoThisYear), fill=Gender)) +
  geom_histogram(stat="count", alpha=0.6) +
  theme_bw(base_size=15) +
  labs(x="Frequency to the city this year")
```

```{r , echo=FALSE, fig.cap=capFig('Number of participants who went to the city very few, sometimes, quite often.'), fig.width=8, fig.height = 6}

quest2022 %>%
  mutate(OpuwoLife2 = case_when(OpuwoLife == 0 ~ "1.Very few",
                               OpuwoLife == 1 ~ "2.Sometimes",
                               OpuwoLife == 2 ~ "3.Quite often")) -> quest2022

ggplot(quest2022[quest2022$Gender!="",], aes(x=OpuwoLife2, fill=Gender)) +
  geom_histogram(stat="count", alpha=0.6) +
  theme_bw(base_size=15)

```

##### Subjective taste

**Question**:	*Are you worried that the Himba culture is changing?* (for example: because of the influence of other cultures and modern things in Namibia)	

Possible **answers**:

 - (0) No 
 - (1) A little worried 
 - (2) Moderately worried 
 - (3) Very worried

```{r , echo=FALSE, fig.cap=capFig('Number of participants for each answer to the variable CultureChangeWorry.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=CultureChangeWorry, fill=Gender)) +
  geom_histogram(stat="count", alpha=0.6) +
  theme_bw(base_size=15)

```

**Question**: What do you think of the way of life in Opuwo or other big cities in Namibia?	

POssible **answers**:

 - (0) I do not like the way of life their 
 - (1) I like it only a little, but prefer village life 
 - (2) I like Opuwo life and village life the same 
 - (3) I like it a lot and wish that I could live there


```{r , echo=FALSE, fig.cap=capFig('Number of participants for each answer to the variable LikeOpuwo'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=LikeOpuwo, fill=Gender)) +
  geom_histogram(stat="count", alpha=0.6) +
  theme_bw(base_size=15)

```

The following variable named **UrbanIndex** is the mean of the 4 variables :
 - *Outfit* (traditional = 1, modern = 0)
 - *CultureChangeWorry*
 - *LikeOpuwo*
 - *Phone* (yes =1, no=0)
 
```{r , echo=FALSE, fig.cap=capFig('Number of participants for the variable Urban Index, colored by gender.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=UrbanIndex, fill=Gender)) +
  geom_density(alpha=0.6) +
  theme_bw(base_size=15)

```

**Urban Knowledge**:

Then the following variable shows how much the participant knows about modern world:

```{r , echo=FALSE, fig.cap=capFig('Number of participants for the variable Urban Knowledge, colored by gender.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=UrbanKnow, fill=Gender)) +
  geom_density(alpha=0.6) +
  theme_bw(base_size=15)

```

**Urban Preference**:

And this one how much this person subjectively prefers modern world:

```{r , echo=FALSE, fig.cap=capFig('Number of participants for the variable Urban Preference, colored by gender.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=UrbanPref, fill=Gender)) +
  geom_density(alpha=0.6) +
  theme_bw(base_size=15)

```

Normally, the variables *UrbanPref* and *UrbanIndex* should be correlated:

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Urban Preference and Urban Index. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanPref, y=UrbanIndex)) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm") +
  theme_bw(base_size=15)
```

As well as the other measures together:

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Urban Preference and Urban Knowledge. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanPref, y=UrbanKnow)) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm") +
  theme_bw(base_size=15)
```

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Urban Knowledge and Urban Index. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanKnow, y=UrbanIndex)) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

```


And they all should be correlated to the number of times going to Opuwo:

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Urban Preference and frequency to the city. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanPref, y=OpuwoThisYear)) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

```


```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Urban Knowledge and frequency to the city. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanKnow, y=OpuwoThisYear)) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm") +
  theme_bw(base_size=15) +
  ylim(0,12)
```

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Urban Index and frequency to the city. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanIndex, y=OpuwoThisYear)) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

```

Now, in order to understand how these variables are related to each other, we perform a PCA analysis:

```{r , echo=FALSE, fig.cap=capFig('Percentage of variance included explained by each principal component.'), fig.width=8, fig.height = 6}

# select relevant columns
quest2022 %>%
  mutate(CultureChangeWorry = as.numeric(CultureChangeWorry),
         LikeOpuwo = as.numeric(LikeOpuwo),
         UrbanIndex = as.numeric(UrbanIndex),
         UrbanKnow = as.numeric(UrbanKnow),
         UrbanPref = as.numeric(UrbanPref),
         OpuwoThisYear = as.numeric(OpuwoThisYear),
         OpuwoLife = as.numeric(OpuwoLife)) %>%
  select(CultureChangeWorry, LikeOpuwo, UrbanIndex, UrbanKnow, UrbanPref, OpuwoThisYear, OpuwoLife) -> subdf_pca

subdf_pca <- subdf_pca[complete.cases(subdf_pca), ]

# compute PCA
res.pca <- PCA(subdf_pca, graph = FALSE)

# Look at eigenvalues:
eigenvalues <- res.pca$eig
barplot(eigenvalues[, 2], names.arg=1:nrow(eigenvalues), 
       main = "Variances",
       xlab = "Principal Components",
       ylab = "Percentage of variances",
       col ="steelblue")
# Add connected line segments to the plot
lines(x = 1:nrow(eigenvalues), eigenvalues[, 2], 
      type="b", pch=19, col = "red")


```


```{r , echo=FALSE, fig.cap=capFig('Looking at the difference variable and how they are captured by the different PCA.'), fig.width=8, fig.height = 6}

fviz_pca_var(res.pca, col.var="contrib")+
scale_color_gradient2(low="blue", mid="white", 
                      high="red", midpoint=55)+theme_bw()

```

Let's look at the **correlations** (using Pearson's correlations):

```{r , echo=FALSE}

cor1 <- cor(quest2022$UrbanIndex[is.na(quest2022$UrbanIndex)==FALSE & is.na(quest2022$UrbanPref)==FALSE],
    quest2022$UrbanPref[is.na(quest2022$UrbanIndex)==FALSE & is.na(quest2022$UrbanPref)==FALSE], method="pearson")

cor2 <- cor(quest2022$UrbanIndex[is.na(quest2022$UrbanIndex)==FALSE & is.na(quest2022$UrbanKnow)==FALSE],
    quest2022$UrbanKnow[is.na(quest2022$UrbanIndex)==FALSE & is.na(quest2022$UrbanKnow)==FALSE], method="pearson")

cor3 <- cor(quest2022$UrbanPref[is.na(quest2022$UrbanPref)==FALSE & is.na(quest2022$UrbanKnow)==FALSE],
    quest2022$UrbanKnow[is.na(quest2022$UrbanPref)==FALSE & is.na(quest2022$UrbanKnow)==FALSE], method="pearson")

```
 
 - The correlation between *UrbanIndex* and *UrbanPref* is `r cor1`
 - The correlation between *UrbanIndex* and *UrbanKnow* is `r cor2`
 - The correlation between *UrbanKnow* and *UrbanPref* is `r cor3``

*UrbanKnow* seems to be a less interesting variable here.

##### Modernity & age

```{r , echo=FALSE, fig.cap=capFig('The relation between urban preference and age. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanPref, y=Age)) +
  geom_point() +
  geom_smooth(method="lm") +
  theme_bw(base_size=15)
```

```{r , echo=FALSE, fig.cap=capFig('The relation between urban Knowledge and age. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$UrbanKnow > 40,], aes(x=UrbanKnow, y=Age)) +
  geom_point() +
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

```

```{r , echo=FALSE, fig.cap=capFig('The relation between urban Index and age. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanIndex, y=Age)) +
  geom_point() +
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

```

Weak effect of age: young people tend to prefer modern life.


##### Modernity & gender

```{r , echo=FALSE, fig.cap=capFig('The relation between Urban Preference and Gender. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=as.numeric(UrbanPref), fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="UrbanPref")
```

```{r , echo=FALSE, fig.cap=capFig('The relation between urban Knowledge and Gender. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=as.numeric(UrbanKnow), fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
    labs(y="UrbanKnow")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between urban Knowledge and Gender. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=as.numeric(UrbanIndex), fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
    labs(y="UrbanIndex")

```

#### Intelligence

We use here three different measures of intelligence:

 - Score to a pre-selected set of 8 Raven's matrices (*ScoreMatrices*)
 - Score to Short-Term Memory empan task (*ScoreMCT*)
 - Score to numerical task (comparison, calculation) (*NumericalAbility*)
 
```{r , echo=FALSE, fig.cap=capFig('Density plot showing the results from the Raven matrice score.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=ScoreMatrices, fill=Gender)) +
  geom_density(alpha=0.6)+
  theme_bw(base_size=15)

```

```{r , echo=FALSE, fig.cap=capFig('Density plot showing the results from the Short term memory score.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=ScoreMCT, fill=Gender)) +
  geom_density(alpha=0.6)+
  theme_bw(base_size=15)

```

```{r , echo=FALSE, fig.cap=capFig('Density plot showing the results from the Numerical Ability score.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=NumericalAbility, fill=Gender)) +
  geom_density(alpha=0.6)+
  theme_bw(base_size=15)

```

Now, are these 3 variables related to each other?

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Score Matrice and Short Term memory. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=ScoreMatrices, y= ScoreMCT)) +
  geom_jitter(width=0.2)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15)

```

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Numerical Ability and Short Term memory. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=ScoreMCT, y=NumericalAbility)) +
  geom_jitter(width=0.2)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15)

```

```{r , echo=FALSE, fig.cap=capFig('Correlations between the variable Score Matrice and Numerical ability. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=NumericalAbility, y=ScoreMatrices)) +
  geom_jitter(width=0.2)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15)

```

Let's look at the **correlations** (using Pearson's correlations):


```{r , echo=FALSE}

cor1 <- cor(quest2022$ScoreMatrices[is.na(quest2022$ScoreMatrices)==FALSE & is.na(quest2022$ScoreMCT)==FALSE],
    quest2022$ScoreMCT[is.na(quest2022$ScoreMatrices)==FALSE & is.na(quest2022$ScoreMCT)==FALSE], method="pearson")

cor2 <- cor(quest2022$ScoreMatrices[is.na(quest2022$NumericalAbility)==FALSE & is.na(quest2022$ScoreMatrices)==FALSE],
    quest2022$NumericalAbility[is.na(quest2022$NumericalAbility)==FALSE & is.na(quest2022$ScoreMatrices)==FALSE], method="pearson")

cor3 <- cor(quest2022$ScoreMCT[is.na(quest2022$NumericalAbility)==FALSE & is.na(quest2022$ScoreMCT)==FALSE],
    quest2022$NumericalAbility[is.na(quest2022$NumericalAbility)==FALSE & is.na(quest2022$ScoreMCT)==FALSE], method="pearson")

```

 - The correlation between *ScoreMatrices* and *ScoreMCT* is `r cor1`
 - The correlation between *ScoreMatrices* and *NumericalAbility* is `r cor1`
 - The correlation between *ScoreMCT* and *NumericalAbility* is `r cor1`

##### Intelligence & Age

```{r , echo=FALSE, fig.cap=capFig('The relation between Score Matrice and age. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=Age, y=as.numeric(ScoreMatrices))) +
  geom_jitter(width=0.05)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15) +
  labs(y="ScoreMatrices")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Short term memory and age. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=Age, y=as.numeric(ScoreMCT))) +
  geom_jitter(width=0.05)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15) +
    labs(y="ScoreMCT")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Numerical ability and age. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=Age, y=as.numeric(NumericalAbility))) +
  geom_jitter(width=0.05)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15) +
    labs(y="NumericalAbility")



```

We can see a weak effect of age: intelligence slightly decrease with age.

##### Intelligence & Gender

```{r , echo=FALSE, fig.cap=capFig('The relation between Score Matrice and Gender. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=as.numeric(ScoreMatrices), fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="ScoreMatrices")
```

```{r , echo=FALSE, fig.cap=capFig('The relation between Score MCT and Gender. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=as.numeric(ScoreMCT), fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="ScoreMCT")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Numerical ability and Gender. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=Gender, y=as.numeric(NumericalAbility), fill=Gender)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="NumericalAbility")


```


##### Intelligence & School

```{r , echo=FALSE, fig.cap=capFig('The relation between Score Matrice and School. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$School!="",], aes(x=School, y=as.numeric(ScoreMatrices), fill=School)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="ScoreMatrice")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Short Term Memory score and School. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=School, y=as.numeric(ScoreMCT), fill=School)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="ScoreMCT")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Numerical ability and School. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$Gender!="",], aes(x=School, y=as.numeric(NumericalAbility), fill=School)) +
  geom_violin(alpha=0.4)+
  geom_jitter(alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15)+
  labs(y="NumericalAbility")


```

##### Intelligence & Literacy

```{r , echo=FALSE, fig.cap=capFig('The relation between Score Matrices and Literacy. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$LiteracyScore!="",], aes(x=LiteracyScore, y=as.numeric(ScoreMatrices))) +
  geom_smooth(method="lm")+
  geom_jitter(width=0.2, alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="ScoreMatrices")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Short Term memory score and Literacy. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$LiteracyScore!="",], aes(x=LiteracyScore, y=as.numeric(ScoreMCT))) +
  geom_smooth(method="lm")+
  geom_jitter(width=0.2, alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="ScoreMCT")

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Numerical ability and Literacy. Points represent participants, the red dot shows the mean and the red line shows the se.'), fig.width=8, fig.height = 6}

ggplot(quest2022[quest2022$LiteracyScore!="",], aes(x=LiteracyScore, y=as.numeric(NumericalAbility))) +
  geom_smooth(method="lm")+
  geom_jitter(width=0.2, alpha=0.4)+
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange",
               colour = "red") +
  theme_bw(base_size=15) +
  labs(y="NumericalAbility")

```


##### Intelligence & Modernity

```{r , echo=FALSE, fig.cap=capFig('The relation between Score Matrice and Urban Index. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

# Urban index
ggplot(quest2022, aes(x=UrbanIndex, y=as.numeric(ScoreMatrices))) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15) 

```

```{r , echo=FALSE, fig.cap=capFig('The relation between Short term memory score and Urban Index. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanIndex, y=as.numeric(ScoreMCT))) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15)
```

```{r , echo=FALSE, fig.cap=capFig('The relation between Numerical ability and Urban Index. Points represent participants: please note that each point can cover many participants. The regression line shows the linear regression going through these points and was obtained using geom_smooth method linear.'), fig.width=8, fig.height = 6}

ggplot(quest2022, aes(x=UrbanIndex, y=as.numeric(NumericalAbility))) +
  geom_jitter(width=0.1)+
  geom_smooth(method="lm")+
  theme_bw(base_size=15)

```


## Counting stones

```{r read table exp stones, echo=FALSE, message=FALSE, warning=FALSE}

stones <-  read.table(paste(pathres, "Himbas_2021/Stones/stones_all.csv", sep=""), header=T, sep=";", quote='"', fill=TRUE) 

```

See [Experiment 1: counting stones] for more information about this experiment. 

Almost every participant have counted correctly the stones: the percentage of success is `r round(mean(stones$output_correct, na.rm=TRUE)*100)`%. However, participants used different strategies to perform the task: they count stones one by one, two by two, group them by 5, 4... The graph below shows the different strategies used to count the 15 and 8 stones:


```{r compute exp 1, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig("Different strategies used by the participants to count the stones: for 8 and 15 stones. The strategy 'direct' means that participants did not touch the stones or count out loud, but directly said the right number. The category 'other groups' gathers other strategies used by a very few participant.")}

stones$nb_stones <- as.factor(stones$nb_stones)

ggplot(stones[!(stones$strategy_plot==""),], aes(x=strategy_plot, fill=nb_stones))+
  geom_histogram(stat="count", position="dodge") +
  coord_flip() +theme_bw(base_size=15) +
  labs(fill = "Number of stones", x="Strategy")
```



```{r compute exp 1 correl, echo=FALSE, message=FALSE, warning=FALSE}

stones_small <- data.frame(participant_id = stones$participant_id[stones$nb_stones==8], strategy_8 = stones$strategy_plot[stones$nb_stones==8], strategy_15 = stones$strategy_plot[stones$nb_stones==15])

stones_quest <- merge(quest2021, stones_small, by=c("participant_id"))
stones_quest <- gather(stones_quest, number_stones, strategy, strategy_8:strategy_15)

# No influence of the school
# table(stones_quest$strategy[stones_quest$school=="yes" & stones_quest$number_stones=="strategy_8"])
# table(stones_quest$strategy[stones_quest$school=="no" & stones_quest$number_stones=="strategy_8"])

# reduce the number of strategy by collapsing categories with few participants
stones_quest$strategy <- as.character(stones_quest$strategy)

stones_quest$strategy[stones_quest$number_stones=="strategy_8" & stones_quest$strategy=="groups of five"] <- "other strategy"
stones_quest$strategy[stones_quest$number_stones=="strategy_8" & stones_quest$strategy=="other groups"] <- "other strategy"
stones_quest$strategy[stones_quest$number_stones=="strategy_8" & stones_quest$strategy=="two by two"] <- "other strategy"
#summary(as.factor(stones_quest$strategy[stones_quest$number_stones=="strategy_8"]))

stones_quest$strategy[stones_quest$number_stones=="strategy_15" & stones_quest$strategy=="other groups"] <- "other strategy"
stones_quest$strategy[stones_quest$number_stones=="strategy_15" & stones_quest$strategy=="two by two"] <- "other strategy"
#summary(as.factor(stones_quest$strategy[stones_quest$number_stones=="strategy_15"]))

stones_quest$strategy <- as.factor(stones_quest$strategy)

# ggplot(data=stones_quest[!(stones_quest$strategy==""),], aes(x=strategy, y=as.numeric(age), fill=strategy))+
#   geom_boxplot() +
#   facet_grid(number_stones ~ .)+
#   theme_bw(base_size=14) +
#   labs(y="", x="Strategy") +
#   coord_flip()
# 
# table(stones_quest$strategy[stones_quest$num_score=="yes" & stones_quest$number_stones=="strategy_8"])
# table(stones_quest$strategy[stones_quest$school=="no" & stones_quest$number_stones=="strategy_8"])
# 


```

The data also suggests that *school* does **not** have an impact on the method the participants used to count the stone, nor do the *numerosity score*, the *matrice score*, the *maximum counting* or the *age.* 



## Ordering cards


### Shape


In this part, we analyze the results of the 3 following ordering cards experiments:

For **Himbas 2021**:

 1. ordering **dotted** cards in a **free** way (*part 1*);
 2. ordering cards with **arabic numbers** in a **free** way (*part 2*);
 3. ordering **dotted** cards in a **linear** way (*part 3*).
 4. ordering **dotted** cards in a **linear** way (*part 3 bis*); this part was added halfway of the experiment for participants that did not ordered the cards in the previous parts. In this additional experiment, the participant is explicitely asked to order the cards from the smaller to the bigger numerosity.
 
For **Himbas 2022**:

 1. ordering **dotted** cards in a **free** way (*part 1*);
 1 bis. ordering **coloured** cards in a **free** way (*part 1 bis*);
 2. ordering cards with **arabic numbers** in a **free** way (*part 2*);

See [Experiment 2: Ordering cards (part 1 part 2)] for more information on the exact order and content of the 4 parts. 

For the **free** ordering cards expriments, most participants have organized the cards in a horizontal line (`r round(nrow(cards_all[cards_all$type=="Dots (free)" & cards_all$shape=="linear_horizontal",]) / nrow(cards_all[cards_all$type=="Dots (free)",])*100)` % for the dotted cards and `r round(nrow(cards_all[cards_all$type=="Numbers (free)" & cards_all$shape=="linear_horizontal",]) / nrow(cards_all[cards_all$type=="Numbers (free)",])*100)` % for the cards with arabic numbers). The other shape were vertical line, diagonal, circle, or no specific shape. See the graph below for more information:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the different shapes for the free ordering experiments with dots (on top) and numbers (on the bottom).'), fig.height=8, fig.width=10}

ggplot(data=cards_all[(cards_all$type=="Dots" | cards_all$type=="Digits" | cards_all$type=="Colors") & (cards_all$shape!="") & is.na(cards_all$shape)==FALSE & is.na(cards_all$type)==FALSE,], aes(x=shape, fill=shape))+
  geom_histogram(stat="count", alpha=0.5) +
  theme_bw(base_size=16) +
  coord_flip() +
  guides(fill=FALSE) +
  facet_grid(type ~ exp) +
  labs(x="") +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  scale_fill_viridis_d()

```

### Strategy

This part was written **only for Himbas 2021** (no available data for Himbas 2022 for the strategy).

Many different strategies were used by the participants to order the cards. The cards can be either **ordered** (either left to right, right to left or following another shape) or **not ordered**.

Strategies used for participants that have **ordered** the cards:

 - *main strategy*: this strategy consists in looking for the wanted card (either by spreading on the table, either by looking one after one in the hand) in the right order (first 1, then 2, etc).
 - *place then switch position*: the participant lays all cards in the final shape and then switch their position to get the right order (left to right or right to left)
 - *other strategy*: gather a few other rare strategy used by the participants
 - *direct*: the participants place card after card directly with the pile order inside the correct position on the table


Strategies used for participants that have **not ordered** the cards:

 - *random*: the participant spread all cards on the table and choose them randomly without any specific order
 - *unknown strategy*: same as above, except that the participant seems to choose carefully the cards (suggesting a not random process). However, the experimenter did not understand the logic behind the choice.
 - *order pile*: place the cards directly with the order of the pile. This is not possible with the Numbers free experiment, as the cards were glued to stones and could not be given in piles.
 - *order pile except first numbers)*: the participant places the cards with the order of the pile but just put correctly the first numbers on one extreme of the line (only the 1, or the 1 and 2)

For other participants, the strategy was not written. The Figure below represents the distribution of participants using each strategy:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the different strategies used to order the cards for the 3 main experiments.'), fig.height=11, fig.width=9}

ggplot(data=cards_all[cards_all$exp=="Himbas2021" & (cards_all$type=="Dots" | cards_all$type=="Digits" | cards_all$type=="DotsLine") &  is.na(cards_all$type)==FALSE  ,], aes(x=strategy, fill=ordered))+
  geom_histogram(stat="count", alpha=0.7) +
  theme_bw(base_size=15) +
  coord_flip() +
  #guides(fill=FALSE) +
  facet_grid(type ~ exp) +
  labs(x="") +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  scale_fill_viridis_d(begin=0.6, end=1)


```

### Order

Is there a left/right bias for ordering cards? We are showing here 4 different types of situation:

 - **L**: a situation where participants ordered cards from **Left to Right** (in the horizontal line)
 - **R**: a situation where participants ordered cards from **Right to Left** (in the horizontal line)
 - **N**: a situation where participants did **not ordered** the cards (any type of shape)
 - **NC**: a situation where participants ordered the cards but the **shape** does not allow any conclusion on a left to right or right to left bias

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias across all experiments except colors. L means left to right, R means right to left, N means no order, and NC means that the shape was not linear horizontal.'),  fig.width=10, fig.height=7}

ggplot(data=cards_all[cards_all$type!="Colors" & !(cards_all$orderNC=="") & !(cards_all$orderNC=="no data") & is.na(cards_all$type)==FALSE,], aes(x=orderNC, fill=orderNC))+
  geom_histogram(stat="count") +
  theme_bw(base_size=15) +
  facet_grid(exp ~ type) +
  #coord_flip() +
  scale_fill_manual(values=c("dodgerblue", "azure2", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  guides(fill=FALSE) +
  labs(x="Order")


```

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias for the color experiment only. B means black to white, W means white to black, N means no order, and NC means that the shape was not linear horizontal.'),  fig.width=5, fig.height=4}

ggplot(data=cards_all[cards_all$type=="Colors" & !(cards_all$orderNC=="") & !(cards_all$orderNC=="no data") & is.na(cards_all$type)==FALSE,], aes(x=orderNC, fill=orderNC))+
  geom_histogram(stat="count") +
  theme_bw(base_size=15) +
  facet_grid(exp ~ type) +
  #coord_flip() +
  scale_fill_manual(values=c("dodgerblue", "azure2", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  guides(fill=FALSE) +
  labs(x="Order")


```

### Consistency in order

Let's observe more precisely the exact order with which the participant has answered: in the next plot, *"Dots R - Num N - DotsLin R"* means that the participant has had a **right to left** (Dots free experiment), **no specific order** (Number free experiment), a **right to left** bias (number line experiment). In order to make the plotting easier to read, we merge the categories N and NC together.

**With Himbas 2021**:
    
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias of the first experiment (with dots), the second experiment (with digits), and the third experiment (with dots linear).'), fig.height=10, fig.width=8}

# change data format
cards_all %>%
  filter(exp=="Himbas2021") %>%
  select(participant_id, type, order, exp) %>%
  tidyr::spread(type, order) %>%
  mutate(change_all = paste("Dots",Dots, " - Dig",Digits, "- DotsLin",DotsLine)) -> cards_spread

# remove participant that did not complete all tasks
cards_spread <- cards_spread[!(cards_spread$participant_id %in% c(1001, 1002, 1003, 1004, 1030)),]

# add a new column mentionning if the strategy is consistent or not
cards_spread$consistent <- "no"
cards_spread$consistent[cards_spread$change_all=="Dots R  - Dig R - DotsLin R" | cards_spread$change_all=="Dots L  - Dig L - DotsLin L" | cards_spread$change_all=="Dots N  - Dig N - DotsLin N" | cards_spread$change_all=="Dots R  - Dig N - DotsLin R" | cards_spread$change_all=="Dots L  - Dig N - DotsLin L" ] <- "yes"

# plot
ggplot(cards_spread[is.na(cards_spread$Digits)==FALSE & !(cards_spread$Digits==""),], aes(x=change_all, fill=Dots, alpha=consistent))+
  geom_histogram(stat="count")+
  theme_bw(base_size=16) +
  coord_flip() +
  guides(fill=FALSE) +
  scale_fill_manual(values=c("dodgerblue",   "azure3", "darkgoldenrod1")) +
  scale_alpha_manual(values=c(0.3, 1)) +
  labs(x="") +
  facet_grid(. ~ exp) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

# add a new column in the cards variable to say if the participant was consistent or not
consistency2021 = data.frame(participant_id = cards_spread$participant_id, consistency = cards_spread$consistent)

```

Let's observe more precisely the exact order with which the participant has answered: in the next plot, *"Dots R - Dig N"* means that the participant has had a **right to left** (Dots experiment), **no specific order** (Digits experiment), a **right to left** bias (number line experiment). In order to make the plotting easier to read, we merge the categories N and NC together.

**With Himbas 2022**:

Only for dots and digits:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias of the first experiment (with dots) and the second experiment (with digits).'), fig.height=7, fig.width=7}

# change data format
cards_all %>%
  filter(exp=="Himbas2022") %>%
  filter(type!="Colors") %>%
  select(participant_id, type, order, exp) %>%
  tidyr::spread(type, order) %>%
  mutate(change_all = paste("Dots",Dots, " - Dig",Digits)) -> cards_spread

# remove participant that did not complete all tasks
cards_spread <- cards_spread[!(cards_spread$participant_id %in% c(1001, 1002, 1003, 1004, 1030)),]

# add a new column mentionning if the strategy is consistent or not
cards_spread$consistent <- "no"
cards_spread$consistent[cards_spread$change_all=="Dots R  - Dig R" | cards_spread$change_all=="Dots L  - Dig L" | cards_spread$change_all=="Dots N  - Dig N" | cards_spread$change_all=="Dots R  - Dig N" | cards_spread$change_all=="Dots L  - Dig N" ] <- "yes"

# plot
ggplot(cards_spread[is.na(cards_spread$Digits)==FALSE & !(cards_spread$Digits=="") & !(cards_spread$Dots==""),], aes(x=change_all, fill=Dots, alpha=consistent))+
  geom_histogram(stat="count")+
  theme_bw(base_size=16) +
  coord_flip() +
  guides(fill=FALSE) +
  scale_fill_manual(values=c("dodgerblue",   "azure3", "darkgoldenrod1")) +
  scale_alpha_manual(values=c(0.3, 1)) +
  labs(x="") +
  facet_grid(. ~ exp) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

# add a new column in the cards variable to say if the participant was consistent or not
consistency2022 = data.frame(participant_id = cards_spread$participant_id, consistency = cards_spread$consistent)
consistency <- rbind(consistency2021, consistency2022)

cards_all <- merge(cards_all, consistency, all.x=TRUE, by="participant_id")
quest_all <- merge(quest_all, consistency, all.x=TRUE, by="participant_id")

```


With dots, digits **and** colors:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias of the first experiment (with dots), the second experiment (with digits), and the third experiment (with dots linear).'), fig.height=10, fig.width=8}

# change data format
cards_all %>%
  filter(exp=="Himbas2022") %>%
  select(participant_id, type, order, exp) %>%
  tidyr::spread(type, order) %>%
  mutate(change_all = paste("Dots",Dots, " - Dig",Digits, "- Colors",Colors)) -> cards_spread

# remove participant that did not complete all tasks
cards_spread <- cards_spread[!(cards_spread$participant_id %in% c(1001, 1002, 1003, 1004, 1030)),]

# add a new column mentionning if the strategy is consistent or not
cards_spread$consistent <- "no"
cards_spread$consistent[cards_spread$change_all=="Dots R  - Dig R - Colors W" | cards_spread$change_all=="Dots L  - Dig L - Colors B" | cards_spread$change_all=="Dots N  - Dig N - Colors N" | cards_spread$change_all=="Dots R  - Dig N - Colors W" | cards_spread$change_all=="Dots L  - Dig N - Colors B" ] <- "yes"

# plot
ggplot(cards_spread[is.na(cards_spread$Digits)==FALSE & is.na(cards_spread$Colors)==FALSE &  !(cards_spread$Digits=="") &  !(cards_spread$Colors==""),], aes(x=change_all, fill=Dots, alpha=consistent))+
  geom_histogram(stat="count")+
  theme_bw(base_size=16) +
  coord_flip() +
  guides(fill=FALSE) +
  scale_fill_manual(values=c("dodgerblue" , "azure3", "darkgoldenrod1")) +
  scale_alpha_manual(values=c(0.3, 1)) +
  labs(x="") +
  facet_grid(. ~ exp) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

# add a new column in the cards variable to say if the participant was consistent or not
consistency2021 = data.frame(participant_id = cards_spread$participant_id, consistency = cards_spread$consistent)

```


### Factors 

Are there some **factors that can explain the variation?**

#### School & Literacy 

In the following analysis, we exclude participants that did an other shape than linear horizontal. Thus, we study a total of `r length(unique(quest_all$participant_id[!(quest_all$Dots=="NC")]))` participants.

**School**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "School", "Dots", "School")
frequency_plot(quest_all, "Himbas2022", "School", "Dots", "School")

```

Investigate the effect of school on consistency? 

**Literacy**:

Here, the participants are split into **two groups**: participants that do not know how to read write (*literacy = none*) and participants having some knowledge about how to read and write (*literacy = some*)

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "literacy_group", "Dots", "Literacy level")
frequency_plot(quest_all, "Himbas2022", "literacy_group", "Dots", "Literacy level")

```

#### Age & Gender


**Age**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between numerical abilities and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "age_cut", "Dots", "Age")
frequency_plot(quest_all, "Himbas2022", "age_cut", "Dots", "Age")

```

**Gender**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between numerical abilities and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "Gender", "Dots", "Sex")
frequency_plot(quest_all, "Himbas2022", "Gender", "Dots", "Sex")

```

#### Modernity


**UrbanIndex**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between urban index and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2022", "uind", "Dots", "Urban Index")

```

**UrbanPreference**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between urban preference and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2022", "upref", "Dots", "Urban Preference")

```

**UrbanKnowledge**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between urban knowledge and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2022", "uknow", "Dots", "Urban Knowledge")

```

**Arabic numbers effect**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "knownum", "Dots", "Know Arabic Number")
frequency_plot(quest_all, "Himbas2022", "knownum", "Dots", "Know Arabic Number")

```


**Telephone effect**:

Does the use of a telephone has an impact on this left to right bias?

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "telephone", "Dots", "Telephone")
frequency_plot(quest_all, "Himbas2022", "telephone", "Dots", "Telephone")

```

#### Intelligence

**Numerical abilities effect**:

There are 3 participants for which no data on numerical abilities nor raven matrice are available.

Global numerical abilities:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between numerical abilities and bias order.'), fig.width=wid_freq, fig.height=hei_freq}


frequency_plot(quest_all, "Himbas2021", "numab", "Dots", "Numerical abilities (overall)")
frequency_plot(quest_all, "Himbas2022", "numab", "Dots", "Numerical abilities (overall)")

```

**Raven matrice**:

Careful !!! The two measures are slightly different (see [Method])

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between raven matrice and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2021", "mat", "Dots", "Raven's matrice")
frequency_plot(quest_all, "Himbas2022", "mat", "Dots", "Raven's matrice")

```

**Short Term memory**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between short term memory and bias order.'), fig.width=wid_freq, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2022", "mct", "Dots", "Short term memory")

```

#### Village effect

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between raven matrice and bias order.'), fig.width=13, fig.height=hei_freq}

frequency_plot(quest_all, "Himbas2022", "Village", "Dots", "Village")
frequency_plot(quest_all, "Himbas2022", "Village", "Dots", "Village")

```

**Himbas 2022**:

However, this difference between village might be related to the ratio of participants being able to go to school in each village: this ratio is of `r round(nrow(quest_all[quest_all$School=="yes" & quest_all$Village=="Otjeme",])/nrow( quest_all[quest_all$Village=="Otjeme",])*100)`% in Otjeme, `r round(nrow(quest_all[quest_all$School=="yes" & quest_all$Village=="Otjirumbu",])/nrow( quest_all[quest_all$Village=="Otjirumbu",])*100)`% in Otjirumbu, `r round(nrow(quest_all[quest_all$School=="yes" & quest_all$Village=="Otuazuma",])/nrow( quest_all[quest_all$Village=="Otuazuma",])*100)`% in Otuazuma, and `r round(nrow(quest_all[quest_all$School=="yes" & quest_all$Village=="Okatura",])/nrow( quest_all[quest_all$Village=="Okatura",])*100)`% in Okatura.

**Himbas 2021**:
Here too, this difference between village might be related to the ratio of participants being able to go to school in each village: this ratio is of `r round(nrow(quest_all[quest_all$School=="yes" & quest_all$Village=="Epembe",])/nrow( quest_all[quest_all$Village=="Epembe",])*100)`% in Epembe, `r round(nrow(quest_all[quest_all$School=="yes" & quest_all$Village=="Orotjitombo",])/nrow( quest_all[quest_all$Village=="Orotjitombo",])*100)`% in Orotjitombo.

Please note that the two participants tested in Opuwo were two of the guides accompanying us. They were tested before being presented to any of our research goal.

### Order (excluding some participants)

#### Only participant with NO school

Thus, we decide to **remove** the participants that have been to school and the participants that own a telephone from the analysis. With these two criteria of exclusion, the total number of participants is N=`r nrow(quest_all_gat[!(quest_all_gat$order=="") & quest_all_gat$School=="no"  & quest_all_gat$type=="Dots",])`. Let's look at the distribution of the bias with these exclusion criteria:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias order with the participants who have never been to school.'), fig.width=10, fig.height=7}

ggplot(data=quest_all_gat[ (quest_all_gat$type=="Dots" | quest_all_gat$type=="Digits") & !(quest_all_gat$order=="no data") & !(quest_all_gat$order=="") & is.na(quest_all_gat$order)==FALSE  & quest_all_gat$School=="no" &  is.na(quest_all_gat$School)==FALSE,], aes(x=order, fill=order))+
  geom_histogram(stat="count") +
  theme_bw(base_size=15) +
  facet_grid(type ~ exp) +
  #coord_flip() +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)


```

#### Only participant with NO school and NO telephone

Thus, we decide to **remove** the participants that have been to school and the participants that own a telephone from the analysis. With these two criteria of exclusion, the total number of participants is N=`r nrow(quest_all_gat[!(quest_all_gat$order=="") & quest_all_gat$School=="no" & quest_all_gat$telephone=="no" & quest_all_gat$type=="Dots",])`. Let's look at the distribution of the bias with these exclusion criteria:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias order with the participants who have never been to school and do not have a telephone.'), fig.width=10, fig.height=7}

ggplot(data=quest_all_gat[ (quest_all_gat$type=="Dots" | quest_all_gat$type=="Digits") & !(quest_all_gat$order=="no data") & !(quest_all_gat$order=="") & is.na(quest_all_gat$order)==FALSE  & quest_all_gat$School=="no" & quest_all_gat$telephone=="no" & is.na(quest_all_gat$School)==FALSE,], aes(x=order, fill=order))+
  geom_histogram(stat="count") +
  theme_bw(base_size=15) +
  facet_grid(type ~ exp) +
  #coord_flip() +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)


```


#### Only participant who do NOT know digits

More generally, all participants that know the arabic numbers are potentially biased in this experiment, as arabic numbers are organized from left to right. The Figure below show the L or R bias for participants that do not know arabic numbers:

```{r exp 2 additional analysis, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias order with participants that do not know arabic numbers.'), fig.width=7, fig.height=7}

ggplot(data=quest_all_gat[(quest_all_gat$type=="Dots" ) & !(quest_all_gat$order=="no data") & !(quest_all_gat$order=="") & is.na(quest_all_gat$order)==FALSE  & quest_all_gat$knownum=="no" & is.na(quest_all_gat$knownum)==FALSE,], aes(x=order, fill=order))+
  geom_histogram(stat="count") +
  theme_bw(base_size=15) +
  facet_grid(exp ~ type) +
  #coord_flip() +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

```

It is hard to draw conclusions as the number of participants who do not know arabic numbers is to small. However, it seems that there are no left to right bias.  

In total, there are:

 - `r nrow(quest_all[quest_all$Digits=="N",])` participants who does not know arabic numbers;
 - `r nrow(quest_all[quest_all$School=="no",])` participants who has never been to school;
 - `r nrow(quest_all[quest_all$telephone=="no",])` participants who do not have a telephone;
 - `r nrow(quest_all[quest_all$telephone=="no" & quest_all$Digits=="N" & quest_all$School=="no",])` participants who do not know arabic numbers, have never been to school and do not have a telephone

<!-- ```{r , echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- list_part_all <- unique(quest_all$participant_id[cards_quest$School=="no" & cards_quest$telephone=="no" & cards_quest$type=="Numbers (free)" & cards_quest$order=="N"]) -->

<!-- list_part_no_num <- unique(cards_quest$participant_id[cards_quest$type=="Numbers (free)" & cards_quest$order=="N"]) -->

<!-- list_part_noschool <- unique(cards_quest$participant_id[cards_quest$School=="no"]) -->

<!-- list_part_nonumnoschool <- unique(cards_quest$participant_id[cards_quest$type=="Numbers (free)" & cards_quest$order=="N" & cards_quest$School=="no"]) -->


<!-- list_part_notel <- unique(cards_quest$participant_id[cards_quest$telephone=="no"]) -->

<!-- list_part_noschoolnotel <- unique(cards_quest$participant_id[cards_quest$School=="no" & cards_quest$telephone=="no"]) -->

<!-- # short version of the data to merge with other experiment -->
<!-- small_cards <- spread(cards[,c(1, 2, 5)], type, order) -->
<!-- names(small_cards)[names(small_cards) == "ID"] <- "participant_id" -->


<!-- ``` -->

<!-- #### Regression -->

<!-- We perform a classic logistic regression using the following variables as covariates: -->

<!--  - school -->
<!--  - knowledge of arabic numbers -->
<!--  - telephone -->
<!--  - literacy_group -->
<!--  - calcul_score_percent (Z-squared) -->
<!--  - comp_score_percent (Z-squared) -->
<!--  - counting_score_percent (Z-squared) -->
<!--  - mat_score_percent (Z-squared) -->
<!--  - age (Z-squared) -->

<!-- ##### Predicting whether participants ordered the cards with L or R order -->

<!-- ```{r , echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- # Scale variables for better comparison -->
<!-- quest_all_gat$NumericalAbility_z <- scale(quest_all_gat$NumericalAbility) -->
<!-- quest_all_gat$ScoreMat_per_z <- scale(quest_all_gat$ScoreMatrices_percent) -->
<!-- quest_all_gat$agez <- scale(quest_all_gat$Age) -->

<!-- ``` -->

<!-- Among the participants that ordered the cards only (no participant that ordered with N), we study the impact of all our variables. -->

<!-- ```{r regression 2, echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- ### LOGISTIC REGRESSION - for left right bias -->
<!-- quest_all_gat_LR <- quest_all_gat[quest_all_gat$order == "L" | quest_all_gat$order=="R",] -->
<!-- quest_all_gat_LR$order <- as.numeric(as.factor(quest_all_gat_LR$order)) -->
<!-- quest_all_gat_LR$order <- quest_all_gat_LR$order - 1 -->

<!-- # remove participants with missing data on calculation -->
<!-- quest_all_gat_LR_nodat <- quest_all_gat_LR[, c("participant_id", "ScoreMat_per_z", "NumericalAbility_z", "OpuwoThisYear", "exp", "Gender", "telephone", "lateralisation", "agez", "knownum", "literacy_group")] -->
<!-- quest_all_gat_LR_nodat <- quest_all_gat_LR_nodat[complete.cases(quest_all_gat_LR_nodat), ] -->

<!-- # All factors -->
<!-- full_mod_glm <- glm(order ~ NumericalAbility_z + ScoreMat_per_z + OpuwoThisYear + exp + Gender + telephone  + lateralisation  + agez +  knownum + literacy_group  , data=quest_all_gat_LR_nodat, family = binomial(link="logit")) -->
<!-- summary(full_mod_glm) -->
<!-- vif(full_mod_glm) -->
<!-- #step(full_mod_glm) -->
<!-- ``` -->

<!-- Among all these variables, we performed a **stepwise regression** to check which variables are the most important. We obtain the following variables (ranked by decreasing order of importance:  -->

<!--  - School (most important) -->
<!--  - Matrice score -->
<!--  - Age -->
<!--  - Knowing arabic number -->

<!-- But their importance is very low (decrease of AIC < 5). -->

<!-- We compute an confusion matrix to check the model performance. -->

<!-- ```{r regression 3, echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- # Stepwise regression -->
<!-- #backwards = step(full_mod_glm) -->
<!-- # best factors -->

<!-- best_mod_glm <- glm(order ~ knownum + School + mat_score_percentz + agez, data=cards_ok_part_LR2, family = binomial(link="logit")) -->
<!-- summary(best_mod_glm) -->

<!-- # Check the performance of this model -->
<!-- smp_size <- floor(0.80 * nrow(cards_ok_part_LR2)) # create your sample size -->
<!-- set.seed(50) -->
<!-- train_ind <- sample(seq_len(nrow(cards_ok_part_LR2)), size = smp_size) -->
<!-- train <- cards_ok_part_LR2[train_ind, ] -->
<!-- test <- cards_ok_part_LR2[-train_ind, ] -->
<!-- pdata <- predict(best_mod_glm, newdata = train, type = "response") -->

<!-- confusionMatrix(data = as.factor(as.numeric(pdata>=0.5)), reference = as.factor(train$order)) -->

<!-- ``` -->

<!-- We find that only 4 parameters are likely to impact the way participants order cards: **older** participants with a **low matrice score** that **knowing arabic numbers** and **attended school** are more likely to order from left to right.  -->

<!-- We find the same best predictors using a mixed-effect model with the village and the day as random factors: -->

<!-- ```{r regression 4, echo=FALSE, message=FALSE, warning=FALSE} -->


<!-- ## MIXED EFFECT MODEL -->
<!-- best_mod_glm <- glmer(as.factor(order) ~ knownum + school + mat_score_percent + agez  + (1|village_name) + (1|day), data=cards_ok_part_LR, family = binomial(link="logit")) -->
<!-- summary(best_mod_glm) -->
<!-- ``` -->



<!-- ##### Predicting whether participant order (L or R) or not (N) the cards -->


<!-- ```{r regression 5, echo=FALSE, message=FALSE, warning=FALSE} -->


<!-- ### LOGISTIC REGRESSION - for ordering -->
<!-- cards_ok_part_OR <- cards_ok_part -->
<!-- cards_ok_part_OR$ordered <- 1 -->
<!-- cards_ok_part_OR$ordered[cards_ok_part_OR$order=="N"] <- 0 -->
<!-- cards_ok_part_OR <- na.omit(cards_ok_part_OR) -->

<!-- full_mod_glm <- glm(as.factor(ordered) ~ calcul_score_percentz + comp_score_percentz + school + telephone   +  counting_scorez + agez +  knownum + literacy_group  + mat_score_percentz, data=cards_ok_part_OR, family = binomial(link="logit")) -->
<!-- summary(full_mod_glm) -->

<!-- ``` -->

<!-- Then, we perform a stepwise regression to find the most important predictors. The best variables are: -->

<!--  - literacy -->
<!--  - know numbers -->
<!--  - counting score -->
<!--  - age -->

<!-- We also check the confusion matrix of this model to see how well these predictors fits the data. -->

<!-- ```{r regression exp6, echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- # Stepwise regression -->
<!-- #backwards = step(full_mod_glm) -->

<!-- # best factors -->
<!-- best_mod_glm <- glm(ordered ~ literacy_group + knownum + counting_scorez + agez , data=cards_ok_part_OR, family = binomial(link="logit")) -->
<!-- summary(best_mod_glm) -->

<!-- # Check the performance of this model -->
<!-- smp_size <- floor(0.80 * nrow(cards_ok_part_OR)) # create your sample size -->
<!-- set.seed(50) -->
<!-- train_ind <- sample(seq_len(nrow(cards_ok_part_OR)), size = smp_size) -->
<!-- train <- cards_ok_part_OR[train_ind, ] -->
<!-- test <- cards_ok_part_OR[-train_ind, ] -->
<!-- pdata <- predict(best_mod_glm, newdata = train, type = "response") -->

<!-- confusionMatrix(data = as.factor(as.numeric(pdata>=0.5)), reference = as.factor(train$ordered)) -->

<!-- ``` -->

<!-- We find that **knowing arabic numbers**, **knowing how to read, write and count** and the **age** are the best predictors to know whether participants order the cards (either L or R) or do not order them (N). -->

<!-- ##### Predicting all (L, R or N) -->

<!-- We use a multinomial regression (categorical variables with more than 2 factors) to predict the order of the cards (L, R or N). -->

<!-- With all factors: -->

<!-- ```{r regression 7, echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- ### MULTINOMIAL REGRESSION -->
<!-- # All factors -->
<!-- multi1 = multinom(order ~  calcul_score_percent + comp_score_percent + school + telephone  + lateralisation +  counting_score + age +  knownum + literacy_group  + mat_score_percent, data=cards_ok_part) -->
<!-- summary(multi1) -->
<!-- ``` -->

<!-- With the best factors: -->

<!-- ```{r regression 8, echo=FALSE, message=FALSE, warning=FALSE} -->

<!-- # Best factors -->
<!-- multi2 = multinom(order ~  agez +  knownum + literacy_group + counting_scorez + mat_score_percentz , data=cards_ok_part) -->
<!-- summary(multi2) -->

<!-- ``` -->



## Baskets

#### Order

##### With all testings

Firstly, we can observe that the participants did not do a lot of mistakes, but completed correctly the task:

```{r compute exp 5, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for all participants and all testings.'), fig.height=7, fig.width=10}

# plot all
ggplot(data=basket_testing, aes(x=basket, fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(exp ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

```


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for Himbas 2021 participants and all testings.'), fig.height=7, fig.width=10}

# plot all
ggplot(data=basket_testing[basket_testing$exp=="Himbas2021",], aes(x=basket, fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(order_par ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

```

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for Preschooler participants and all testings.'), fig.height=7, fig.width=10}

# plot all
ggplot(data=basket_testing[basket_testing$exp=="Preschooler",], aes(x=basket, fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(order_par ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

```

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for Italian adults participants and all testings.'), fig.height=7, fig.width=10}

# plot all
ggplot(data=basket_testing[basket_testing$exp=="Adults",], aes(x=basket, fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(order_par ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)

```


##### With the first testing only

The previous plot showed **all testings**.

The plot below shows the distribution of answers for the **first testing** only.

```{r compute exp 5 bis, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for all participants and all testings.'), fig.height=7, fig.width=10}

# plot all
ggplot(data=basket_first_test, aes(x=as.factor(basket), fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(exp ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 

```


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for Himbas 2021 participants and all testings.'), fig.height=7, fig.width=10}

ggplot(data=basket_first_test[basket_first_test$exp=="Himbas2021",], aes(x=as.factor(basket), fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(order_par ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 


```

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for Italian adults participants and all testings.'), fig.height=7, fig.width=10}

ggplot(data=basket_first_test[basket_first_test$exp=="Adults",], aes(x=as.factor(basket), fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(order_par ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 


```

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for children participants and all testings.'), fig.height=7, fig.width=10}

ggplot(data=basket_first_test[basket_first_test$exp=="Preschooler",], aes(x=as.factor(basket), fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(order_par ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 


```

##### Laterality index

**NOTE !!! THIS PART WAS NOT COMPUTED WITH CHILDREN AND ADULTS !!!**

The previous plot showed **all testings** or the **first testing**. We aggregated all testings by participant, thus creating an index of laterality for each participant. This index of laterality is defined as:

$$
\frac{R}{R + L}
$$

where R are the testing where the participant showed a right to left bias and L are the testings where the participant showed a left to right bias.

```{r compute exp 5 bis all testings by par, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for all participants and all testings.'), fig.height=7, fig.width=10}

# add a laterality index measure
# compute laterality index
quest2021$L <- quest2021$count3when3 + quest2021$count8when8
quest2021$R <- quest2021$count3when8 + quest2021$count8when3
quest2021$latindex_baskets <- quest2021$R / (quest2021$R + quest2021$L)

ggplot(quest2021, aes(x=latindex_baskets))  + 
  geom_histogram(bins = 50) +
  theme_bw(base_size=15) +
  labs(x="L <-- Laterality index --> R")


```

```{r compute exp 5 order bis all testings by par, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant when the ball is on the 3rd and 8th position, for all participants and all testings.'), fig.height=7, fig.width=10}


new_basket <- merge(quest2021, basket_per_part)

ggplot(new_basket, aes(x=latindex_baskets))  + 
  geom_histogram(bins = 50) +
  facet_grid(order_par ~ .) +
  theme_bw(base_size=15) +
  labs(x="L <-- Laterality index --> R")


```


#### Consistency in order

We look at the exact process: did participant changes the side, did they keep showing the same basket? 

We report 3 distinct strategies:

 - *"3"*: the participant has showed the position **"3"** 7 or 8 times out of the 8 total testing
 - *"8"*: the participant has showed the position **"8"** 7 or 8 times out of the 8 total testing 
  - *"mixed"*: the participant has showed the position **"3" and "8"** during the same testing (less than 7 times for each location)
  
```{r compute exp 5 add 11, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the position choose by the participant in the order they saw the ball (3 then 8 for half of the participant, 8 then 3 for the rest.'),  fig.height=9, fig.width=8}

# complete pre existing dataframe
basket_per_part$pos_par_3 <- "mixed"
basket_per_part$pos_par_8 <- "mixed"

# create the new variable gathering the counterbalanced order of presentation
for (id_par in unique(basket_testing$participant)){
  # Compute frequency of basket occurence for position dot = 3
  basket_testing %>%
    filter(participant_id == id_par & position_dot == "Ball on 3rd pos") -> sub_df
  pos_par_3 = "mixed"
  if (length(which(sub_df$basket==3))>=7) { pos_par_3 = "3" }
  if (length(which(sub_df$basket==8))>=7) { pos_par_3 = "8" }
  
  # do the same process for ball at position 8
  basket_testing %>%
    filter(participant_id == id_par & position_dot == "Ball on 8th pos") -> sub_df
  pos_par_8 = "mixed"
  if (length(which(sub_df$basket==3))>=7) { pos_par_8 = "3" }
  if (length(which(sub_df$basket==8))>=7) { pos_par_8 = "8" }

  basket_per_part$pos_par_3[basket_per_part$participant_id == id_par] <- pos_par_3
  basket_per_part$pos_par_8[basket_per_part$participant_id == id_par] <- pos_par_8
}

# reshape data and create bias_consistency column
basket_per_part %>%
  mutate(order_done_part = paste(pos_par_3, pos_par_8, sep="-"),
        bias_consistency = ifelse(((order_done_part == "3-8" & order_par=="Order: 3 then 8") | (order_done_part ==  "8-3" & order_par=="Order: 8 then 3")), "L", ifelse(((order_done_part == "3-8" & order_par=="Order: 8 then 3") | (order_done_part == "8-3" & order_par=="Order: 3 then 8")), "R", "N")))  -> basket_per_part

ggplot(data=basket_per_part, aes(x=order_done_part, fill=bias_consistency)) +
  geom_bar(stat="count") +
  facet_grid(order_par ~ .) +
  theme_bw(base_size=15) +
  coord_flip() +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  labs(x="")



```

Same type of plot, but presented differently:

```{r compute exp 5 add 1, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Same Figure as above but presented differently.'), fig.height=7, fig.width=7}

# gather position 3 and 8
basket_per_part2 <- gather(basket_per_part, condition, measurement, pos_par_3:pos_par_8)

# reshape dataframe
basket_per_part2 %>%
  mutate(type_bias = ifelse(((condition == "pos_par_3" & measurement=="3") | (condition ==  "pos_par_8" & measurement=="8")), "L", ifelse(((condition == "pos_par_3" & measurement=="8") | (condition == "pos_par_8" & measurement=="3")), "R", "N")),
         condition = case_when(condition=="pos_par_3" ~ "Ball at position 3",
                               condition=="pos_par_8" ~ "Ball at position 8")) -> basket_per_part2

# plot
ggplot(data=basket_per_part2, aes(x=measurement, fill=type_bias)) +
  geom_bar(stat="count") +
  facet_grid(order_par ~ condition) +
  theme_bw(base_size=15) +
  theme(
    axis.title.x = element_text(),
    axis.title.y = element_text()) +
    scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  labs(x="")

```

#### Factors

##### School & Literacy

**School**: 

-> On cards order

```{r plot exp 5 - school effect, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.')}

# change a few name to match with "ordering cards"
baskets_quest %>%
  mutate(order = case_when(order_par == "Order: 3 then 8" ~ "L",
                           order_par == "Order: 8 then 3" ~ "R")) -> baskets_quest

# plot frequency plot
frequency_plot(baskets_quest, "Himbas2021", "School", "", "School")


```

School does not have a strong effect on our result. 

-> On consistency

Here, **consistency** means more precisely if the participant have always chosen the same basket within the same trial.
For example, if the particant has chosen 7 times or more the basket "3" when the ball was presented under the basket 8, we say that the participant was consistent, no matter of the bias direction.

TO-DO


<!-- ```{r plot exp 5 - school effect consis, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.')} -->

<!-- # add a consistency column -->
<!-- basket_per_part %>% -->
<!--   mutate(consistency = case_when((bias_consistency == "R" | bias_consistency == "L") ~ "yes", -->
<!--                                  bias_consistency == "N" ~ "no")) -> basket_per_part -->

<!-- # merge the dataframe per participant with the dataframe with all info -->
<!-- basket_all <- merge(basket_per_part, baskets_quest, by=c("participant_id", "exp"), all.x = TRUE, all.y= TRUE) -->

<!-- # create frequency table -->
<!-- df_schoolcons <- data.frame(school=c("yes", "yes", "no", "no"), consistency=c("yes", "no", "yes", "no"), relat_freq=c(0, 0, 0, 0)) -->

<!-- # find frequencies -->
<!-- for (row in c(1:nrow(df_schoolcons))){ -->
<!--   df_schoolcons[row,3] <- round(nrow(basket_all[basket_all$School==df_schoolcons[row,1] & basket_all$consistency==df_schoolcons[row,2],]) / nrow(basket_all[ basket_all$School==df_schoolcons[row,1],])*100) -->
<!-- } -->

<!-- # find number of participants -->
<!-- nb_part_yes <- length(unique(basket_all$participant_id[basket_all$consistency=="yes"])) -->
<!-- nb_part_no <- length(unique(basket_all$participant_id[basket_all$consistency=="no"])) -->
<!-- dat_text <- data.frame(label = c(paste("N =",as.character(nb_part_yes)), paste("N =", as.character(nb_part_no))), consistency = c("yes", "no")) -->

<!-- # reshape data -->
<!-- df_schoolcons %>% -->
<!--   mutate(school = case_when(school=="yes" ~ "School: yes", -->
<!--                             school=="no" ~ "School: no")) -> df_schoolcons -->
<!-- # plot -->
<!-- ggplot(data=df_schoolcons, aes(x=school, y=relat_freq))+ -->
<!--   geom_histogram(stat="identity", position="dodge", size=1, aes(fill=school)) + -->
<!--   theme_bw(base_size=15) + -->
<!--   facet_grid(exp ~ consistency) + -->
<!--   #scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1"))+ -->
<!--   guides(fill=FALSE) + -->
<!--   geom_text(aes(label=paste(relat_freq, "%")), vjust=-0.3, size=4.8)   + -->
<!--   ylim(0,90) + -->
<!--   theme(plot.title = element_text(hjust = 0.5, size=15)) + -->
<!--   labs(y="Relative frequency (in %)", x="",title="Consistency") + -->
<!--   geom_text(data = dat_text, mapping = aes(x = 2, y = 80, label = label), hjust = -0.1, vjust = -1) -->



<!-- ``` -->


**Literacy**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.')}

frequency_plot(baskets_quest, "Himbas2021", "literacy_group", "", "Literacy_group")


```

##### Age & Gender


**Age**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between numerical abilities and bias order.')}

frequency_plot(baskets_quest, "Himbas2021", "age_cut", "", "Age")

```

**Gender**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between numerical abilities and bias order.')}

frequency_plot(baskets_quest, "Himbas2021", "Gender", "", "Gender")

```

##### Cards

```{r compute exp 5 part 2bis, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the card ordering in experiment 2 on bias order.'),  fig.width=10, fig.height=6}

frequency_plot(baskets_quest, "Himbas2021", "bias_cards_free", "", "Bias ordering cards")

```  


Controlling for school effect:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the card ordering in experiment 2 on bias order.'),  fig.width=10, fig.height=6}
 
frequency_plot(baskets_quest[baskets_quest$School=="yes",], "Himbas2021", "bias_cards_free", "", "Bias ordering cards (Schooled)")
frequency_plot(baskets_quest[baskets_quest$School=="no",],"Himbas2021", "bias_cards_free", "", "Bias ordering cards (Unschooled)")

```


##### Modernity

**Telephone**:

Owning a telephone has absolutely no effect on the bias order of the participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the card ordering in experiment 2 on bias order.'),  fig.width=10, fig.height=6}
 
frequency_plot(baskets_quest, "Himbas2021", "telephone", "", "Possess a telephone")

```


**Knowing arabic numbers**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r l, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.')}

frequency_plot(baskets_quest, "Himbas2021", "knownum", "", "Know arabic number")

```

##### Intelligence

**Numerical abilities effect**:

There are 3 participants for which no data on numerical abilities nor raven matrice are available.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between numerical abilities and bias order.'), fig.width=6, fig.height=12}

frequency_plot(baskets_quest, "Himbas2021", "numab", "", "Numerical ability")
#frequency_plot(baskets_quest, "Himbas2021", "numcomp", "", "Comparison score")
#frequency_plot(baskets_quest, "Himbas2021", "numcal", "", "Calculation score")
#frequency_plot(baskets_quest, "Himbas2021", "numcount", "", "Counting score")


```


**Matrice effect**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the correlation between raven matrice and bias order.')}

frequency_plot(baskets_quest, "Himbas2021", "mat", "", "Matrice score")

```


#### Order (excluding some participants)

##### Excluding participants with NO school

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias with participants that have not been to school.'),  fig.width=10, fig.height=6}

# plot all
ggplot(data=baskets_quest[baskets_quest$exp=="Himbas2021" & baskets_quest$School=="no",]
, aes(x=basket, fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(. ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)


```

There are `r nrow(baskets_quest[baskets_quest$School=="no",])` participants that do not know arabic numbers.

##### Excluding participants with NO school and NO knowledge of digits

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the bias with participants that have not been to school and that do not know arabic numbers.'),  fig.width=10, fig.height=6}

# plot all
ggplot(data=baskets_quest[baskets_quest$exp=="Himbas2021" & baskets_quest$School=="no" & baskets_quest$knownum=="no",]
, aes(x=basket, fill=bias))+
  geom_histogram(stat="count") +
  facet_grid(. ~ position_dot) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("dodgerblue", "azure3", "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4)


```

There are `r length(unique(baskets_quest$participant_id[baskets_quest$School=="no" & baskets_quest$knownum=="no"]))` participants that do not know arabic numbers.


#### Regression

##### By participant: predicting laterality index

```{r , echo=FALSE, message=FALSE, warning=FALSE}

### BY PARTICIPANT
quest_nodat <- quest2021[is.na(quest2021$latindex_baskets) == FALSE,]

full_mod_glm <- lm(latindex_baskets ~ calcul_score_percent + comp_score_percent + School + telephone  + lateralisation +  counting_score + Age +  knownum + literacy_group  + mat_score_percent, data=quest_nodat)
summary(full_mod_glm)


```

The R-squared is very low, the variables do not predict well the laterality index.

##### By participant: using first testing only

###### First testing when ball is at position 3

```{r , echo=FALSE, message=FALSE, warning=FALSE}

basket_first_stat <- merge(basket_first_test, quest2021, by="participant_id")
basket_first_stat <- basket_first_stat[basket_first_stat$bias == "R" | basket_first_stat$bias=="L",]
basket_first_stat$bias_num <- as.numeric(as.factor(basket_first_stat$bias)) -1

full_mod_glm <- glm(as.factor(bias_num) ~ calcul_score_percent + comp_score_percent + School + telephone  + lateralisation +  counting_score + Age +  knownum + literacy_group  + mat_score_percent, data=basket_first_stat[basket_first_stat$position_dot=="Ball on 3rd pos",], family = binomial(link="logit"))
summary(full_mod_glm)

# Check the performance of this model
smp_size <- floor(0.80 * nrow(basket_first_stat[basket_first_stat$position_dot=="Ball on 3rd pos",])) # create your sample size
set.seed(50)
train_ind <- sample(seq_len(nrow(basket_first_stat[basket_first_stat$position_dot=="Ball on 3rd pos",])), size = smp_size)
train <- basket_first_stat[basket_first_stat$position_dot=="Ball on 3rd pos",][train_ind, ]
test <- basket_first_stat[basket_first_stat$position_dot=="Ball on 3rd pos",][-train_ind, ]
pdata <- predict(full_mod_glm, newdata = train, type = "response")

confusionMatrix(data = as.factor(as.numeric(pdata>=0.5)), reference = as.factor(train$bias_num))

```


###### First testing when ball is at position 8

```{r , echo=FALSE, message=FALSE, warning=FALSE}


full_mod_glm <- glm(as.factor(bias_num) ~ calcul_score_percent + comp_score_percent + School + telephone  + lateralisation +  counting_score + Age +  knownum + literacy_group  + mat_score_percent, data=basket_first_stat[basket_first_stat$position_dot=="Ball on 8th pos",], family = binomial(link="logit"))
summary(full_mod_glm)

# Check the performance of this model
smp_size <- floor(0.80 * nrow(basket_first_stat[basket_first_stat$position_dot=="Ball on 8th pos",])) # create your sample size
set.seed(50)
train_ind <- sample(seq_len(nrow(basket_first_stat[basket_first_stat$position_dot=="Ball on 8th pos",])), size = smp_size)
train <- basket_first_stat[basket_first_stat$position_dot=="Ball on 8th pos",][train_ind, ]
test <- basket_first_stat[basket_first_stat$position_dot=="Ball on 8th pos",][-train_ind, ]
pdata <- predict(full_mod_glm, newdata = train, type = "response")

confusionMatrix(data = as.factor(as.numeric(pdata>=0.5)), reference = as.factor(train$bias_num))


```

###### First testing when ball is at position 3 or 8: multinomial regression

We have the following situation:

 - the participant had a *left* bias for ball at location 3 **AND** the participant had a *left* bias for ball at location 8 -> the participant has a full **L** bias for the first testing
 
 - the participant had a *right* bias for ball at location 3 **AND** the participant had a *right* bias for ball at location 8 -> the participant has a full **L** bias for the first testing

  -  - the participant had a *right* bias for ball at location 3 **AND** the participant had a *left* bias for ball at location 8 (or vice-versa) -> the participant has a full **N** bias for the first testing (basically, it means that the participant has always clicked on the same side, even if the ball changed side)
  
```{r regression exp 5 - 4, echo=FALSE, message=FALSE, warning=FALSE}
quest2021$final_bias_first <- "none"

for (parti in unique(basket_first_stat$participant_id)){
  vec_bias <- basket_first_stat$bias[basket_first_stat$participant_id==parti]
  if (length(which(vec_bias=="L"))==2) { final_bias_first = "L" }
  if (length(which(vec_bias=="R"))==2) { final_bias_first = "R" }
  if (length(which(vec_bias=="R"))==1) { final_bias_first = "N" }
  quest2021$final_bias_first[quest2021$participant_id==parti] <- final_bias_first
}

quest_stat_2 <- quest2021[quest2021$final_bias_first!="none",]
# find the index
### MULTINOMIAL REGRESSION
# All factors
multi1 = multinom(final_bias_first ~  calcul_score_percent + comp_score_percent + School + telephone  + lateralisation +  counting_score + Age +  knownum + literacy_group  + mat_score_percent, data=quest_stat_2)
summary(multi1)

```

Best model:

```{r regression exp 5 - 5, echo=FALSE, message=FALSE, warning=FALSE}

multi2 = multinom(final_bias_first ~  Age  + mat_score_percent, data=quest_stat_2)
summary(multi2)

```

To conclude, we find the following variables to be of importance to predict the bias:

 - calculation score
 - matrice score
 - age
 - literacy group

##### By participant: using all testings

We try here to predict all testings independently. We remove testings == N.

```{r regression exp 5 - 6, echo=FALSE, message=FALSE, warning=FALSE}

### LOGISTIC REGRESSION - for left right bias
baskets_quest_LR <- baskets_quest[baskets_quest$bias == "L" | baskets_quest$bias=="R",]
baskets_quest_LR$bias <- as.numeric(as.factor(baskets_quest_LR$bias))
baskets_quest_LR$bias <- baskets_quest_LR$bias - 1

# All factors
full_mod_glm <- glm(bias ~ calcul_score_percent + comp_score_percent + School + telephone  + lateralisation +  counting_score + Age +  knownum + literacy_group  + mat_score_percent, data=baskets_quest_LR, family = binomial(link="logit"))
summary(full_mod_glm)
```

```{r regression exp 5 - 7, echo=FALSE, message=FALSE, warning=FALSE}

# Stepwise regression
#backwards = step(full_mod_glm)

# best factors
best_mod_glm <- glmer(as.factor(bias) ~ Age + lateralisation + calcul_score_percent + mat_score_percent + knownum + School + literacy_group + (1|participant_id), data=baskets_quest_LR, family = binomial(link="logit"))
summary(best_mod_glm)


```

With this method, we find that the best factors are:

 - age
 - lateralisation
 - calculation score
 - matrice score
 - know numbers
 - school
 - literacy


## VT (Newnewborn)


### Reminder and terminology

#### Groups

In this experiment, there are 2 groups:
 
 - one group that should press the button *only* when the stimuli *decreases*; we call this group the **Decreasing group**
 - one group that should press the button *only* when the stimuli *increases*; we call this group the **Increasing group**
 
#### Stimuli

Both groups sees stimuli that increase **and** decrease. Increasing stimuli presented to the Increasing group are called **"target stimuli"**. In the same way, decreasing stimuli presented to the Decreasing group are called target stimuli too. 
However, in order to check whether the participant had understood the experiment, the Increasing group is also presented with decreasing stimuli and the Decreasing group is also presented with Decreasing stimuli. We call these stimuli the **non-target stimuli**.

#### Condition

Our  hypothesis supports the idea that **decreasing stimuli** will be faster and more accurately detected on the **left** and **increasing stimuli** will be faster and more accuractely detected on the **right.** 

Thus, we create two conditions:
 
 - **congruent condition**: decreasing stimuli on the left for the decreasing group, and increasing stimuli on the right for the increasing group
 - **incongruent condition**: decreasing stimuli on the right for the decreasing group, and increasing stimuli on the left for the increasing group
 
#### Table

For the **decreasing group**:

| Condition --> | **Congruent** (*left*) | **Incongruent** (*right*)  |
|------------|-------------|-------------|
| Stimuli: *Target*  | 15 stimuli |  15 stimuli |
| Stimuli: *Non-target* | 9 stimuli | 9 stimuli | 

This is the same for the **increasing group**:

| Condition --> | **Congruent** (*right*) | **Incongruent** (*left*)  |
|------------|-------------|-------------|
| Stimuli: *Target*  | 15 stimuli |  15 stimuli |
| Stimuli: *Non-target* | 9 stimuli | 9 stimuli | 


#### Errors

The participants need to press a button when being confronted with a *target* stimulus, and wait if presented with a *non-target* stimuli. Thus, we define the **errors** as the following:

 - waiting when being presented with a *target* stimulus;
 - pressing when being presented with a *non-target* stimulus
 
#### Inverse efficiency

We define here a measure of inverse efficiency, which corresponds to the $\frac{time}{correct}$, where correct is the percentage of correct answers.

### Reaction time

Looking at the distribution of the time (with target stimuli and correct answer only):

```{r , echo=F, message=F, warning=FALSE,  fig.cap=capFig('Histogram of time for each experiment and condition.'), fig.height=7, fig.width=10}

ggplot(all_ag[all_ag$stimuli=="target" & (all_ag$exp=="VT_2022" | all_ag$exp=="VT_2021" | all_ag$exp=="VT_adul" | all_ag$exp=="VT_pres"),], aes(x=mean_time, fill=condition))+
  facet_grid(group ~ exp) +
  geom_density(alpha=0.2) +
  theme_bw(base_size=15) +
  labs(x="Mean time")


```


#### Plot reaction time

The function used to look at standard errors is the following: sd(x)/sqrt(length(x))

With all stimuli:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) time for different experiments (VT_2021 and VT_2022 are Himbas2021 and Himbas2022 respectively, VT_adult shows results for Italian adults and VT_pres shows the results for Italian preschoolers), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=9, fig.width=12}

# compute the average mean for each condition, position and situation
all %>%
  filter((exp == "VT_2021" | exp == "VT_2022" | exp == "VT_adul" | exp == "VT_pres") & stimuli=="target") %>%
  dplyr::group_by(condition, group, stimuli, exp) %>%
  dplyr::summarize(se = se(time),
                   time = mean(time, na.rm=TRUE)) -> rt_tab

# plot table
pmean <- ggplot(rt_tab, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Mean time")


all %>%
  filter((exp == "VT_2021" | exp == "VT_2022" | exp == "VT_adul" | exp == "VT_pres") & stimuli=="target") %>%
  dplyr::group_by(condition, group, stimuli, exp) %>%
  dplyr::summarize(se = se(time),
                   time = median(time, na.rm=TRUE)) -> rt_tab_med

# plot table
pmedian <- ggplot(rt_tab_med, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Median time")

grid.arrange(pmean, pmedian, nrow=2)
#rt_tab
```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot.'), fig.height=11, fig.width=14}

pmean2 <- ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], aes(x=exp, y=mean_time,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean time", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pmedian2 <- ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], aes(x=exp, y=median_time,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Median time", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(pmean2, pmedian2, nrow=2)

```


Now looking only at 12 versus 36/4 amorce stimuli:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at reaction time means and se deviation when the amorce is 12, or when the amorce is 4/36.'), fig.height=8, fig.width=12}

# compute the average mean for each condition, position and situation
all %>%
  mutate(amorce2 = ifelse(amorce == 12, "12", "36/4")) %>%
  filter((exp == "VT_2021" | exp == "VT_2022" | exp == "VT_adul" | exp == "VT_pres") & stimuli=="target") %>%
  dplyr::group_by(condition, group, stimuli, exp, amorce2) %>%
  dplyr::summarize(se = se(time),
                   time = mean(time, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(amorce2 ~ exp) +
  labs(x="", y="Mean time")


```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=14, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=8, fig.width=9}

all$telephone <- as.character(all$telephone)

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "time", "mean", "mat", "Score Matrice (%)")

```

#### Statistics

##### All experiments together (except Himbas 2021)

**Experiment & Group & condition**:

Note that *experiment* variable is betweenèsubject, while *group* and *condition* variables are within subject.

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

**Group & condition**

Here we only select condition = decreasing

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = mean_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = median_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

##### Himbas 2022

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian adults

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

##### Italian preschoolers

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

### Errors

Looking at the distribution of errors (with *target* stimuli):

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Histogram of errors for each experiment and condition for target stimuli.'), fig.height=7, fig.width=10}

ggplot(all_ag[all_ag$stimuli=="target" & (all_ag$exp=="VT_2022" | all_ag$exp=="VT_2021" | all_ag$exp=="VT_adul" | all_ag$exp=="VT_pres"),], aes(x=mean_error, fill=condition))+
  facet_grid(group ~ exp) +
  geom_density(alpha=0.2)+
  theme_bw(base_size=15) +
  labs(x="Mean error")

```

Looking at the distribution of errors (with *non-target* stimuli):

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Histogram of errors for each experiment and condition for non-target stimuli.'), fig.height=7, fig.width=10}

ggplot(all_ag[all_ag$stimuli=="non-target" & (all_ag$exp=="VT_2022" | all_ag$exp=="VT_2021" | all_ag$exp=="VT_adul" | all_ag$exp=="VT_pres"),], aes(x=mean_error, fill=condition))+
  facet_grid(group ~ exp) +
  geom_density(alpha=0.2)+
  theme_bw(base_size=15) +
  labs(x="Mean error")

```

#### Plot error


```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) error rate for different experiments (VT_2021 and VT_2022 are Himbas2021 and Himbas2022 respectively, VT_adult shows results for Italian adults and VT_pres shows the results for Italian preschoolers), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=9, fig.width=12}

# compute the average mean for each condition, position and situation
all %>%
  filter(exp == "VT_2021" | exp=="VT_2022" | exp=="VT_adul" | exp=="VT_pres") %>%
  dplyr::group_by(group, condition, stimuli, exp) %>%
  dplyr::summarize(se = se(errors),
                   error = mean(errors)) -> rt_tab_errors

# plot table
ggplot(rt_tab_errors, aes(x=condition, y=error*100, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=error*100-se*100, ymax=error*100+se*100), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(stimuli ~ exp) +
  labs(x="", y="Mean error (%)")

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot. It shows only target stimuli.'), fig.height=11, fig.width=14}

ptarget <- ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], aes(x=exp, y=mean_error,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean error - target", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pnontarget <- ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="non-target",], aes(x=exp, y=mean_error,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean error - non-target", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(ptarget, pnontarget, nrow=2)

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot. It shows only non-target stimuli.'), fig.height=9, fig.width=10}

ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022") & all_ag$stimuli=="non-target",], aes(x=condition, y=mean_error, color=condition, fill=condition))+
  geom_violin(alpha=0.6)+
  geom_jitter(alpha=0.9)+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(group ~ exp) +
  geom_boxplot(width=0.1, color="black", fill="white", alpha=0.2) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  guides(fill=FALSE) +
  labs(y="Mean error", x="", title="DISTRACTOR") 

```


Now looking only at 12 versus 36/4 amorce stimuli:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at reaction time means and se deviation when the amorce is 12, or when the amorce is 4/36.'), fig.height=8, fig.width=12}

# compute the average mean for each condition, position and situation
all %>%
  mutate(amorce2 = ifelse(amorce == 12, "12", "36/4")) %>%
  filter((exp == "VT_2021" | exp == "VT_2022" | exp == "VT_adul" | exp == "VT_pres") & stimuli=="target") %>%
  dplyr::group_by(condition, group, stimuli, exp, amorce2) %>%
  dplyr::summarize(se = se(errors),
                   time = mean(errors, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(amorce2 ~ exp) +
  labs(x="", y="Mean time - 12 dots")


```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=8, fig.width=9}
all_target <- all[all$stimuli == "target",]

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=14, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("VT_2021", "VT_2022"), "errors", "mean", "mat", "Score Matrice (%)")

```

#### Statistics

##### All experiments together (except Himbas 2021)

**Experiment & Group & condition**:

Note that *experiment* variable is between subject, while *group* and *condition* variables are within subject.

So far, all statistics below are only performed using target stimuli.

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```


**Group & condition**

Here we only select condition = decreasing

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = mean_error, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```


##### Himbas 2022

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_error[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_error[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian adults

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian preschoolers

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_error[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_error[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

### Inverse efficiency

Looking at the distribution of the inverse efficiency (with target stimuli and correct answer only):

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Histogram of inverse efficiency for each experiment and condition for non-target stimuli.'), fig.height=7, fig.width=10}

ggplot(all_ag[all_ag$stimuli=="target" & (all_ag$exp=="VT_2022" | all_ag$exp=="VT_2021"),], aes(x=invefi, fill=condition))+
  facet_grid(group ~ exp) +
  geom_density(alpha=0.2)+
  theme_bw(base_size=15) +
  labs(x="Mean inverse efficiency")


```

#### Plot inverse efficiency

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) inverse efficiency for different experiments (VT_2021 and VT_2022 are Himbas2021 and Himbas2022 respectively, VT_adult shows results for Italian adults and VT_pres shows the results for Italian preschoolers), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=9, fig.width=12}

# compute the average mean for each condition, position and situation
all_ag %>%
  filter((exp == "VT_2021" | exp == "VT_2022" | exp == "VT_adul" | exp == "VT_pres") & stimuli == "target") %>%
  filter(stimuli=="target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(se = se(invefi),
                   mean_invefi = mean(invefi, na.rm=TRUE)) -> rt_tab

# plot table
pmean <- ggplot(rt_tab, aes(x=condition, y=mean_invefi, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=mean_invefi-se, ymax=mean_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Mean invefi")


# compute the average mean for each condition, position and situation
all_ag %>%
  filter((exp == "VT_2021" | exp == "VT_2022" | exp == "VT_adul" | exp == "VT_pres") & stimuli == "target") %>%
  filter(stimuli=="target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(se = se(invefi),
                   med_invefi = median(invefi, na.rm=TRUE)) -> rt_tab

# plot table
pmedian <- ggplot(rt_tab, aes(x=condition, y=med_invefi, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=med_invefi-se, ymax=med_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Median invefi")

grid.arrange(pmean, pmedian, nrow=2)
#rt_tab

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot.'), fig.height=11, fig.width=14}

pmean2 <- ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], aes(x=exp, y=invefi,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean inverse efficiency", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pmedian2 <- ggplot(data=all_ag[(all_ag$exp=="VT_2021" | all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], aes(x=exp, y=invefi_med,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Median inverse efficiency", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(pmean2, pmedian2, nrow=2)

```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=14, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=8, fig.width=6}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=8, fig.width=9}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("VT_2021", "VT_2022"), "errors", "mean", "mat", "Score Matrice (%)")

```


#### Statistics

##### All experiments together (except Himbas 2021)

**Experiment & Group & condition**:

Note that *experiment* variable is between subject, while *group* and *condition* variables are within subject.

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

**Group & condition**

Here we only select condition = decreasing

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = invefi, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="VT_2022" | all_ag$exp=="VT_pres" | all_ag$exp=="VT_adul") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = invefi_med, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

##### Himbas 2022

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[all_ag$exp=="VT_2022" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian adults

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[all_ag$exp=="VT_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

##### Italian preschoolers

**Group & condition**

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[all_ag$exp=="VT_pres" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

### Plot all info

With all stimuli : 

```{r , echo=F, message=F, warning=FALSE, fig.height=13, fig.width=10}

all_ag %>%
  filter(stimuli == "target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(time = se(mean_time),
                   error = se(mean_error)*100,
                   mean_invefi = se(invefi))-> se_tab
se_tab_s <- gather(se_tab, type_data, se, time:mean_invefi)

all_ag %>%
  filter(stimuli == "target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(time = mean(mean_time, na.rm=TRUE),
                   error = mean(mean_error, na.rm=TRUE)*100,
                   mean_invefi = mean(invefi, na.rm=TRUE))-> values_tab
values_tab_s <- gather(values_tab, type_data, values, time:mean_invefi)

rt_tab <- merge(se_tab_s, values_tab_s, by=c("group", "exp", "type_data", "condition"), all.x=TRUE, all.y=TRUE)

# rename columns for better plotting

rt_tab %>%
  mutate(group = case_when(group=="decreasing" ~ "Decreasing",
                   group=="increasing" ~ "Increasing"),
         # exp = case_when(exp=="VT_2022" ~ "Himbas2022",
         #           exp=="VT_2021" ~ "Himbas2021",
         #           exp=="VT_pres" ~ "ItalPreschool",
         #           exp=="VT_adul" ~ "ItalAdults"),
        condition = case_when(condition=="congruent" ~ "Congruent",
                   condition=="incongruent" ~ "Incongruent"),
         type_data = case_when(type_data=="error" ~ "Errors (%)",
                   type_data=="time" ~ "Time (sec)",
                   type_data=="mean_invefi" ~ "Inverse eff.")) -> rt_tab

rt_tab$type_data <- factor(rt_tab$type_data, levels = c("Time (sec)", "Errors (%)", "Inverse eff."))

ggplot(rt_tab[rt_tab$exp=="VT_2022" | rt_tab$exp=="VT_2021" | rt_tab$exp=="VT_pres" | rt_tab$exp=="VT_adul",], aes(x=condition, y=values, colour=exp, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=2) +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
 
```

Quick note: SE are present, it is just that sometimes it is too small to be perceived:


```{r , echo=F, message=F, warning=FALSE, fig.height=13, fig.width=13}

p1 <- ggplot(rt_tab[ rt_tab$exp=="VT_pres",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="purple") +
    geom_point(position=position_dodge(0.1), size=2, color="purple") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="Preschoolers") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p2 <- ggplot(rt_tab[ rt_tab$exp=="VT_2021",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="red") +
    geom_point(position=position_dodge(0.1), size=2, color="red") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="Himbas2021") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p3 <- ggplot(rt_tab[ rt_tab$exp=="VT_2022",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="green") +
    geom_point(position=position_dodge(0.1), size=2, color="green") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="Himbas2022") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p4 <- ggplot(rt_tab[ rt_tab$exp=="VT_adul",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="turquoise2") +
    geom_point(position=position_dodge(0.1), size=2, color="turquoise2") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="ItalAdults") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

grid.arrange(p1,p2, p3, p4, nrow=2, ncol=2)
```


## Squares, Candies, Snakes

### Reaction time

#### Plot reaction time

The function used to look at standard errors is the following: sd(x)/sqrt(length(x))


```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) time for different experiments (Squares2022, Candies2022, and Snakes2022 show the results for Himbas2022 while Snakes_adul and Candies_adul show the results for the Italian adults), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=9, fig.width=15}

# compute the average mean for each condition, position and situation
all %>%
  filter((exp == "Squares_2022" | exp == "Snakes_2022" | exp == "Candies_2022" | exp == "Snakes_adul" | exp == "Candies_adul") & stimuli== "target") %>%
  dplyr::group_by(condition, group, stimuli, exp) %>%
  dplyr::summarize(se = se(time),
                   time = mean(time, na.rm=TRUE)) -> rt_tab

# plot table
pmean <- ggplot(rt_tab, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Mean time")


all %>%
  filter((exp == "Squares_2022" | exp == "Snakes_2022" | exp == "Candies_2022" | exp == "Snakes_adul" | exp == "Candies_adul") & stimuli == "target") %>%
  dplyr::group_by(condition, group, stimuli, exp) %>%
  dplyr::summarize(se = se(time),
                   time = median(time, na.rm=TRUE)) -> rt_tab_med

# plot table
pmedian <- ggplot(rt_tab_med, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Median time")

grid.arrange(pmean, pmedian, nrow=2)
#rt_tab

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot.'), fig.height=11, fig.width=14}

pmean2 <- ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="target",], aes(x=exp, y=mean_time,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean time", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pmedian2 <- ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="target",], aes(x=exp, y=median_time,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Median time", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(pmean2, pmedian2, nrow=2)

```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=15, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Squares_2022", "Candies_2022", "Snakes_2022"), "time", "mean", "mat", "Score Matrice (%)")

```


#### Statistics

##### All experiments together (Himbas 2022 & Adults 2022)

**Experiment & Group & condition**:

Note that *experiment* variable is between-subject, while *group* and *condition* variables are within subject.

~~ With the *mean*

Only Candies and Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[( all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022" ) & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

Only Candies and Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[( all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022" ) & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

**Group & condition**

Here we only select condition = decreasing

~~ With the *mean*

Only candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = mean_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = mean_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

Only candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = median_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = median_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

##### Himbas 2022

**Group & condition**

~~ With the *mean*

Candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


~~ With the *median*

Candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

Candies & Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

Candies & Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian adults

**Group & condition**

~~ With the *mean*

Candies:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


~~ With the *median*

Candies : 

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

Snakes :

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_time[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_time[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

Candies

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes
```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$median_time[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$median_time[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

### Errors

#### Plot error


```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) error rate for different experiments (Squares2022, Candies2022, and Snakes2022 show the results for Himbas2022 while Snakes_adul and Candies_adul show the results for the Italian adults), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=9, fig.width=15}

# compute the average mean for each condition, position and situation
all %>%
  filter(exp == "Squares_2022" | exp == "Snakes_2022" | exp == "Candies_2022" | exp == "Snakes_adul" | exp == "Candies_adul") %>%
  dplyr::group_by(group, condition, stimuli, exp) %>%
  dplyr::summarize(se = se(errors),
                   error = mean(errors)) -> rt_tab_errors

# plot table
ggplot(rt_tab_errors, aes(x=condition, y=error*100, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=error*100-se*100, ymax=error*100+se*100), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(stimuli ~ exp) +
  labs(x="", y="Mean error (%)")

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot. It shows only target stimuli.'), fig.height=11, fig.width=14}

ptarget <- ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="target",], aes(x=exp, y=mean_error,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean error - target", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pnontarget <- ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="non-target",], aes(x=exp, y=mean_error,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean error - non-target", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

grid.arrange(ptarget, pnontarget, nrow=2)

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot. It shows only non-target stimuli.'), fig.height=9, fig.width=10}

ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="target",], aes(x=condition, y=mean_error, color=condition, fill=condition))+
  geom_violin(alpha=0.6)+
  geom_jitter(alpha=0.9)+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(group ~ exp) +
  geom_boxplot(width=0.1, color="black", fill="white", alpha=0.2) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  guides(fill=FALSE) +
  labs(y="Mean error", x="", title="DISTRACTOR") 

```

#### Factors

We will only look here at the target stimuli.

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=8, fig.width=13}

all_target <- all[all$stimuli=="target",]

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=15, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all_target, c("Squares_2022", "Candies_2022", "Snakes_2022"), "errors", "mean", "mat", "Score Matrice (%)")

```


#### Statistics

##### All experiments together (Himbas 2022 & Adults 2022)

**Experiment & Group & condition**:

Note that *experiment* variable is between-subject, while *group* and *condition* variables are within subject.

~~ With the *mean*

Only Candies and Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[( all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022" ) & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

**Group & condition**

Here we only select condition = decreasing

~~ With the *mean*

Only candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = mean_error, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = mean_error, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```


##### Himbas 2022

**Group & condition**

~~ With the *mean*

Candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

Candies & Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_error[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_error[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$mean_error[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$mean_error[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian adults



```{r , echo=F, message=F, warning=FALSE}

summary(all_ag$mean_error[all_ag$exp=="Candies_adul" & all_ag$stimuli=="target"])

```

Adults made no errors in the target condition, thus we won't look at the statistics here.
### Inverse efficiency

#### Plot inverse efficiency

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) inverse efficiency for different experiments (Squares2022, Candies2022, and Snakes2022 show the results for Himbas2022 while Snakes_adul and Candies_adul show the results for the Italian adults), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=9, fig.width=15}

# compute the average mean for each condition, position and situation
all_ag %>%
  filter((exp == "Squares_2022" | exp == "Snakes_2022" | exp == "Candies_2022" | exp == "Snakes_adul" | exp == "Candies_adul") & stimuli == "target") %>%
  filter(stimuli=="target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(se = se(invefi),
                   mean_invefi = mean(invefi, na.rm=TRUE)) -> rt_tab

# plot table
pmean <- ggplot(rt_tab, aes(x=condition, y=mean_invefi, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=mean_invefi-se, ymax=mean_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Mean invefi")


# compute the average mean for each condition, position and situation
all_ag %>%
  filter((exp == "Squares_2022" | exp == "Snakes_2022" | exp == "Candies_2022" | exp == "Snakes_adul" | exp == "Candies_adul") & stimuli == "target") %>%
  filter(stimuli=="target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(se = se(invefi),
                   med_invefi = median(invefi, na.rm=TRUE)) -> rt_tab

# plot table
pmedian <- ggplot(rt_tab, aes(x=condition, y=med_invefi, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=med_invefi-se, ymax=med_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Median invefi")

grid.arrange(pmean, pmedian, nrow=2)
#rt_tab

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot.'), fig.height=11, fig.width=14}

pmean2 <- ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="target",], aes(x=exp, y=invefi,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean inverse efficiency", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pmedian2 <- ggplot(data=all_ag[(all_ag$exp=="Squares_2022" | all_ag$exp=="Snakes_2022" | all_ag$exp=="Candies_2022" | all_ag$exp=="Candies_adul" | all_ag$exp=="Candies_adul") & all_ag$stimuli=="target",], aes(x=exp, y=invefi_med,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Median inverse efficiency", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(pmean2, pmedian2, nrow=2)


```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=15, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=8, fig.width=13}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Snakes_2022", "Squares_2022", "Candies_2022"), "errors", "mean", "mat", "Score Matrice (%)")

```


#### Statistics

##### All experiments together (Himbas 2022 & Adults 2022)

**Experiment & Group & condition**:

Note that *experiment* variable is between-subject, while *group* and *condition* variables are within subject.

~~ With the *mean*

Only Candies and Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[( all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022" ) & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

Only Candies and Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[( all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022" ) & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

**Group & condition**

Here we only select condition = decreasing

~~ With the *mean*

Only candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = invefi, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = invefi, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

~~ With the *median*

Only candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_adul" |  all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = invefi_med, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Only snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_adul" |  all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target" & all_ag$group=="decreasing",], dv = invefi_med, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

##### Himbas 2022

**Group & condition**

~~ With the *mean*

Candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


~~ With the *median*

Candies and squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition),
  between = c(exp)
  )

get_anova_table(res.aov)

```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022") & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

Candies & Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

Candies & Squares:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition),
  between = c(exp)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[(all_ag$exp=="Candies_2022" | all_ag$exp=="Squares_2022") & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[(all_ag$exp=="Snakes_2022" ) & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


##### Italian adults

**Group & condition**

~~ With the *mean*

Candies:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


~~ With the *median*

Candies : 

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

Snakes :

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```

**Condition**:

~~ With the *mean*

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

~~ With the *median*

Candies

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[all_ag$exp=="Candies_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```

Snakes
```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(condition)
  )

get_anova_table(res.aov)

t.test(all_ag$invefi_med[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="congruent"],
       all_ag$invefi_med[all_ag$exp=="Snakes_adul" & all_ag$group=="decreasing" & all_ag$stimuli=="target" & all_ag$condition=="incongruent"], paired=TRUE)
```


### Plot all info

With all stimuli : 

```{r , echo=F, message=F, warning=FALSE, fig.height=13, fig.width=10}

all_ag %>%
  filter(stimuli == "target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(time = se(mean_time),
                   error = se(mean_error)*100,
                   mean_invefi = se(invefi))-> se_tab
se_tab_s <- gather(se_tab, type_data, se, time:mean_invefi)

all_ag %>%
  filter(stimuli == "target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(time = mean(mean_time, na.rm=TRUE),
                   error = mean(mean_error, na.rm=TRUE)*100,
                   mean_invefi = mean(invefi, na.rm=TRUE))-> values_tab
values_tab_s <- gather(values_tab, type_data, values, time:mean_invefi)

rt_tab <- merge(se_tab_s, values_tab_s, by=c("group", "exp", "type_data", "condition"), all.x=TRUE, all.y=TRUE)

# rename columns for better plotting

rt_tab %>%
  mutate(group = case_when(group=="decreasing" ~ "Decreasing",
                   group=="increasing" ~ "Increasing"),
         # exp = case_when(exp=="VT_2022" ~ "Himbas2022",
         #           exp=="VT_2021" ~ "Himbas2021",
         #           exp=="VT_pres" ~ "ItalPreschool",
         #           exp=="VT_adul" ~ "ItalAdults"),
        condition = case_when(condition=="congruent" ~ "Congruent",
                   condition=="incongruent" ~ "Incongruent"),
         type_data = case_when(type_data=="error" ~ "Errors (%)",
                   type_data=="time" ~ "Time (sec)",
                   type_data=="mean_invefi" ~ "Inverse eff.")) -> rt_tab

rt_tab$type_data <- factor(rt_tab$type_data, levels = c("Time (sec)", "Errors (%)", "Inverse eff."))


ggplot(rt_tab[rt_tab$exp=="Snakes_2022" | rt_tab$exp=="Candies_2022" | rt_tab$exp=="Squares_2022" | rt_tab$exp=="Snakes_adul" | rt_tab$exp=="Candies_adul",], aes(x=condition, y=values, colour=exp, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=2) +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
 
```

Quick note: SE are present, it is just that sometimes it is too small to be perceived:


```{r , echo=F, message=F, warning=FALSE, fig.height=16, fig.width=13}

p1 <- ggplot(rt_tab[ rt_tab$exp=="Snakes_2022",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="green") +
    geom_point(position=position_dodge(0.1), size=2, color="green") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="Himb_Snakes") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p2 <- ggplot(rt_tab[ rt_tab$exp=="Candies_2022",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="red") +
    geom_point(position=position_dodge(0.1), size=2, color="red") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="Himb_Candies") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p3 <- ggplot(rt_tab[ rt_tab$exp=="Squares_2022",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="purple") +
    geom_point(position=position_dodge(0.1), size=2, color="purple") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="Himb_Squares") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p4 <- ggplot(rt_tab[ rt_tab$exp=="Candies_adul",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="yellow") +
    geom_point(position=position_dodge(0.1), size=2, color="yellow") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="ItalAd_Candies") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p5 <- ggplot(rt_tab[ rt_tab$exp=="Snakes_adul",], aes(x=condition, y=values, group=exp)) + 
    geom_errorbar(aes(ymin=values-se, ymax=values+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1), color="green") +
    geom_point(position=position_dodge(0.1), size=2, color="green") +
   facet_grid(type_data ~ group, scales="free_y") +
  theme_bw(base_size=15) +
  labs(x="", y="", color="Type experiment", title="ItalAd_Snakes") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

grid.arrange(p1, p2, p3, p4, p5, nrow=2, ncol=3)
```


## Luminance

### Reaction time

#### Plot reaction time

The function used to look at standard errors is the following: sd(x)/sqrt(length(x))


```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) time for different experiments (Luminance2022 shows the results for Himbas2022 while Luminance_adul shows the results for the Italian adults), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=8, fig.width=10}

# compute the average mean for each condition, position and situation
all %>%
  filter((exp == "Luminance_2022" | exp == "Luminance_adul") & stimuli == "target") %>%
  dplyr::group_by(condition, group, stimuli, exp) %>%
  dplyr::summarize(se = se(time),
                   time = mean(time, na.rm=TRUE)) -> rt_tab

# plot table
pmean <- ggplot(rt_tab, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Mean time")


all %>%
  filter((exp == "Luminance_2022" | exp == "Luminance_adul") & stimuli == "target") %>%
  dplyr::group_by(condition, group, stimuli, exp) %>%
  dplyr::summarize(se = se(time),
                   time = median(time, na.rm=TRUE)) -> rt_tab_med

# plot table
pmedian <- ggplot(rt_tab_med, aes(x=condition, y=time, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=time-se, ymax=time+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Median time")

grid.arrange(pmean, pmedian, nrow=2)
#rt_tab

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot.'), fig.height=11, fig.width=12}

pmean2 <- ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="target",], aes(x=exp, y=mean_time,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean time", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pmedian2 <- ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="target",], aes(x=exp, y=median_time,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Median time", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(pmean2, pmedian2, nrow=2)

```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=13, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "Gender", "Sex")

```

##### Modernity

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical Ability**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "time", "mean", "mat", "Score Matrice (%)")

```

#### Statistics

These statistics are made intra-subjects, only with Himbas 2022 (note that Himbas 2021 were cross-subjects):

**Using the mean**:

Group & condition:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic using anova test
res.aov <- anova_test(
  data = all_ag[all_ag$exp=="Luminance_2022" & all_ag$stimuli=="target",], dv = mean_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


**Using the median**:

Group & condition:

```{r , echo=F, message=F, warning=FALSE}

res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Luminance_2022") & all_ag$stimuli=="target",], dv = median_time, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```



### Errors

#### Plot error



```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) errors for different experiments (Luminance2022 shows the results for Himbas2022 while Luminance_adul shows the results for the Italian adults), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=8, fig.width=10}

# compute the average mean for each condition, position and situation
all %>%
  filter(exp == "Luminance_2022" | exp == "Luminance_adul") %>%
  dplyr::group_by(group, condition, stimuli, exp) %>%
  dplyr::summarize(se = se(errors),
                   error = mean(errors)) -> rt_tab_errors

# plot table
ggplot(rt_tab_errors, aes(x=condition, y=error*100, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=error*100-se*100, ymax=error*100+se*100), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(stimuli ~ exp) +
  labs(x="", y="Mean error (%)")

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot. It shows only target stimuli.'), fig.height=9, fig.width=10}

ptarget <- ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="target",], aes(x=exp, y=mean_error,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean error - target", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pnontarget <- ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="non-target",], aes(x=exp, y=mean_error,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean error - non-target", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(ptarget, pnontarget, nrow=2)


```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot. It shows only non-target stimuli.'), fig.height=9, fig.width=10}

ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="target",], aes(x=condition, y=mean_error, color=condition, fill=condition))+
  geom_violin(alpha=0.6)+
  geom_jitter(alpha=0.9)+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(group ~ exp) +
  geom_boxplot(width=0.1, color="black", fill="white", alpha=0.2) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  guides(fill=FALSE) +
  labs(y="Mean error", x="", title="DISTRACTOR") 

```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender Bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=13, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "Gender", "Sex")

```

##### Modernity

**Urban index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "uind", "Urban Index")

```

**Urban preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "upref", "Urban Preference")

```

**Urban knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical abilities**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "mat", "Score Matrice (%)")

```

#### Statistics

These statistics are made intra-subjects, only with Himbas 2022 (note that Himbas 2021 were cross-subjects):

**Using the mean**:

Group & condition & stimuli:

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Luminance_2022"),], dv = mean_error, wid = participant_id,
  within = c(group, condition, stimuli)
  )

get_anova_table(res.aov)

```

Group & condition (stimuli == target):

```{r , echo=F, message=F, warning=FALSE}

# Quick statistic
res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Luminance_2022") & all_ag$stimuli=="target",], dv = mean_error, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


### Inverse efficiency

#### Plot inverse efficiency

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Looking at the mean (top) and median (bottom) inverse efficiency for different experiments (Luminance2022 shows the results for Himbas2022 while Luminance_adul shows the results for the Italian adults), different groups (decreasing versus increasing; please note that this is a within-participant design), and different conditions (congruent versus incongruent).'), fig.height=8, fig.width=10}

# compute the average mean for each condition, position and situation
all_ag %>%
  filter((exp == "Luminance_2022" | exp == "Luminance_adul") & stimuli == "target") %>%
  filter(stimuli=="target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(se = se(invefi),
                   mean_invefi = mean(invefi, na.rm=TRUE)) -> rt_tab

# plot table
pmean <- ggplot(rt_tab, aes(x=condition, y=mean_invefi, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=mean_invefi-se, ymax=mean_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Mean invefi")


# compute the average mean for each condition, position and situation
all_ag %>%
  filter((exp == "Luminance_2022" | exp == "Luminance_adul") & stimuli == "target") %>%
  filter(stimuli=="target") %>%
  dplyr::group_by(condition, group, exp) %>%
  dplyr::summarize(se = se(invefi),
                   med_invefi = median(invefi, na.rm=TRUE)) -> rt_tab

# plot table
pmedian <- ggplot(rt_tab, aes(x=condition, y=med_invefi, colour=group, group=group)) + 
    geom_errorbar(aes(ymin=med_invefi-se, ymax=med_invefi+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ exp) +
  labs(x="", y="Median invefi")

grid.arrange(pmean, pmedian, nrow=2)
#rt_tab

```

Same but values for each participant:

```{r , echo=F, message=F, warning=FALSE, fig.cap=capFig('Same plot as above, except that here the results for each participant is presented (point), with boxplot and violin plot.'), fig.height=9, fig.width=10}

pmean2 <- ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="target",], aes(x=exp, y=invefi,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Mean inverse efficiency", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))

pmedian2 <- ggplot(data=all_ag[(all_ag$exp=="Luminance_2022" | all_ag$exp=="Luminance_adul") & all_ag$stimuli=="target",], aes(x=exp, y=invefi_med,  fill=condition))+
  geom_violin(alpha=0.2, position = position_dodge(1))+
  #geom_jitter(alpha=0.9, position = position_dodge(1))+
  #stat_summary(fun.y=mean, geom="point", shape=23, size=2)
  facet_grid(. ~ group) +
  geom_boxplot(width=0.1, alpha=0.7, position = position_dodge(1)) +
  theme_bw(base_size=15) +
  scale_fill_manual(values=c("tomato3", "slateblue2")) +
  scale_color_manual(values=c("tomato3", "slateblue2")) +
  #guides(fill=FALSE) +
  labs(y="Median inverse efficiency", x="") +
  theme(axis.text.x = element_text(angle = 30, hjust=1))
grid.arrange(pmean2, pmedian2, nrow=2)

```

#### Factors

##### School & Literacy Bias

**School**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "School", "School?")

```

**Literacy**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "literacy_group", "Literacy level")

```

##### Age & Gender bias

**Age**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on bias order.'), fig.height=13, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "age_cut", "Age")

```

**Gender**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "Gender", "Sex")

```

##### Modernity Index

**Urban Index**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "uind", "Urban Index")

```

**Urban Preference**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "upref", "Urban Preference")

```

**Urban Knowledge**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "uknow", "Urban Knowledge")

```

**Telephone**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "telephone", "Possess a telephone")

```

**Know arabic numbers**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "knownum", "Know arabic numbers")

```

##### Intelligence

**Numerical abilities**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "numab", "Numerical Ability")

```

**Short Term memory**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "mct", "Short term memory")

```

**Score Matrice**:

These plots only show the mean.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on bias order.'), fig.height=7, fig.width=7}

# indicate: dataframe to use, list of experiments, time/errors/invefi, mean/median, variable, y axis title
plot_bias(all, c("Luminance_2022"), "errors", "mean", "mat", "Score Matrice (%)")

```

#### Statistics


**Using the mean**:

Group & condition:

```{r , echo=F, message=F, warning=FALSE}

all_ag <- all_ag %>% ungroup

res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Luminance_2022") & all_ag$stimuli=="target",], dv = invefi, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```


**Using the median**:

Group & condition:

```{r , echo=F, message=F, warning=FALSE}

all_ag <- all_ag %>% ungroup

res.aov <- anova_test(
  data = all_ag[(all_ag$exp=="Luminance_2022") & all_ag$stimuli=="target",], dv = invefi_med, wid = participant_id,
  within = c(group, condition)
  )

get_anova_table(res.aov)

```



<!-- ## GAZE TRACKING -->

<!-- ```{r , echo=F, message=F, warning=FALSE} -->

<!-- pathf = (paste(path,"analysis_manualcoding", sep="")) -->

<!-- lfiles = list.files(path=pathf, pattern=NULL, all.files=FALSE,full.names=FALSE) -->

<!-- df = data.frame(matrix(ncol=6, nrow=0)) -->
<!-- for (fil in lfiles){ -->
<!--   d <- read.csv(paste(path, "analysis_manualcoding/", fil, sep=""), header=T) -->
<!--   part_id = substr( as.character(fil), 1, 4) -->
<!--   dots = substr( as.character(fil), 15, 15) -->
<!--   d$participant_id = part_id -->
<!--   d$dots <- dots -->
<!--   df <- rbind(df, d) -->
<!-- } -->

<!-- # correct a mistake -->
<!-- df$type_for_plot[df$number_of_stimuli==36] <- "testing_36" -->
<!-- df$dots[df$dots=="3"] <- "36" -->
<!-- df$dots <- as.numeric(df$dots) -->

<!-- ``` -->


<!-- ```{r , echo=F, message=F, warning=FALSE} -->

<!-- df %>% count(participant_id, dots, type_for_plot, position) -> df1 -->

<!-- df %>% count(dots, type_for_plot, position) -> df2 -->

<!-- df <- df[complete.cases(df), ] -->

<!-- df1$type <- "testing" -->
<!-- df1$type[df1$type_for_plot=="habituation"] <- "habituation" -->

<!-- df1$concat <- paste(df1$participant_id, df1$type, df1$dots, sep="_") -->
<!-- df_fin = data.frame(matrix(nrow = 0, ncol = 7))  -->

<!-- for (el in unique(df1$concat)){ -->
<!--   subdf <- df1[df1$concat == el,] -->
<!--   somme = sum(subdf$n) -->
<!--   sommelr = sum(subdf$n[(subdf$position=="LEFT" | subdf$position=="RIGHT") & is.na(subdf$position)==FALSE]) -->
<!--   subdf$posall <- subdf$n/somme -->
<!--   subdf$poslr <- subdf$n/sommelr -->
<!--   subdf$poslr[subdf$position=="CENTER" | subdf$position == "CLOSED_UNSURE"] <- NA -->

<!--   subdf %>% -->
<!--     select(participant_id, dots, type, position, posall, poslr) -> subdf2 -->

<!--   df_fin <- rbind(df_fin, subdf2) -->
<!-- } -->


<!-- ggplot(df1[(df1$position=="LEFT" | df1$position=="RIGHT") & is.na(df1$type)==FALSE & is.na(df1$dots)==FALSE & is.na(df1$position)==FALSE,], aes(x=position, y=n, group=participant_id)) + -->
<!--   facet_grid(type ~ dots)+ -->
<!--   geom_point() + -->
<!--   geom_line()+ -->
<!--   guides(color=FALSE) + -->
<!--   theme_bw(base_size=15) -->

<!-- ggplot(df1[(df1$position=="LEFT" | df1$position=="RIGHT") & is.na(df1$type)==FALSE & is.na(df1$dots)==FALSE & is.na(df1$position)==FALSE,], aes(x=position, y=n)) + -->
<!--   facet_grid(type ~ dots)+ -->
<!--   geom_boxplot()+ -->
<!--   guides(color=FALSE) + -->
<!--   theme_bw(base_size=15) -->




<!-- ggplot(df_fin[(df_fin$position=="LEFT" | df_fin$position=="RIGHT") & is.na(df_fin$type)==FALSE & is.na(df_fin$dots)==FALSE & is.na(df_fin$position)==FALSE,], aes(x=position, y=poslr, group=participant_id)) + -->
<!--   facet_grid(type ~ dots)+ -->
<!--   geom_point() + -->
<!--   geom_line()+ -->
<!--   guides(color=FALSE) + -->
<!--   theme_bw(base_size=15) -->

<!-- ggplot(df_fin[(df_fin$position=="LEFT" | df_fin$position=="RIGHT") & is.na(df_fin$type)==FALSE & is.na(df_fin$dots)==FALSE & is.na(df_fin$position)==FALSE,], aes(x=position, y=n)) + -->
<!--   facet_grid(type ~ dots)+ -->
<!--   geom_boxplot()+ -->
<!--   guides(color=FALSE) + -->
<!--   theme_bw(base_size=15) -->


<!-- ``` -->


## NumberLine (logarithmic or linear)

### Reminder on Dahaene et al (2008) paper

 - 33 Mundurucus (adults + children)
 - *"The Mundurucu’s mean responses revealed that **they understood the task**. Although some participants tended to use only the end points of the scale, most used the full response continuum and adopted a consistent strategy of mapping consecutive numbers onto consecutive locations"*
 - *"The Mundurucu population is heterogeneous, and some of our participants, particularly the children, had received a little education."*
 - *"Crucially, the Mundurucu’s nonlinearity remained significant even when the analysis was restricted to adults (t = 4.34, 23 df, P = 0.0002), to monolingual speakers (t = 5.36, 29 df, P < 10 −5), or to uneducated participants (t = 2.60, 7 df, P = 0.035; see figs. S7 to S10 for a graphic depiction of subgroup performance) (24). t tests and linear and rank-order regression analyses showed **no effect of gender, age, education, or bilingualism**. There was only a trend toward reduced nonlinearity as a function of age (Kendall tau = –0.23, P = 0.055)."*
 
No access to more detailed Materials and Methods: error 404 on Science journal

![*Illustration N: extracted from Dehaene et al (2008) paper. Average location of segments on the horizontal segments, for American and Mundurucs participants. Experiments: dots.*](pictures/dehaene_answer.png){width=50%}


### Exclusion criteria


#### Criteria exclusion 1: no correct order

Many participants have not understood this task. We remove participants for which the numbers are not placed in a ordered way (`r length(list_part_remove)` out of `r length(unique(nbline_aggr$participant_id))` participants).

```{r plot exp6 without outlier strict, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average location of segments on the horizontal segments. This graph excludes participants using Pica et al (2008)s method.'), fig.height=6, fig.width=6}


nbline_noout$stim_number <- as.numeric(nbline_noout$stim_number)

# the size of the segment is 1280 pixel. 
accurate_pos <- c( -640, -640+142.2222, -640+(142.2222*2), -640+(142.2222*3), -640+(142.2222*4), -640+(142.2222*5), -640+(142.2222*6), -640+(142.2222*7), -640+(142.2222*8), -640+(142.2222*9), 640)


ggplot(nbline_noout[is.na(nbline_noout$x)==FALSE,], aes(x=stim_number, y=x)) + 
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  geom_smooth() +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  labs(x="Stimulus number", y="Response location")
 
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average location of segments on the horizontal segments. This graph excludes participants using Pica et al (2008)s method.'), fig.height=10, fig.width=10}
 
# split num score into two groups of equal size
nbline_quest$numab <- as.numeric(cut2(nbline_quest$num_score, g=2))
nbline_quest$numcomp <- "Comparison score: low"
nbline_quest$numcomp[nbline_quest$comp_score_percent == 100] <- "Comparison score: high"
nbline_quest$numcal <- as.numeric(cut2(nbline_quest$calcul_score_percent, g=2))
nbline_quest$numcount <- as.numeric(cut2(nbline_quest$counting_score, g=2))

nbline_quest %>%
  mutate(numab=case_when(numab==1 ~ "Numerical abilities: low",
                         numab==2 ~ "Numerical abilities: high"),
         numcal=case_when(numcal==1 ~ "Calculation score: low",
                         numcal==2 ~ "Calculation score: high"),
         numcount=case_when(numcount==1 ~ "Counting score: low",
                         numcount==2 ~ "Counting score: high"),
         school=case_when(School=="yes" ~ "School: yes",
                         School=="no" ~ "School: no"),
         knownum=case_when(knownum=="yes" ~ "Know numbers: yes",
                         knownum=="no" ~ "Know numbers: no"),
         numab = factor(numab),
         numcomp = factor(numcomp),
         numcal = factor(numcal),
         numcount = factor(numcount)) -> nbline_quest

nbline_quest$participant_id <- as.factor(nbline_quest$participant_id)

ggplot(nbline_quest[is.na(nbline_quest$x)==FALSE,], aes(x=stim_number, y=x, color=participant_id)) + 
  geom_smooth(se=FALSE) +
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  facet_grid(School ~ numab) +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  labs(x="Stimulus number", y="Response location") +
  guides(color=FALSE)

ggplot(nbline_quest[is.na(nbline_quest$x)==FALSE,], aes(x=stim_number, y=x, color=participant_id)) + 
  geom_smooth(se=FALSE) +
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  facet_grid(school ~ numcal) +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  labs(x="Stimulus number", y="Response location") +
  guides(color=FALSE)

ggplot(nbline_quest[is.na(nbline_quest$x)==FALSE,], aes(x=stim_number, y=x, color=participant_id)) + 
  geom_smooth(se=FALSE) +
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  facet_grid(school ~ numcomp) +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  labs(x="Stimulus number", y="Response location") +
  guides(color=FALSE)

ggplot(nbline_quest[is.na(nbline_quest$x)==FALSE,], aes(x=stim_number, y=x, color=participant_id)) + 
  geom_smooth(se=FALSE) +
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  facet_grid(school ~ numcount) +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  labs(x="Stimulus number", y="Response location") +
  guides(color=FALSE)
                            
```


However, this exclusion criteria was very selective and we had only `length(unique(nbline_noout$participant_id))` participants. Among these `r length(unique(nbline_noout$participant_id))` participants, `r nrow(nbline_quest[nbline_quest$school=="yes",])/10` have attended school, so half of the participant. 

```{r plot exp6 with no outlier no school, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average location of segments on the horizontal segments. This graph excludes participants from Pica et al (2008) strict exclusion criteria and participants that have been to school.'), fig.height=10, fig.width=10}

ggplot(nbline_quest, aes(x=stim_number, y=x)) + 
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  geom_smooth() +
  facet_grid(School ~ knownum) +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  theme(plot.title = element_text(hjust = 0.5, size=15)) +
  labs(x="Stimulus number", y="Response location", title="Have been to school?") 


```



<!-- ```{r plot exp6 with no outlier no school, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average location of segments on the horizontal segments. This graph excludes participants from Pica et al (2008) strict exclusion criteria and participants that do not know arabic numbers.'), fig.height=6, fig.width=10} -->

<!-- nbline_noout$knownum <- "yes" -->
<!-- nbline_noout$knownum[nbline_noout$participant_id %in% list_part_no_num] <- "no" -->
<!-- nbline_noout$knownum <- factor(nbline_noout$knownum, levels=c("yes", "no")) -->

<!-- ggplot(nbline_noout, aes(x=stim_number, y=x)) +  -->
<!--     #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) + -->
<!--   stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) + -->
<!--   stat_summary(fun=mean, geom="point", color="black", size=3) + -->
<!--   theme_bw(base_size=15) + -->
<!--   geom_smooth() + -->
<!--   facet_grid(. ~ knownum) + -->
<!--   geom_jitter(alpha=0.1, position=position_jitter(0.2)) + -->
<!--   scale_x_discrete(limits=factor(1:10)) + -->
<!--   scale_y_discrete(limits=accurate_pos, labels=c(1:10)) + -->
<!--   theme(plot.title = element_text(hjust = 0.5, size=15)) + -->
<!--   labs(x="Stimulus number", y="Response location", title="Know arabic numbers?")  -->



<!-- ``` -->

We can not study the impact of knowing arabic numbers: indeed, `r length(unique(nbline_quest$participant_id[nbline_quest$knownum=="yes"]))` participants know arabic numbers and only `r length(unique(nbline_quest$participant_id[nbline_quest$knownum=="no"]))` does not know arabic numbers. 


#### Criteria exclusion 2: allowing for some inversions

Many participants understood the consigns but might have done some inversion in the order with which they classified the numbers. For example, the order for some of them might be "1 - 2 - 3 - 4 - 5 - 6 - 7 - **9 - 8** - 10. In this case, there is **one inversion**.

If we soften the previous exclusion criteria, we might allow for some inversions. Thus, we observe what happens if we use an inversion index of 1, 2 or 3.

```{r remove outliers part 2 exp 6, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average location of segments on the horizontal segments. This graph excludes participants using a less strict method compared to Pica et al (2008)s method.'), fig.height=6, fig.width=6}

df_total_excl2 <- nbline_noout[,c(1:3)]
df_total_excl2$inversion <- 0

for (invers in c(1,2,3)) {
  
  nb_inversion_ok = invers
  
  list_part_remove2 <- c()
  for (part in unique(nbline_aggr$participant_id)){
    items = as.numeric(nbline_aggr$x[nbline_aggr$participant_id==part])
    item_ordered = sort(items, decreasing=FALSE)
    if (sum((items==item_ordered), na.rm = TRUE) < (10 - (nb_inversion_ok*2))){
      list_part_remove2 <- c(list_part_remove2, part)
    }
  }
  
  nbline_noout2 <- nbline_aggr[!(nbline_aggr$participant_id %in% list_part_remove2),]
  nbline_noout2$inversion <- invers
  df_total_excl2 <- rbind(df_total_excl2, nbline_noout2)
}

```



```{r plot exp6 no outlier less strict, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Average location of segments on the horizontal segments. This graph excludes participants using a less strict method compared to Pica et al (2008)s method.'), fig.height=12, fig.width=5}

df_total_excl2$stim_number <- as.numeric(df_total_excl2$stim_number)

df_total_excl2 %>%
  mutate(inversion = case_when(inversion == 0 ~ "0 inversion",
                               inversion == 1 ~ "1 inversions",
                               inversion == 2 ~ "2 inversions",
                               inversion == 3 ~ "3 inversions")) -> df_total_excl2

  ggplot(df_total_excl2[!(df_total_excl2$inversion=="0 inversion"),], aes(x=stim_number, y=x)) + 
    #geom_dotplot(binaxis='y', stackdir='center', alpha=0.1, size=0.1) +
  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom="errorbar", color="black", width=0.3) +
  stat_summary(fun=mean, geom="point", color="black", size=3) +
  theme_bw(base_size=15) +
  geom_smooth() +
    facet_grid(inversion ~ .) +
  geom_jitter(alpha=0.1, position=position_jitter(0.2)) +
  scale_x_discrete(limits=factor(1:10)) +
    ylim(-640-142.222, 640+142.222) +

  scale_y_discrete(limits=accurate_pos, labels=c(1:10)) +
  labs(x="Stimulus number", y="Response location") 

nbline_quest2 <- merge(nbline_noout2, quest2021, by="participant_id", all.x=TRUE)
part_keep_noschool <- unique(nbline_quest2$participant_id[nbline_quest2$school=="no"])


```


## GazeTracking experiment

### Himbas 2021


In this experiment, we extracted manually and automatically the eye location in order to find where the participant is looking at (right or left side). We selected only participants with a high correlation, using 2 different methods: 

 - a conservative method (selecting only the best participants for a very accurate tracking on the left and right side). It contains `r length(exp1_small)` participants for the first part and `r length(exp2_small)` participants for the second part.
 
 - a less conservative methods (selecting all participants for which the tracking was OK). It contains `r length(exp1_big)` participants for the first part and `r length(exp2_big)` participants for the second part.

For more details about the exact choices, please refer to the RMarkdown file eye_tracking.html.

```{r read table exp 4, echo=FALSE, message=FALSE, warning=FALSE}

summary(gt2021_part1)

#summary(gt2022_part1_man)

```


These files only contain data for the testing stimulus.
Here are the columns explanation:

| name column  | Short description |
|------------|-------------|
| *index* | Index per participant | 
| *condition* |  **Increase** if the testing stimulus is bigger compared to the habituation stimulus, **Decrease* if the testing stimulus is smaller compared to the habituation stimulus | 
| *sub_exp* |  Exp number: this is the number of the testing stimulus presentation (in total, there 6 testing stimulus presentation per participant) | 
| *number_experiment* | Number of the experiment: part 1 or 2 (see Method for more information) | 
| *participant_id* | Some participants could not be properly tracked by DLC and were removed. | 
| *manual* | Eye location of the **manual tracking** per frame. | 
| *type_for_plot* | habituation and the type of testing |
| *automatic* | Eye location of the **automatic tracking** per frame. | 

#### Part 1


##### All frames

Plotting the automated tracking result (but excluding participant where the correspondency between manual and automated tracking is too low)

###### Big dataset

There are `r length(exp1_big)` participants with this method.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants by the automated tracking. Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


## Check correlation between manual and automated tracking
ggplot(part1_big, aes(x=automatic, fill=automatic))+
  geom_histogram(stat="count", position="dodge") +
  theme_bw(base_size=15) +
  facet_grid(condition ~ .) +
  scale_fill_manual(values=c( "azure3", "azure2", "dodgerblue",  "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  labs(y="count: frames") +
  guides(fill=FALSE)


```

we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part1_big_reg <- part1_big
part1_big_reg <- part1_big_reg[part1_big_reg$automatic=="Right" | part1_big_reg$automatic == "Left",]
part1_big_reg$automatic <- as.numeric(as.factor(part1_big_reg$automatic)) - 3

model = glmer(automatic ~ condition + (1 | participant_id), data=part1_big_reg, family = binomial(link="logit"))
summary(model)

```



###### Small dataset 

There are `r length(exp1_small)` participants with this method.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants by the automated tracking. Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


## Check correlation between manual and automated tracking
ggplot(part1_small, aes(x=automatic, fill=automatic))+
  geom_histogram(stat="count", position="dodge") +
  theme_bw(base_size=15) +
  facet_grid(condition ~ .) +
  scale_fill_manual(values=c( "azure3", "azure2", "dodgerblue",  "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  labs(y="count: frames") +
  guides(fill=FALSE)


```


we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part1_small_reg <- part1_small
part1_small_reg <- part1_small_reg[part1_small_reg$automatic=="Right" | part1_small_reg$automatic == "Left",]
part1_small_reg$automatic <- as.numeric(as.factor(part1_small_reg$automatic)) - 3

model = glmer(automatic ~ condition + (1 | participant_id), data=part1_small_reg, family = binomial(link="logit"))
summary(model)

```


##### Laterality index

We compute a **laterality index** per participant: this index is defined as

$$\frac{R}{R + L}$$
where $R$ are the frames where the participant looked on the right and $L$ are the frames where the participant looked on the left.

###### Big dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Laterality index per participant, for the testing with 36 dots and the testing with 4 dots, using the automatic tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


# compute laterality index
part1_big %>%
  filter(automatic == "Left" | automatic=="Right") %>%
  group_by(participant_id, condition) %>%
  count(automatic) %>%
  tidyr::spread(automatic, n) %>%
  mutate(sum_left_right = sum(Right, Left, na.rm=TRUE),
         latindex = Right/sum_left_right) -> latind_corr_m1

# check the specific case where LI is 0 or 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Right)] <- 0
latind_corr_m1$latindex[is.na(latind_corr_m1$Left)] <- 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Left) & is.na(latind_corr_m1$Right)] <- NA

#plot
ggplot(latind_corr_m1, aes(x=condition, y=latindex, fill=condition)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.2, position=position_jitter(0.2)) +
  theme_bw(base_size=15) +
  ylim(0,1) +
  geom_hline(yintercept=0.5, linetype="dashed", color = "black") +
  labs(y="Left <--    Laterality index    --> Right", x="") +
  guides(fill=FALSE)


df_latind_ok <- latind_corr_m1

```


Statistics:

 - Comparing the Increase condition to the theoretical mean of 0.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Increase"], mu=0.5)

```

 - Comparing the testing with 4 dots to the theoretical mean of 0.5:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Decrease"], mu=0.5)

```


###### Small dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Laterality index per participant, for the testing with 36 dots and the testing with 4 dots, using the automatic tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}

# compute laterality index
part1_small %>%
  filter(automatic == "Left" | automatic=="Right") %>%
  group_by(participant_id, condition) %>%
  count(automatic) %>%
  tidyr::spread(automatic, n) %>%
  mutate(sum_left_right = sum(Right, Left, na.rm=TRUE),
         latindex = Right/sum_left_right) -> latind_corr_m1

# check the specific case where LI is 0 or 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Right)] <- 0
latind_corr_m1$latindex[is.na(latind_corr_m1$Left)] <- 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Left) & is.na(latind_corr_m1$Right)] <- NA

#plot
ggplot(latind_corr_m1, aes(x=condition, y=latindex, fill=condition)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.2, position=position_jitter(0.2)) +
  theme_bw(base_size=15) +
  ylim(0,1) +
  geom_hline(yintercept=0.5, linetype="dashed", color = "black") +
  labs(y="Left <--    Laterality index    --> Right", x="") +
  guides(fill=FALSE)


```


Statistics:

 - Comparing the Increase condition to the theoretical mean of 0.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Increase"], mu=0.5)

```

 - Comparing the testing with 4 dots to the theoretical mean of 0.5:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Decrease"], mu=0.5)

```

##### Side first gaze

We also measure the side of the first gaze during each testing. 

Each partipant undergoes **6 experiments**: 3 for the testing with 36 dots, and 3 with the testing with 4 dots. There are `r length(unique(latind_corr_m1$participant_id))` participants, so in total `r length(unique(latind_corr_m1$participant_id))*6` testings have been done. The Figure below shows the distribution of the first look side for all of these experiments. 

###### Big dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the direction of the first gaze for all testings, in the condition with 36 dots and the testing with 4 dots, using the automated tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


df_firstlook =data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_firstlook) = c("participant_id", "first_look_side", "exp_num", "dots")

myrow = 1
for (part_id in unique(part1_big$participant_id)){
  subdf = part1_big[part1_big$participant_id==part_id,]
  subdf$exp = "0"
  
  # find number of exp
  myind=0
  for (exp in c(1:6)){
    for (el in (1:90)){
      subdf$exp[(90*myind)+el] = myind+1
    }
    myind = myind + 1
  }
  
  # then check first occurence in each exp
  for (myexp in unique(subdf$exp)){
    subsubdf = subdf[subdf$exp==myexp,]
    subsubdf = subsubdf[subsubdf$automatic=="Left" | subsubdf$automatic=="Right",]
    
    df_firstlook[myrow,] <- c(part_id, as.character(subsubdf$automatic[1]), myexp, as.character(subsubdf$condition[1]))
    myrow= myrow+1
    
  }
}

df_firstlook$first_look_side <- as.character(df_firstlook$first_look_side)
df_firstlook$first_look_side[is.na(df_firstlook$first_look_side)] <- "No side"
df_firstlook$first_look_side <- as.factor(df_firstlook$first_look_side)


ggplot(data=df_firstlook[is.na(df_firstlook$dots)==FALSE,], aes(x=first_look_side, fill=first_look_side))+
  geom_histogram(stat="count", position="dodge", size=1)+
  theme_bw(base_size=15) +
  facet_grid(dots ~ .) +
  labs(x="First look side") +
  scale_fill_manual(values=c("dodgerblue", "darkgoldenrod1"))+
  guides(fill=FALSE) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 

df_firstlook_ok <- df_firstlook

```


we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part1_big_reg <- df_firstlook
part1_big_reg <- part1_big_reg[part1_big_reg$first_look_side=="Right" | part1_big_reg$first_look_side == "Left",]
part1_big_reg$side <- 0
part1_big_reg$side[part1_big_reg$first_look_side == "Right"] <- 1

model = glmer(side ~ dots + (1 | participant_id), data=part1_big_reg, family = binomial(link="logit"))
summary(model)

```

###### Small dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the direction of the first gaze for all testings, in the condition with 36 dots and the testing with 4 dots, using the automated tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}

## CORR M1
df_firstlook =data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_firstlook) = c("participant_id", "first_look_side", "exp_num", "dots")

myrow = 1
for (part_id in unique(part1_small$participant_id)){
  subdf = part1_small[part1_small$participant_id==part_id,]
  subdf$exp = "0"
  
  # find number of exp
  myind=0
  for (exp in c(1:6)){
    for (el in (1:90)){
      subdf$exp[(90*myind)+el] = myind+1
    }
    myind = myind + 1
  }
  
  # then check first occurence in each exp
  for (myexp in unique(subdf$exp)){
    subsubdf = subdf[subdf$exp==myexp,]
    subsubdf = subsubdf[subsubdf$automatic=="Left" | subsubdf$automatic=="Right",]
    
    df_firstlook[myrow,] <- c(part_id, as.character(subsubdf$automatic[1]), myexp, as.character(subsubdf$condition[1]))
    myrow= myrow+1
    
  }
}

df_firstlook$first_look_side <- as.character(df_firstlook$first_look_side)
df_firstlook$first_look_side[is.na(df_firstlook$first_look_side)] <- "No side"
df_firstlook$first_look_side <- as.factor(df_firstlook$first_look_side)


ggplot(data=df_firstlook[is.na(df_firstlook$dots)==FALSE,], aes(x=first_look_side, fill=first_look_side))+
  geom_histogram(stat="count", position="dodge", size=1)+
  theme_bw(base_size=15) +
  facet_grid(dots ~ .) +
  labs(x="First look side") +
  scale_fill_manual(values=c("dodgerblue", "darkgoldenrod1"))+
  guides(fill=FALSE) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 


```


we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part1_big_reg <- df_firstlook
part1_big_reg <- part1_big_reg[part1_big_reg$first_look_side=="Right" | part1_big_reg$first_look_side == "Left",]
part1_big_reg$side <- 0
part1_big_reg$side[part1_big_reg$first_look_side == "Right"] <- 1

model = glmer(side ~ dots + (1 | participant_id), data=part1_big_reg, family = binomial(link="logit"))
summary(model)

```


##### Factors

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# merge questionnaire
gt2021_quest <- merge(part1_big, quest2021, by="participant_id", all.x=TRUE)
gt2021_quest2 <- gt2021_quest[gt2021_quest$automatic=="Left" | gt2021_quest$automatic=="Right",]

```

###### School & Literacy

**School**:

Note that here, "L" means "left", "R" means "right", and **"N" means "Center"**.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the school on gaze side.'),  fig.width=10, fig.height=6}

frequency_plot(gt2021_quest2, "Himbas2021", "School", "Gaze", "School")

```

**Literacy**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers  on gaze side.')}

frequency_plot(gt2021_quest2, "Himbas2021", "literacy_group", "Gaze", "Literacy_group")


```

######Age & Gender

**Age**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on gaze side.')}

frequency_plot(gt2021_quest2, "Himbas2021", "age_cut", "Gaze", "Age")

```

**Gender**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on gaze side.')}

frequency_plot(gt2021_quest2, "Himbas2021", "Gender", "Gaze", "Gender")

```

###### Modernity

**Telephone**:

Owning a telephone has absolutely no effect on the bias order of the participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the card ordering  on gaze side.'),  fig.width=10, fig.height=6}
 
frequency_plot(gt2021_quest2, "Himbas2021", "telephone", "Gaze", "Possess a telephone")

```


**Knowing arabic numbers**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on gaze side.')}

frequency_plot(gt2021_quest2, "Himbas2021", "knownum", "Gaze", "Know arabic number")

```

###### Intelligence

**Bias for numerical abilities effect**:

There are 3 participants for which no data on numerical abilities nor raven matrice are available.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical abilities on gaze side.'), fig.width=6, fig.height=12}

frequency_plot(gt2021_quest2, "Himbas2021", "numab", "Gaze", "Numerical ability")
#frequency_plot(gt2021_quest2, "Himbas2021", "numcomp", "Gaze", "Comparison score")
#frequency_plot(gt2021_quest2, "Himbas2021", "numcal", "Gaze", "Calculation score")
#frequency_plot(gt2021_quest2, "Himbas2021", "numcount", "Gaze", "Counting score")


```


**Matrice**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of raven matrice on gaze side.')}

frequency_plot(gt2021_quest2, "Himbas2021", "mat", "Gaze", "Matrice score")

```

##### Regression

###### Based on all data

**Both conditions (increase and decrease)**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

gt2021_quest2$side_num <- as.numeric(as.factor(gt2021_quest2$automatic)) -1

model_reg <- glmer(as.factor(side_num) ~ as.factor(condition) + as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) + (1|participant_id), data = gt2021_quest2, family=binomial(link="logit"))
summary(model_reg)


gt2021_quest3 <- gather(gt2021_quest2, type_num, measurement, c(num_score, comp_score_percent, counting_score, calcul_score_percent))

ggplot(gt2021_quest3, aes(x=measurement, y=side_num, color=type_num))+
  geom_point() +
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

```

**Condition: increase**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_reg <- glmer(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) + (1|participant_id), data=gt2021_quest2[gt2021_quest2$condition=="Increase",], family=binomial(link="logit"))
summary(model_reg)


```

**Condition: decrease**


```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_reg <- glmer(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) + (1|participant_id), data=gt2021_quest2[gt2021_quest2$condition=="Decrease",], family=binomial(link="logit"))
summary(model_reg)

```

###### Based on the laterality index

**Both conditions (increase and decrease)**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

quest_latind <- merge(df_latind_ok, quest2021, by="participant_id")

model_latind <- lm(latindex ~ as.factor(School) + as.factor(condition) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=quest_latind)
summary(model_latind)

```

**Condition: increase**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_latind <- lm(latindex ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=quest_latind[quest_latind$condition =="Increase",])
summary(model_latind)
```

**Condition: decrease**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_latind <- lm(latindex ~ as.factor(School) +  as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=quest_latind[quest_latind$condition =="Decrease",])
summary(model_latind)

```


###### Based on the first look index

**Both conditions (increase and decrease)**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## FIRST LOOK

firstlook_quest <- merge(df_firstlook_ok, quest2021, by="participant_id")


firstlook_quest$side_num <- as.numeric(as.factor(firstlook_quest$first_look_side)) -1

model_fl <- glm(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data = firstlook_quest, family=binomial(link="logit"))
summary(model_fl)

```

**Condition: increase**

```{r, echo=FALSE, message=FALSE, warning=FALSE}


model_fl <- glm(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) , data=firstlook_quest[firstlook_quest$dots=="Increase",], family=binomial(link="logit"))
summary(model_fl)


```

**Condition: decrease**

```{r, echo=FALSE, message=FALSE, warning=FALSE}


model_fl <- glm(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=firstlook_quest[firstlook_quest$dots=="Decrease",], family=binomial(link="logit"))
summary(model_fl)


```

#### Part 2


##### All frames

Plotting the automated tracking result (but excluding participant where the correspondency between manual and automated tracking is too low)

###### Big dataset

There are `r length(exp2_big)` participants with this method.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants by the automated tracking. Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


## Check correlation between manual and automated tracking
ggplot(part2_big, aes(x=automatic, fill=automatic))+
  geom_histogram(stat="count", position="dodge") +
  theme_bw(base_size=15) +
  facet_grid(condition ~ .) +
  scale_fill_manual(values=c( "azure3", "azure2", "dodgerblue",  "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  labs(y="count: frames") +
  guides(fill=FALSE)


```

we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part2_big_reg <- part2_big
part2_big_reg <- part2_big_reg[part2_big_reg$automatic=="Right" | part2_big_reg$automatic == "Left",]
part2_big_reg$automatic <- as.numeric(as.factor(part2_big_reg$automatic)) - 3

model = glmer(automatic ~ condition + (1 | participant_id), data=part2_big_reg, family = binomial(link="logit"))
summary(model)

```



###### Small dataset 

There are `r length(exp2_small)` participants with this method.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants by the automated tracking. Distribution of all the frames classified as Left, Right, Center, or Closed/unsure across all experiments and all participants. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


## Check correlation between manual and automated tracking
ggplot(part2_small, aes(x=automatic, fill=automatic))+
  geom_histogram(stat="count", position="dodge") +
  theme_bw(base_size=15) +
  facet_grid(condition ~ .) +
  scale_fill_manual(values=c( "azure3", "azure2", "dodgerblue",  "darkgoldenrod1")) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) +
  labs(y="count: frames") +
  guides(fill=FALSE)


```


we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part2_small_reg <- part2_small
part2_small_reg <- part2_small_reg[part2_small_reg$automatic=="Right" | part2_small_reg$automatic == "Left",]
part2_small_reg$automatic <- as.numeric(as.factor(part2_small_reg$automatic)) - 3

model = glmer(automatic ~ condition + (1 | participant_id), data=part2_small_reg, family = binomial(link="logit"))
summary(model)

```


##### Laterality index

We compute a **laterality index** per participant: this index is defined as

$$\frac{R}{R + L}$$
where $R$ are the frames where the participant looked on the right and $L$ are the frames where the participant looked on the left.

###### Big dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Laterality index per participant, for the testing with 36 dots and the testing with 4 dots, using the automatic tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


# compute laterality index
part2_big %>%
  filter(automatic == "Left" | automatic=="Right") %>%
  group_by(participant_id, condition) %>%
  count(automatic) %>%
  tidyr::spread(automatic, n) %>%
  mutate(sum_left_right = sum(Right, Left, na.rm=TRUE),
         latindex = Right/sum_left_right) -> latind_corr_m1

# check the specific case where LI is 0 or 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Right)] <- 0
latind_corr_m1$latindex[is.na(latind_corr_m1$Left)] <- 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Left) & is.na(latind_corr_m1$Right)] <- NA

#plot
ggplot(latind_corr_m1, aes(x=condition, y=latindex, fill=condition)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.2, position=position_jitter(0.2)) +
  theme_bw(base_size=15) +
  ylim(0,1) +
  geom_hline(yintercept=0.5, linetype="dashed", color = "black") +
  labs(y="Left <--    Laterality index    --> Right", x="") +
  guides(fill=FALSE)


df_latind_ok <- latind_corr_m1
```


Statistics:

 - Comparing the Increase condition to the theoretical mean of 0.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Increase"], mu=0.5)

```

 - Comparing the testing with 4 dots to the theoretical mean of 0.5:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Decrease"], mu=0.5)

```


###### Small dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Laterality index per participant, for the testing with 36 dots and the testing with 4 dots, using the automatic tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}

# compute laterality index
part2_small %>%
  filter(automatic == "Left" | automatic=="Right") %>%
  group_by(participant_id, condition) %>%
  count(automatic) %>%
  tidyr::spread(automatic, n) %>%
  mutate(sum_left_right = sum(Right, Left, na.rm=TRUE),
         latindex = Right/sum_left_right) -> latind_corr_m1

# check the specific case where LI is 0 or 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Right)] <- 0
latind_corr_m1$latindex[is.na(latind_corr_m1$Left)] <- 1
latind_corr_m1$latindex[is.na(latind_corr_m1$Left) & is.na(latind_corr_m1$Right)] <- NA

#plot
ggplot(latind_corr_m1, aes(x=condition, y=latindex, fill=condition)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.2, position=position_jitter(0.2)) +
  theme_bw(base_size=15) +
  ylim(0,1) +
  geom_hline(yintercept=0.5, linetype="dashed", color = "black") +
  labs(y="Left <--    Laterality index    --> Right", x="") +
  guides(fill=FALSE)


```


Statistics:

 - Comparing the Increase condition to the theoretical mean of 0.5:
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Increase"], mu=0.5)

```

 - Comparing the testing with 4 dots to the theoretical mean of 0.5:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('whatever')}

t.test(latind_corr_m1$latindex[latind_corr_m1$condition=="Decrease"], mu=0.5)

```

##### Side first gaze

We also measure the side of the first gaze during each testing. 

Each partipant undergoes **6 experiments**: 3 for the testing with 36 dots, and 3 with the testing with 4 dots. There are `r length(unique(latind_corr_m1$participant_id))` participants, so in total `r length(unique(latind_corr_m1$participant_id))*6` testings have been done. The Figure below shows the distribution of the first look side for all of these experiments. 

###### Big dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the direction of the first gaze for all testings, in the condition with 36 dots and the testing with 4 dots, using the automated tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}


df_firstlook =data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_firstlook) = c("participant_id", "first_look_side", "exp_num", "dots")

myrow = 1
for (part_id in unique(part2_big$participant_id)){
  subdf = part2_big[part2_big$participant_id==part_id,]
  subdf$exp = "0"
  
  # find number of exp
  myind=0
  for (exp in c(1:6)){
    for (el in (1:90)){
      subdf$exp[(90*myind)+el] = myind+1
    }
    myind = myind + 1
  }
  
  # then check first occurence in each exp
  for (myexp in unique(subdf$exp)){
    subsubdf = subdf[subdf$exp==myexp,]
    subsubdf = subsubdf[subsubdf$automatic=="Left" | subsubdf$automatic=="Right",]
    
    df_firstlook[myrow,] <- c(part_id, as.character(subsubdf$automatic[1]), myexp, as.character(subsubdf$condition[1]))
    myrow= myrow+1
    
  }
}

df_firstlook$first_look_side <- as.character(df_firstlook$first_look_side)
df_firstlook$first_look_side[is.na(df_firstlook$first_look_side)] <- "No side"
df_firstlook$first_look_side <- as.factor(df_firstlook$first_look_side)


ggplot(data=df_firstlook[is.na(df_firstlook$dots)==FALSE,], aes(x=first_look_side, fill=first_look_side))+
  geom_histogram(stat="count", position="dodge", size=1)+
  theme_bw(base_size=15) +
  facet_grid(dots ~ .) +
  labs(x="First look side") +
  scale_fill_manual(values=c("dodgerblue", "darkgoldenrod1"))+
  guides(fill=FALSE) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 

df_firstlook_ok <- df_firstlook

```


we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part2_big_reg <- df_firstlook
part2_big_reg <- part2_big_reg[part2_big_reg$first_look_side=="Right" | part2_big_reg$first_look_side == "Left",]
part2_big_reg$side <- 0
part2_big_reg$side[part2_big_reg$first_look_side == "Right"] <- 1

model = glmer(side ~ dots + (1 | participant_id), data=part2_big_reg, family = binomial(link="logit"))
summary(model)

```


###### Small dataset

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Distribution of the direction of the first gaze for all testings, in the condition with 36 dots and the testing with 4 dots, using the automated tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.')}

## CORR M1
df_firstlook =data.frame(matrix(ncol = 4, nrow = 0))
colnames(df_firstlook) = c("participant_id", "first_look_side", "exp_num", "dots")

myrow = 1
for (part_id in unique(part2_small$participant_id)){
  subdf = part2_small[part2_small$participant_id==part_id,]
  subdf$exp = "0"
  
  # find number of exp
  myind=0
  for (exp in c(1:6)){
    for (el in (1:90)){
      subdf$exp[(90*myind)+el] = myind+1
    }
    myind = myind + 1
  }
  
  # then check first occurence in each exp
  for (myexp in unique(subdf$exp)){
    subsubdf = subdf[subdf$exp==myexp,]
    subsubdf = subsubdf[subsubdf$automatic=="Left" | subsubdf$automatic=="Right",]
    
    df_firstlook[myrow,] <- c(part_id, as.character(subsubdf$automatic[1]), myexp, as.character(subsubdf$condition[1]))
    myrow= myrow+1
    
  }
}

df_firstlook$first_look_side <- as.character(df_firstlook$first_look_side)
df_firstlook$first_look_side[is.na(df_firstlook$first_look_side)] <- "No side"
df_firstlook$first_look_side <- as.factor(df_firstlook$first_look_side)


ggplot(data=df_firstlook[is.na(df_firstlook$dots)==FALSE,], aes(x=first_look_side, fill=first_look_side))+
  geom_histogram(stat="count", position="dodge", size=1)+
  theme_bw(base_size=15) +
  facet_grid(dots ~ .) +
  labs(x="First look side") +
  scale_fill_manual(values=c("dodgerblue", "darkgoldenrod1"))+
  guides(fill=FALSE) +
  geom_text(stat='count', aes(label=..count..), position = position_stack(vjust = 0.5),size=4) 


```


we check in a model, with participant_id as a random effect, whether the 2 conditions are significantly different from each other:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('wt')}

part2_big_reg <- df_firstlook
part2_big_reg <- part2_big_reg[part2_big_reg$first_look_side=="Right" | part2_big_reg$first_look_side == "Left",]
part2_big_reg$side <- 0
part2_big_reg$side[part2_big_reg$first_look_side == "Right"] <- 1

model = glmer(side ~ dots + (1 | participant_id), data=part2_big_reg, family = binomial(link="logit"))
summary(model)

```


##### Effect of variables

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# merge questionnaire
gt2021_part2_quest <- merge(part2_big, quest2021, by="participant_id", all.x=TRUE)
gt2021_part2_quest2 <- gt2021_part2_quest[gt2021_part2_quest$automatic=="Left" | gt2021_part2_quest$automatic=="Right",]

```

###### School & Literacy

**School**:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the school on gaze side.'),  fig.width=10, fig.height=6}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "School", "Gaze", "School")

```


**Literacy**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r plot exp 5 - literacy, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers  on gaze side.')}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "literacy_group", "Gaze", "Literacy_group")


```

###### Age & Gender

**Age**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on gaze side.')}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "age_cut", "Gaze", "Age")

```

**Gender**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on gaze side.')}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "Gender", "Gaze", "Gender")

```


###### Modernity

**Telephone**:

Owning a telephone has absolutely no effect on the bias order of the participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of the card ordering  on gaze side.'),  fig.width=10, fig.height=6}
 
frequency_plot(gt2021_part2_quest2, "Himbas2021", "telephone", "Gaze", "Possess a telephone")

```


**Knowing arabic numbers**:

Also, we believe that knowing arabic numbers (which are ordered from left to right) might bias the result.

```{r plot exp 5 - arabic7 tel, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on gaze side.')}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "knownum", "Gaze", "Know arabic number")

```


###### Intelligence


**Numerical abilities**:

There are 3 participants for which no data on numerical abilities nor raven matrice are available.

```{r plot exp 5 - num, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical abilities on gaze side.'), fig.width=6, fig.height=12}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "numab", "Gaze", "Numerical ability")
#frequency_plot(gt2021_part2_quest2, "Himbas2021", "numcomp", "Gaze", "Comparison score")
#frequency_plot(gt2021_part2_quest2, "Himbas2021", "numcal", "Gaze", "Calculation score")
#frequency_plot(gt2021_part2_quest2, "Himbas2021", "numcount", "Gaze", "Counting score")


```


**Matrice**:

```{r plot exp 5 - raven, echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of raven matrice on gaze side.')}

frequency_plot(gt2021_part2_quest2, "Himbas2021", "mat", "Gaze", "Matrice score")

```

##### Regression

###### Based on all data

**Both conditions (increase and decrease)**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

gt2021_part2_quest2$side_num <- as.numeric(as.factor(gt2021_part2_quest2$automatic)) -1

model_reg <- glmer(as.factor(side_num) ~ as.factor(condition) + as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) + (1|participant_id), data = gt2021_part2_quest2, family=binomial(link="logit"))
summary(model_reg)


gt2021_part2_quest3 <- gather(gt2021_part2_quest2, type_num, measurement, c(num_score, comp_score_percent, counting_score, calcul_score_percent))

ggplot(gt2021_part2_quest3, aes(x=measurement, y=side_num, color=type_num))+
  geom_point() +
  geom_smooth(method="lm") +
  theme_bw(base_size=15)

ggplot(gt2021_part2_quest2, aes(x=num_score, y=side_num))+
  geom_point() +
  geom_smooth() +
  theme_bw(base_size=15)
```

**Condition: increase**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_reg <- glmer(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) + (1|participant_id), data=gt2021_part2_quest2[gt2021_part2_quest2$condition=="Increase",], family=binomial(link="logit"))
summary(model_reg)


```

**Condition: decrease**


```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_reg <- glmer(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) + (1|participant_id), data=gt2021_part2_quest2[gt2021_part2_quest2$condition=="Decrease",], family=binomial(link="logit"))
summary(model_reg)

```

###### Based on the laterality index

**Both conditions (increase and decrease)**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

quest_latind <- merge(df_latind_ok, quest2021, by="participant_id")

model_latind <- lm(latindex ~ as.factor(School) + as.factor(condition) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=quest_latind)
summary(model_latind)

```

**Condition: increase**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_latind <- lm(latindex ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=quest_latind[quest_latind$condition =="Increase",])
summary(model_latind)
```

**Condition: decrease**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

model_latind <- lm(latindex ~ as.factor(School) +  as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=quest_latind[quest_latind$condition =="Decrease",])
summary(model_latind)

```


###### Based on the first look index

**Both conditions (increase and decrease)**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## FIRST LOOK

firstlook_quest <- merge(df_firstlook_ok, quest2021, by="participant_id")


firstlook_quest$side_num <- as.numeric(as.factor(firstlook_quest$first_look_side)) -1

model_fl <- glm(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data = firstlook_quest, family=binomial(link="logit"))
summary(model_fl)

```

**Condition: increase**

```{r, echo=FALSE, message=FALSE, warning=FALSE}


model_fl <- glm(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent) , data=firstlook_quest[firstlook_quest$dots=="Increase",], family=binomial(link="logit"))
summary(model_fl)



```

**Condition: decrease**

```{r, echo=FALSE, message=FALSE, warning=FALSE}


model_fl <- glm(as.factor(side_num) ~ as.factor(School) + as.factor(knownum) + as.factor(literacy_group) + scale(Age) + scale(num_score) + scale(comp_score_percent) + scale(counting_score) + scale(calcul_score_percent) + scale(mat_score_percent), data=firstlook_quest[firstlook_quest$dots=="Decrease",], family=binomial(link="logit"))
summary(model_fl)


```

#### Summarizing results

```{r, echo=FALSE, message=FALSE, warning=FALSE}

ratio_inc_1small = table(part1_small$automatic[part1_small$condition=="Increase"])["Right"] / 
  sum(table(part1_small$automatic[part1_small$condition=="Increase"])[c("Right", "Left")])
ratio_dec_1small = table(part1_small$automatic[part1_small$condition=="Decrease"])["Right"] /
  sum(table(part1_small$automatic[part1_small$condition=="Decrease"])[c("Right", "Left")])
ratio_inc_1big = table(part1_big$automatic[part1_big$condition=="Increase"])["Right"] /
  sum(table(part1_big$automatic[part1_big$condition=="Increase"])[c("Right", "Left")])
ratio_dec_1big = table(part1_big$automatic[part1_big$condition=="Decrease"])["Right"] /
  sum(table(part1_big$automatic[part1_big$condition=="Decrease"])[c("Right", "Left")])

ratio_inc_2small = table(part2_small$automatic[part2_small$condition=="Increase"])["Right"] /
  sum(table(part2_small$automatic[part2_small$condition=="Increase"])[c("Right", "Left")])
ratio_dec_2small = table(part2_small$automatic[part2_small$condition=="Decrease"])["Right"] /
  sum(table(part2_small$automatic[part2_small$condition=="Decrease"])[c("Right", "Left")])
ratio_inc_2big = table(part2_big$automatic[part2_big$condition=="Increase"])["Right"] /
  sum(table(part2_big$automatic[part2_big$condition=="Increase"])[c("Right", "Left")])
ratio_dec_2big = table(part2_big$automatic[part2_big$condition=="Decrease"])["Right"] /
  sum(table(part2_big$automatic[part2_big$condition=="Decrease"])[c("Right", "Left")])

df_all_gt <- data.frame(exp_id = c(1, 1, 1, 1, 2, 2, 2, 2),
                             condition = c("Increase", "Decrease", "Increase", "Decrease", "Increase", "Decrease", "Increase", "Decrease"),
                             type_dataset = c("small", "small", "big", "big", "small", "small", "big", "big"),
                             ratio = c(ratio_inc_1small, ratio_dec_1small, ratio_inc_1big, ratio_dec_1big,
                                       ratio_inc_2small, ratio_dec_2small, ratio_inc_2big, ratio_dec_2big))

ggplot(data=df_all_gt, aes(x=as.factor(exp_id), y=ratio, fill=condition)) +
  geom_bar(stat="identity", position="dodge") +
  facet_grid(type_dataset ~ .) +
  theme_bw(base_size=15) +
  ylim(0, 1) +
  geom_hline(yintercept=0.5)


gt_big <- rbind(part1_big, part2_big)
gt_big_reg <- gt_big
gt_big_reg <- gt_big_reg[gt_big_reg$automatic=="Left" | gt_big_reg$automatic=="Right",]
gt_big_reg$automatic <- as.numeric(as.factor(gt_big_reg$automatic)) - 3

mod <- glmer(automatic ~ exp_id*condition + (1|participant_id), data=gt_big_reg, family = binomial(link="logit"))
summary(mod)

```

### Himbas 2022

#### Percentage gaze on each side 

##### All participants

###### Manual tracking

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.height=20}

rem_closed = 0.90
rem_corr = 0.75

```

**We remove participants that looked at the testing more than `r rem_closed*100` % of the time**

First, we look at the histogram of closed looks for each phase and experiment:

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# SPREAD THIS FINAL TABLE
gt_2022 %>%
  select(-c(ratio_auto, sum_manual, sum_auto)) %>%
  spread(position, ratio_manual) %>%
  mutate(not_lr = `Closed (or unsure)` + Center) -> gt_2022_spread 

# plot distribution of "not looking at the 
ggplot(gt_2022_spread, aes(x=not_lr*100)) +
  geom_histogram() +
  facet_grid(condition ~ type_exp) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = rem_closed*100, color="red") +
  labs(x="Percentage of time not looking at right or left during testing")

# remove participant not looking enough at the testing stimulus
gt_2022_spread %>%
  dplyr::group_by(participant_id, type_exp, condition) %>%
  dplyr::summarize(mean_notok = mean(not_lr)) -> gt_2022_notlr
  
part_rem_1_man <- unique(gt_2022_notlr$participant_id[gt_2022_notlr$mean_notok > rem_closed & gt_2022_notlr$type_exp=="testing"])

```

We used this to remove the participants that did not look enough at the testing: in total, we removed `r length(part_rem_1_man)` participants.

Now, let's look at the distribution of values for all remaining participants. Please note that to compute this plot, we had summarized the ratio of gaze on the left, right, center and closed for each participants. Colored point shows the average for all participants, while errors bars shows the standard errors.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Note that we look at the first, second and third (up to down) habituation and testing (left to right) phase.'), fig.height=11}

gt_2022 %>%
  filter(!(participant_id %in% part_rem_1_man)) %>%
  dplyr::group_by(type_exp, nb_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_manual),
                   ratio = mean(ratio_manual)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")  + 
  theme(axis.text.x = element_text(angle = 40, vjust = 1, hjust=1))

```

Now, we look at the values only considering the **two last habituation** phase, and the **three testing phase**:

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
gt_2022 %>%
  filter(!(participant_id %in% part_rem_1_man)) %>%
  filter(!(type_exp=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(type_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_manual),
                   ratio = mean(ratio_manual)) -> rt_tab

# plot
ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking") + 
  theme(axis.text.x = element_text(angle = 40, vjust = 1, hjust=1))


```

Please note that these plots include `r length(unique(gt_2022$participant_id)) - length(part_rem_1_man)` participants.

###### Automatic tracking

Now, we look exactly at the same values but using data from the automatic tracking (with DeepLabcut)

First, we look at the histogram of closed looks for each phase and experiment:

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# SPREAD THIS FINAL TABLE
gt_2022 %>%
  select(-c(ratio_manual, sum_manual, sum_auto)) %>%
  spread(position, ratio_auto) %>%
  mutate(not_lr = `Closed (or unsure)` + Center) -> gt_2022_spread 

# plot
ggplot(gt_2022_spread[gt_2022_spread$not_lr <= 1,], aes(x=not_lr*100)) +
  geom_histogram() +
  facet_grid(condition ~ type_exp) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = rem_closed*100, color="red") +
  labs(x="Percentage of not looking at left or right")

# check outliers to remove
gt_2022_spread %>%
  dplyr::group_by(participant_id, type_exp, condition) %>%
  dplyr::summarize(mean_notok = mean(not_lr)) -> gt_2022_notlr
  
part_rem_1_auto <- unique(gt_2022_notlr$participant_id[gt_2022_notlr$mean_notok > rem_closed & gt_2022_notlr$type_exp=="testing"])

```

We used this to remove the participants that did not look enough at the testing: in total, we removed `r length(part_rem_1_auto)` participants.

Now, let's look at the distribution of values for all remaining participants. Please note that to compute this plot, we had summarized the ratio of gaze on the left, right, center and closed for each participants. Colored point shows the average for all participants, while errors bars shows the standard errors.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.height=11}

gt_2022 %>%
  filter(!(participant_id %in% part_rem_1_auto)) %>%
  dplyr::group_by(type_exp, nb_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_auto),
                   ratio = mean(ratio_auto)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")

```

Now, we look at the values only considering the two last habituation phase, and the three testing phase:

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# remove first habituation and merge
gt_2022 %>%
  filter(!(participant_id %in% part_rem_1_auto)) %>%
  filter(!(type_exp=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(type_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_auto),
                   ratio = mean(ratio_auto)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")


```

Please note that this plot includes `r length(unique(gt_2022$participant_id)) - length(part_rem_1_auto)` participants.

##### Smaller set of participants

Here, we removed participants based on the correlations between manual and automatic tracking. We assume that the tracking for participants who have high correlations between the two methods might be more accurate.



First, let's plot the correlations between the two methods. 

```{r , echo=FALSE, message=FALSE, warning=FALSE}

# summarize the correlation between manual and automatic for all individuals 
gt2022_all %>%
  filter(type_exp_precise != "Other") %>%
  dplyr::group_by(participant_id, dots, type_for_plot) %>%
  dplyr::summarize(mean_corr = mean(corr_m2, na.rm=TRUE)) %>%
  rename(phase = type_for_plot,
         condition = dots) -> gt2022_corr

# add the previous information about people who looked less than 50% of the time at the testing
gt2022_corr$rem <- "no"
gt2022_corr$rem[gt2022_corr$participant_id %in% part_rem_1_man] <- "yes"

# plot the histogram of correlation
ggplot(gt2022_corr, aes(x=mean_corr, fill=rem)) +
  geom_histogram(alpha=0.7) +
  facet_grid(phase ~ condition) +
  theme_bw(base_size=15) +
  geom_vline(xintercept = rem_corr, color="black") +
  labs(x="Correlation Manual and Automatic tracking", fill="Removed previously")
  
# find participant for which the correlation was lower than indicated for the TESTING ONLY
part_rem_2 <- unique(gt2022_corr$participant_id[gt2022_corr$phase=="testing" & gt2022_corr$mean_corr < rem_corr])
# uncomment this line if you want to be for the testing AND habituation
# part_rem_2 <- unique(gt2022_corr$participant_id[gt2022_corr$mean_corr < rem_corr])

# also remove participant who did not look enough at the testing
part_rem_2 <- c(part_rem_2, part_rem_1_man)
# and remove duplicated
part_rem_2 <- part_rem_2[!duplicated(part_rem_2)]

```

We **remove participants who have a corelation lower than `r rem_corr*100`% AND participants who did look at the testing less than `r rem_closed*100` of the time.**. Doing so, we remove `r length(part_rem_2)` participants.


###### Manual tracking

Now, we look again at the same values using the smaller set of participants, including `r length(unique(gt_2022$participant_id)) - length(part_rem_2)` participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.height=11}

gt_2022 %>%
  filter(!(participant_id %in% part_rem_2)) %>%
  dplyr::group_by(type_exp, nb_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_manual),
                   ratio = mean(ratio_manual)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")

```

Now, we look at the values only considering the two last habituation phase, and the three testing phase:

```{r , echo=FALSE, message=FALSE, warning=FALSE}


# now, remove first habituation
gt_2022 %>%
  filter(!(participant_id %in% part_rem_2)) %>%
  filter(!(type_exp=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(type_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_manual),
                   ratio = mean(ratio_manual)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")


```

Please note that this plot includes `r length(unique(gt_2022$participant_id)) - length(part_rem_2)` participants.

###### Automatic tracking

Now, we looked at the same   `r length(unique(gt_2022$participant_id)) - length(part_rem_2)` participants except that here we look at the values of the automatic tracking. This should be relatively similar as above given that the automatic and manual tracking method are highly correlated for these participants.

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.height=11}

gt_2022 %>%
  filter(!(participant_id %in% part_rem_2)) %>%
  dplyr::group_by(type_exp, nb_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_auto),
                   ratio = mean(ratio_auto)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")

```

Now, we look at the values only considering the two last habituation phase, and the three testing phase:

```{r , echo=FALSE, message=FALSE, warning=FALSE}


# now, remove first habituation
gt_2022 %>%
  filter(!(participant_id %in% part_rem_2)) %>%
  filter(!(type_exp=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(type_exp, condition, position) %>%
  dplyr::summarize(se = se(ratio_auto),
                   ratio = mean(ratio_auto)) -> rt_tab

ggplot(rt_tab, aes(x=position, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(. ~ type_exp) +
  labs(x="", y="Mean ratio - Manual tracking")


```

Please note that this plot includes `r length(unique(gt_2022$participant_id)) - length(part_rem_2)` participants.

#### Laterality index

We compute a **laterality index** per participant: this index is defined as

$$\frac{R}{R + L}$$
where $R$ are the frames where the participant looked on the right and $L$ are the frames where the participant looked on the left.

##### All participant

###### Manual tracking

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Laterality index per participant, for the testing with 36 dots and the testing with 4 dots, using the automatic tracking output. Participants having a low correlation between manual and automatic coding are excluded from this analysis.'), fig.height=7, fig.width=7}


# create laterality index column

# first, create sum left+right for each participant
gt_2022 %>%
  filter(position=="Left" | position == "Right") %>%
  dplyr::group_by(participant_id, type_exp, nb_exp, condition) %>%
  dplyr::summarize(sum_lr_man = sum(sum_manual),
            sum_lr_auto = sum(sum_auto)) -> sum_gt_2022
# then, select only Right
gt_2022 %>%
  filter(position == "Right") %>%
  rename(right_man = sum_manual,
         right_auto = sum_auto) %>%
  select(-position) -> right_gt_2022

# merge both tables
pre_li_gt_2022 <- merge(sum_gt_2022, right_gt_2022, by=c("participant_id", "type_exp", "nb_exp", "condition"))

# now compute the LI 
pre_li_gt_2022 %>%
  filter(!(participant_id %in% part_rem_1_man)) %>%
  mutate(li_man = right_man / sum_lr_man,
         li_auto = right_auto / sum_lr_auto) %>%
  select(-c(right_man, right_auto, sum_lr_man, sum_lr_auto)) %>%
  rename(phase=type_exp) -> li_gt_2022


# plot
ggplot(li_gt_2022, aes(x=condition, y=li_man, fill=phase)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.2, position=position_jitterdodge(0.1)) +
  theme_bw(base_size=15) +
  ylim(0,1) +
  geom_hline(yintercept=0.5, linetype="dashed", color = "black") +
  labs(y="Left <--    Laterality index    --> Right", x="") +
  guides(fill=FALSE) +
  facet_grid(nb_exp ~ .)

```

Plot using the same way as before, with all participants (except those not looking at the testing enough)

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_1_man)) %>%
  dplyr::group_by(phase, condition, nb_exp) %>%
  dplyr::summarize(se = se(li_man),
                   ratio = mean(li_man, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ .) +
  labs(x="", y="left <-- LI --> right") +
  geom_hline(yintercept = 0.5, linetype="dotted")

```

Plot without the first habituation:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_1_man)) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, condition) %>%
  dplyr::summarize(se = se(li_man),
                   ratio = mean(li_man, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  labs(x="", y="left <-- LI --> right") +
  geom_hline(yintercept = 0.5, linetype="dotted")

```


Statistics:

 - Comparing the Increase to the Decrease condition for habituation:
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}

li_gt_2022 %>%
  select(participant_id, phase, nb_exp, condition, li_man) %>%
  spread(condition, li_man) %>%
  filter(!(participant_id %in% part_rem_1_man)) %>%
  filter(is.na(Decrease)==FALSE & is.na(Increase)==FALSE) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, participant_id) %>%
  dplyr::summarize(dec = mean(Decrease, na.rm=TRUE),
                   inc = mean(Increase, na.rm=TRUE)) -> for_ttest


t.test(for_ttest$dec[for_ttest$phase=="habituation"], for_ttest$inc[for_ttest$phase=="habituation"], paired=TRUE)

```

 - Comparing the Increase to the Decrease condition for testing:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

t.test(for_ttest$dec[for_ttest$phase=="testing"], for_ttest$inc[for_ttest$phase=="testing"], paired=TRUE)

```

###### Automatic tracking

Now, we do the same but using values from the automatic tracking


```{r, echo=FALSE, message=FALSE, warning=FALSE}

# plot
ggplot(li_gt_2022, aes(x=condition, y=li_auto, fill=phase)) + 
  geom_boxplot() + 
  geom_jitter(alpha=0.2, position=position_jitterdodge(0.1)) +
  theme_bw(base_size=15) +
  ylim(0,1) +
  geom_hline(yintercept=0.5, linetype="dashed", color = "black") +
  labs(y="Left <--    Laterality index    --> Right", x="") +
  guides(fill=FALSE) +
  facet_grid(nb_exp ~ .)

```

Plot using the same way as before, with all participants (except those not looking at the testing enough)

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_1_auto)) %>%
  dplyr::group_by(phase, condition, nb_exp) %>%
  dplyr::summarize(se = se(li_auto),
                   ratio = mean(li_auto, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ .) +
  labs(x="", y="left <-- LI --> right") +
  geom_hline(yintercept = 0.5, linetype="dotted")

```

Plot without the first habituation:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_1_auto)) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, condition) %>%
  dplyr::summarize(se = se(li_auto),
                   ratio = mean(li_auto, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  labs(x="", y="left <-- LI --> right") +
  geom_hline(yintercept = 0.5, linetype="dotted")

```


Statistics:

 - Comparing the Increase to the Decrease condition for habituation:
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}

li_gt_2022 %>%
  select(participant_id, phase, nb_exp, condition, li_auto) %>%
  spread(condition, li_auto) %>%
  filter(!(participant_id %in% part_rem_1_auto)) %>%
  filter(is.na(Decrease)==FALSE & is.na(Increase)==FALSE) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, participant_id) %>%
  dplyr::summarize(dec = mean(Decrease, na.rm=TRUE),
                   inc = mean(Increase, na.rm=TRUE)) -> for_ttest


t.test(for_ttest$dec[for_ttest$phase=="habituation"], for_ttest$inc[for_ttest$phase=="habituation"], paired=TRUE)

```

 - Comparing the Increase to the Decrease condition for testing:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

t.test(for_ttest$dec[for_ttest$phase=="testing"], for_ttest$inc[for_ttest$phase=="testing"], paired=TRUE)

```

##### Smaller set of participants

###### Manual tracking

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_2)) %>%
  dplyr::group_by(phase, condition, nb_exp) %>%
  dplyr::summarize(se = se(li_man),
                   ratio = mean(li_man, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ .) +
  labs(x="", y="Mean ratio - Manual tracking")

```


Plot without the first habituation:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_2)) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, condition) %>%
  dplyr::summarize(se = se(li_man),
                   ratio = mean(li_man, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  labs(x="", y="Mean ratio - Manual tracking")

```


Statistics:

 - Comparing the Increase condition to the theoretical mean of 0.5:
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}

li_gt_2022 %>%
  select(participant_id, phase, nb_exp, condition, li_auto) %>%
  spread(condition, li_auto) %>%
  filter(!(participant_id %in% part_rem_2)) %>%
  filter(is.na(Decrease)==FALSE & is.na(Increase)==FALSE) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, participant_id) %>%
  dplyr::summarize(dec = mean(Decrease, na.rm=TRUE),
                   inc = mean(Increase, na.rm=TRUE)) -> for_ttest


t.test(for_ttest$dec[for_ttest$phase=="habituation"], for_ttest$inc[for_ttest$phase=="habituation"], paired=TRUE)

```

 - Comparing the Decrease testing  to the theoretical mean of 0.5:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

t.test(for_ttest$dec[for_ttest$phase=="testing"], for_ttest$inc[for_ttest$phase=="testing"], paired=TRUE)

```

###### Automatic tracking

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_2)) %>%
  dplyr::group_by(phase, condition, nb_exp) %>%
  dplyr::summarize(se = se(li_auto),
                   ratio = mean(li_auto, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(nb_exp ~ .) +
  labs(x="", y="Mean ratio - Manual tracking")

```


Plot without the first habituation:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# now, remove first habituation
li_gt_2022 %>%
  filter(!(li_gt_2022$participant_id %in% part_rem_2)) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, condition) %>%
  dplyr::summarize(se = se(li_auto),
                   ratio = mean(li_auto, na.rm=TRUE)) -> rt_tab

ggplot(rt_tab, aes(x=phase, y=ratio, colour=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se),  width=.1, position=position_dodge(0.1)) +
    #geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  labs(x="", y="Mean ratio - Manual tracking")

```


Statistics:

 - Comparing the Increase condition to the theoretical mean of 0.5:
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}

li_gt_2022 %>%
  select(participant_id, phase, nb_exp, condition, li_auto) %>%
  spread(condition, li_auto) %>%
  filter(!(participant_id %in% part_rem_2)) %>%
  filter(is.na(Decrease)==FALSE & is.na(Increase)==FALSE) %>%
  filter(!(phase=="habituation" & nb_exp == 1)) %>%
  dplyr::group_by(phase, participant_id) %>%
  dplyr::summarize(dec = mean(Decrease, na.rm=TRUE),
                   inc = mean(Increase, na.rm=TRUE)) -> for_ttest


t.test(for_ttest$dec[for_ttest$phase=="habituation"], for_ttest$inc[for_ttest$phase=="habituation"], paired=TRUE)

```

 - Comparing the Decrease testing  to the theoretical mean of 0.5:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

t.test(for_ttest$dec[for_ttest$phase=="testing"], for_ttest$inc[for_ttest$phase=="testing"], paired=TRUE)

```

#### Factors

```{r, echo=FALSE, message=FALSE, warning=FALSE}

gt_2022_spread %>%
  filter(!(participant_id %in% part_rem_1_man)) %>%
  mutate(sum_leftright = Left + Right,
         left100 = (Left*100)/sum_leftright,
         right100 = (Right*100)/sum_leftright) %>%
  filter(type_exp=="testing") -> gt2022_big

# change format to wide
gt2022_big2 <- gather(gt2022_big, side100, ratio, left100:right100)

gt2022_big2 %>%
  mutate(side100 = case_when(side100=="left100" ~ "Left",
                               side100=="right100"~"Right")) -> gt2022_big2

# change participant id 
gt2022_big2 %>%
  mutate(participant_id = as.numeric(participant_id)) %>%
  mutate(participant_id = participant_id + 1000) %>%
  mutate(participant_id = as.character(participant_id)) -> gt2022_big2

# merge questionnaire
gt2022_quest <- merge(gt2022_big2, quest2022, by="participant_id", all.x=TRUE)
gt2022_quest2 <- gt2022_quest[gt2022_quest$automatic=="Left" | gt2022_quest$automatic=="Right",]

# remove NA values
gt2022_quest %>%
  filter(is.na(School)==FALSE) -> gt2022_quest

```
 
We look here only at the **testing** phase, using the **big** set of participants and **manual** tracking.

###### School & Literacy

**School**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of school on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, School) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(School ~ .) +
  labs(x="", y="Mean ratio", title="School")


```

**Literacy**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of literacy on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, literacy_group) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(literacy_group ~ .) +
  labs(x="", y="Mean ratio", title="Literacy level")

```

###### Age & gender

**Age**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of age on gaze side.'), fig.height=13, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, age_cut) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(age_cut ~ .) +
  labs(x="", y="Mean ratio", title="Age")

```

**Gender**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of gender on gaze side.'), fig.height=7, fig.width=7}
gt2022_quest %>%
  dplyr::group_by(side100, condition, Gender) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(Gender ~ .) +
  labs(x="", y="Mean ratio", title="Gender")

```

###### Modernity Index

**Urban Index**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban index on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, uind) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(uind ~ .) +
  labs(x="", y="Mean ratio", title="Urban Index")

```

**Urban Preference**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban preference on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, upref) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(upref ~ .) +
  labs(x="", y="Mean ratio", title="Urban Preference")

```

**Urban Knowledge**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of urban knowledge on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, uknow) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(uknow ~ .) +
  labs(x="", y="Mean ratio", title="Urban Knowledge")

```

**Telephone**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of telephone on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  filter(is.na(telephone)==FALSE | telephone!="") %>%
  dplyr::group_by(side100, condition, telephone) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

rt_tab %>%
  mutate(telephone=as.character(telephone)) %>%
  filter(telephone!="") -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(telephone ~ .) +
  labs(x="", y="Mean ratio", title="Telephone")

```

**Know arabic numbers**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of knowing arabic numbers on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, knownum) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(knownum ~ .) +
  labs(x="", y="Mean ratio", title="Know arabic number")

```

###### Intelligence

**Numerical Ability**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of numerical ability on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, numab) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab
rt_tab %>%
  filter(is.na(numab)==FALSE) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(numab ~ .) +
  labs(x="", y="Mean ratio", title="Numerical abilities")

```

**Short Term memory**:


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of short term memory on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, mct) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab
rt_tab %>%
  filter(is.na(mct)==FALSE) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(mct ~ .) +
  labs(x="", y="Mean ratio", title="Short term memory")


```

**Score Matrice**:

```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap=capFig('Looking at the effect of ravens matrice score (in percent) on gaze side.'), fig.height=7, fig.width=7}

gt2022_quest %>%
  dplyr::group_by(side100, condition, mat) %>%
  dplyr::summarize(se = se(ratio),
                   ratio = mean(ratio, na.rm=TRUE)) -> rt_tab
rt_tab %>%
  filter(is.na(mat)==FALSE) -> rt_tab

# plot table
ggplot(rt_tab, aes(x=side100, y=ratio, colour=condition, group=condition)) + 
    geom_errorbar(aes(ymin=ratio-se, ymax=ratio+se), colour="black", width=.1, position=position_dodge(0.1)) +
    geom_line(position=position_dodge(0.1)) +
    geom_point(position=position_dodge(0.1), size=3) +
  theme_bw(base_size=15) +
  facet_grid(mat ~ .) +
  labs(x="", y="Mean ratio", title="Score Matrice")

```

